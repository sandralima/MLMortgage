{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beginning Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel package will be renamed from release 2.8; in order to keep installing from binary please use \"pip install psycopg2-binary\" instead. For details see: <http://initd.org/psycopg/docs/install.html#binary-install-from-pypi>.\n",
      "  \"\"\")\n",
      "2018-10-19 18:15:36,377 - matplotlib - DEBUG - CACHEDIR=/home/ubuntu/.cache/matplotlib\n",
      "2018-10-19 18:15:36,380 - matplotlib.font_manager - DEBUG - Using fontManager instance from /home/ubuntu/.cache/matplotlib/fontList.json\n",
      "2018-10-19 18:15:36,510 - matplotlib.backends - DEBUG - backend module://ipykernel.pylab.backend_inline version unknown\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/ubuntu/MLMortgage/src/data', '', '/home/ubuntu/src/cntk/bindings/python', '/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python36.zip', '/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6', '/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/lib-dynload', '/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages', '/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/IPython/extensions', '/home/ubuntu/.ipython']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-19 18:15:36,748 - matplotlib.backends - DEBUG - backend module://ipykernel.pylab.backend_inline version unknown\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import argparse\n",
    "import psutil\n",
    "import numpy as np\n",
    "import logging\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import glob\n",
    "from tensorflow.python.framework import ops\n",
    "import math\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "import ftplib\n",
    "\n",
    "\n",
    "nb_dir = os.path.join(Path(os.getcwd()).parents[0], 'src', 'data')\n",
    "if nb_dir not in sys.path:\n",
    "    sys.path.insert(0, nb_dir)\n",
    "print(sys.path)\n",
    "import features_selection as fs\n",
    "import make_dataset as md\n",
    "import build_data as bd\n",
    "import get_raw_data as grd\n",
    "import data_classes\n",
    "\n",
    "models_dir = os.path.join(Path(os.getcwd()).parents[0], 'src', 'models')\n",
    "if models_dir not in sys.path:\n",
    "    sys.path.insert(0, models_dir)\n",
    "import nn_real as nn\n",
    "\n",
    "try:\n",
    "    import horovod.tensorflow as hvd\n",
    "except:\n",
    "    print(\"Failed to import horovod module. \"\n",
    "          \"%s is intended for use with Uber's Horovod distributed training \"\n",
    "          \"framework. To create a Docker image with Horovod support see \"\n",
    "          \"docker-examples/Dockerfile.horovod.\" % __file__)\n",
    "    raise\n",
    "\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/MLMortgage/data/raw /home/ubuntu/MLMortgage/data/processed\n"
     ]
    }
   ],
   "source": [
    "RAW_DIR = os.path.join(Path(os.getcwd()).parents[0], 'data', 'raw') \n",
    "PRO_DIR = os.path.join(Path(os.getcwd()).parents[0], 'data', 'processed')\n",
    "RANDOM_SEED = 123  # Set the seed to get reproducable results.\n",
    "DT_FLOAT = tf.float32\n",
    "NP_FLOAT = np.dtype('float32')\n",
    "\n",
    "print(RAW_DIR, PRO_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hvd.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FLAGS_setting(FLAGS, net_number):\n",
    "    # To determine an optimal set of hyperparameters, see Section 11.4.2 of the\n",
    "    # deep learning book. Has (1) grid, (2) random, and (3) Bayesian\n",
    "    # model-based search methods.Swersky et al. have a paper mentioned in that\n",
    "    # section (published in 2014).\n",
    "\n",
    "    # Hyperparameters\n",
    "    # FLAGS.epoch_num = 2  # 14  # 17  # 35  # 15\n",
    "    #print(\"FLAGS.epoch_num\", FLAGS.epoch_num)\n",
    "    # FLAGS.batch_size = 141600 # 4425 # 4000  \n",
    "    FLAGS.dropout_keep = 0.9  # 0.9  # 0.95  # .75  # .6\n",
    "    # ### parameters for training optimizer.\n",
    "    #FLAGS.learning_rate = .1  # .075  # .15  # .25\n",
    "    FLAGS.momentum = .5  # used by the momentum SGD.\n",
    "\n",
    "    # ### parameters for inverse_time_decay\n",
    "    FLAGS.decay_rate = 1\n",
    "    FLAGS.decay_step = 800 * 4400 #steps_per_epoch 1 * 80000 #according to paper: 800 epochs\n",
    "    FLAGS.rate_min = .0015\n",
    "    # ### parameters for exponential_decay\n",
    "    # FLAGS.decay_base = .96  # .96\n",
    "    # FLAGS.decay_step = 15000  # 12320  # 4 * 8700\n",
    "\n",
    "    # ### parameters for regularization\n",
    "    FLAGS.reg_rate = .01 * 1e-3  # * 1e-3\n",
    "\n",
    "    FLAGS.batch_norm = True  # False  #\n",
    "    FLAGS.dropout = True\n",
    "    # A flag to show the results on the held-out test set. Keep this at False.\n",
    "    FLAGS.test_flag = True\n",
    "    FLAGS.xla = True  # False\n",
    "    FLAGS.stratified_flag = False\n",
    "    #FLAGS.batch_type = 'batch'    \n",
    "    FLAGS.weighted_sampling = False  # True  #\n",
    "    # FLAGS.logdir =  os.path.join(Path.home(), 'real_summaries')  # \n",
    "    #FLAGS.n_hidden = 3\n",
    "    #FLAGS.s_hidden = [200, 140, 140]\n",
    "    # FLAGS.allow_summaries = False\n",
    "    FLAGS.epoch_flag = 0    \n",
    "    \n",
    "    #FLAGS.max_epoch_size = 141600*70 #137 # -1\n",
    "    \n",
    "    FLAGS.valid_batch_size = 100000\n",
    "    FLAGS.test_batch_size = 100000\n",
    "    \n",
    "    FLAGS.train_dir = 'chuncks_random_c1millx2_train'\n",
    "    FLAGS.valid_dir = 'chuncks_random_c1millx2_valid'\n",
    "    FLAGS.test_dir = 'chuncks_random_c1millx2_test'\n",
    "    FLAGS.train_period=[121,279] #[121, 143] \n",
    "    FLAGS.valid_period=[280,285] #[144, 147] \n",
    "    FLAGS.test_period=[286,304] #[148, 155]\n",
    "    FLAGS.epoch_num=15 \n",
    "    FLAGS.max_epoch_size=-1 \n",
    "    FLAGS.batch_size=4425\n",
    "    FLAGS.lr_decay_policy       = 'time'\n",
    "    FLAGS.lr_decay_epochs       = 30\n",
    "    FLAGS.lr_decay_rate         = 0.1\n",
    "    FLAGS.lr_poly_power         = 2.\n",
    "    FLAGS.eval = False # True=Evaluation else Training\n",
    "    FLAGS.save_interval = 450\n",
    "    FLAGS.nstep_burnin = 20 # step from to count consuming time for a batch\n",
    "    FLAGS.summary_interval = 1800 # Time in seconds between saves of summary statistics\n",
    "    FLAGS.display_every = 100 # How often (in iterations) to print out running information\n",
    "    \n",
    "    #Retrieveng from ftp:\n",
    "    FLAGS.ftp_dir = 'processed/c1mill_99-01'\n",
    "    \n",
    "    \n",
    "    if FLAGS.n_hidden < 0 : raise ValueError('The size of hidden layer must be at least 0')\n",
    "    if (FLAGS.n_hidden > 0) and (FLAGS.n_hidden != len(FLAGS.s_hidden)) : raise ValueError('Sizes in hidden layers should match!')\n",
    "    \n",
    "    if (net_number==0):\n",
    "        FLAGS.name ='default_settings'        \n",
    "    elif (net_number==1):\n",
    "        FLAGS.name ='batch_layer_type'\n",
    "        FLAGS.batch_layer_type = 'batch'        \n",
    "        \n",
    "    return FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNPARSED ['-f', '/run/user/1000/jupyter/kernel-70e3eb8e-08a0-4eb6-b26f-5a0d8d81f0ca.json']\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "FLAGS, UNPARSED = nn.update_parser(argparse.ArgumentParser())\n",
    "print(\"UNPARSED\", UNPARSED)\n",
    "FLAGS.logdir = Path(str('/home/ubuntu/real_summaries4425-15ep_test/'))\n",
    "if not os.path.exists(os.path.join(FLAGS.logdir)): #os.path.exists\n",
    "    os.makedirs(os.path.join(FLAGS.logdir))\n",
    "FLAGS = FLAGS_setting(FLAGS, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FLAGS Namespace(allow_summaries=True, batch_layer_type='batch', batch_norm=True, batch_size=4425, batch_type='layer', data_dir='/input_data', decay_rate=1, decay_step=3520000, display_every=100, dropout=True, dropout_keep=0.9, epoch_flag=0, epoch_num=15, eval=False, ftp_dir='processed/c1mill_99-01', learning_rate=0.1, log_file=None, logdir=PosixPath('/home/ubuntu/real_summaries4425-15ep_test'), loss_tolerance=0.0001, lr_decay_epochs=30, lr_decay_policy='time', lr_decay_rate=0.1, lr_poly_power=2.0, max_epoch_size=-1, momentum=0.5, n_hidden=3, name='batch_layer_type', nstep_burnin=20, rate_min=0.0015, reg_rate=1e-05, s_hidden=[200, 140, 140], save_interval=450, stratified_flag=False, summary_interval=450, test_batch_size=100000, test_dir='chuncks_random_c1millx2_test', test_flag=True, test_period=[286, 304], train_dir='chuncks_random_c1millx2_train', train_period=[121, 279], valid_batch_size=100000, valid_dir='chuncks_random_c1millx2_valid', valid_period=[280, 285], weighted_sampling=False, xla=True)\n"
     ]
    }
   ],
   "source": [
    "print(\"FLAGS\", FLAGS) #you can change the FLAGS by adding the setting before this line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Builder, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPUNetworkBuilder(object):\n",
    "    \"\"\"This class provides convenient methods for constructing feed-forward\n",
    "    networks with internal data layout of 'NCHW'.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 # is_training,\n",
    "                 dtype=DT_FLOAT,\n",
    "                 activation='RELU',\n",
    "                 use_batch_norm=True,\n",
    "                 batch_norm_config = {'decay':   0.9,\n",
    "                                      'epsilon': 1e-4,\n",
    "                                      'scale':   True,\n",
    "                                      'zero_debias_moving_mean': False}):\n",
    "        self.dtype             = dtype\n",
    "        self.activation_func   = activation\n",
    "        # self.is_training       = is_training\n",
    "        self.use_batch_norm    = use_batch_norm\n",
    "        self.batch_norm_config = batch_norm_config\n",
    "        #self._layer_counts     = defaultdict(lambda: 0)        \n",
    "        \n",
    "    def variable_summaries(self, name, var, allow_summaries):\n",
    "        \"\"\"Create summaries for the given Tensor (for TensorBoard visualization (TB graphs)).\n",
    "            Calculate the mean, min, max, histogram and standardeviation for 'var' variable and save the information\n",
    "            in tf.summary.\n",
    "\n",
    "        Args: \n",
    "             name (String): the of the scope for summaring. For min, max and standardeviation 'calculate_std' is used as sub-scope.\n",
    "             var (Tensor): This is the tensor variable for building summaries.\n",
    "        Returns: \n",
    "            None\n",
    "        Raises:        \n",
    "        \"\"\"\n",
    "        if allow_summaries:\n",
    "            with tf.name_scope(name):\n",
    "                mean = tf.reduce_mean(var)\n",
    "                tf.summary.scalar('mean', mean)\n",
    "                with tf.name_scope('calculate_std'):\n",
    "                    stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "                tf.summary.scalar('stddev', stddev)\n",
    "                tf.summary.scalar('min', tf.reduce_min(var))\n",
    "                tf.summary.scalar('max', tf.reduce_max(var))\n",
    "                tf.summary.histogram('histogram', var)\n",
    "                \n",
    "    def _variable_on_cpu(self, name,\n",
    "                     shape,\n",
    "                     initializer=None,\n",
    "                     regularizer=None,\n",
    "                     dtype=DT_FLOAT):\n",
    "        \"\"\"Create a Variable or get an existing one stored on CPU memory.    \n",
    "\n",
    "        Args:\n",
    "            name (String): name of the variable.\n",
    "            shape (list of ints): Shape of the variable.\n",
    "            initializer: Default None. Initializer for Variable.\n",
    "            regularizer (A (Tensor -> Tensor or None) function): Default None. Regularizer for Variable.\n",
    "            dtype (TYPE): Type of the new variable.\n",
    "        Returns:\n",
    "            Variable Tensor\n",
    "        \"\"\"\n",
    "        with tf.device('/gpu:1'): # this operation is assigned to this device, but this make a copy of data when is transferred on and off the device, which is expensive.\n",
    "            var = tf.get_variable(\n",
    "                name,\n",
    "                shape,\n",
    "                initializer=initializer,\n",
    "                regularizer=regularizer,\n",
    "                dtype=dtype)\n",
    "        return var\n",
    "\n",
    "    def _create_variable(self, name,\n",
    "                         shape, allow_summaries, \n",
    "                         initializer=None,\n",
    "                         regularizer=None,\n",
    "                         dtype=DT_FLOAT):\n",
    "        \"\"\"Call _variable_on_cpu methods and variable_summaries for the 'name' tensor variable. \n",
    "\n",
    "        Args:\n",
    "            name (String): name of the variable.\n",
    "            shape (list of ints): Shape of the variable.\n",
    "            initializer: Default None. Initializer for Variable.\n",
    "            regularizer (A (Tensor -> Tensor or None) function): Default None. Regularizer for Variable.\n",
    "            dtype (TYPE): Type of the new variable.\n",
    "        Returns:\n",
    "            Variable Tensor\n",
    "        \"\"\"\n",
    "        var = self._variable_on_cpu(name, shape, initializer, regularizer, dtype)\n",
    "        self.variable_summaries(name + '/summaries', var, allow_summaries)\n",
    "        return var\n",
    "\n",
    "    def create_weights(self, name, shape, reg_rate, allow_summaries):\n",
    "        \"\"\"Create a Variable initialized with weights which are truncated normal distribution and regularized by\n",
    "        l1_regularizer (L1 regularization encourages sparsity, Regularization can help prevent overfitting).    \n",
    "\n",
    "        Args:\n",
    "            name (String): name of the variable.\n",
    "            shape (list of ints): Shape of the variable.\n",
    "        Returns:\n",
    "            Variable Tensor\n",
    "        \"\"\"    \n",
    "        dtype = DT_FLOAT\n",
    "        # kernel_initializer = tf.uniform_unit_scaling_initializer(\n",
    "        #     factor=1.43, dtype=DT_FLOAT)\n",
    "        # kernel_initializer = tf.contrib.layers.xavier_initializer(\n",
    "        #     uniform=True, dtype=DT_FLOAT)\n",
    "        kernel_initializer = tf.truncated_normal_initializer(\n",
    "            stddev=(1.0 / np.sqrt(shape[0])), dtype=dtype)\n",
    "\n",
    "        regularizer = tf.contrib.layers.l2_regularizer(\n",
    "            np.float32(reg_rate), 'penalty')\n",
    "        return self._create_variable(name, shape, allow_summaries, kernel_initializer, regularizer,\n",
    "                                dtype)\n",
    "\n",
    "    def bias_variable(self, name, shape, layer_name, weighted_sampling): # FLAGS.weighted_sampling\n",
    "        \"\"\"Create a bias variable with appropriate initialization. In case of FLAGS.weighted_sampling==False\n",
    "        and layer_name contains 'soft' the bias variable will contain a np.array of Negative values. Otherwise\n",
    "        the bias variable will be initialized in zero.\n",
    "\n",
    "        Args:\n",
    "            name (String): name of the variable.\n",
    "            shape (list of ints): Shape of the variable.\n",
    "            layer_name (String): name of the layer.\n",
    "        Returns:\n",
    "            Variable Tensor.\n",
    "        \"\"\"\n",
    "        def initial_bias(layer_name):\n",
    "            \"\"\"Get the initial value for the bias of the layer with layer_name.\"\"\"\n",
    "            if (not weighted_sampling) and 'soft' in layer_name:\n",
    "                return np.array(\n",
    "                    [-4.66, -3.81, -4.81, -3.90, -0.08, -3.90, -7.51],\n",
    "                    dtype=NP_FLOAT) + NP_FLOAT(4.1)\n",
    "            return 0.0\n",
    "\n",
    "        initial_value = initial_bias(layer_name)\n",
    "        with tf.name_scope(name) as scope:\n",
    "            initial = tf.constant(initial_value, shape=shape)\n",
    "            bias = tf.Variable(initial, name=scope)\n",
    "            self.variable_summaries('summaries', bias)\n",
    "        return bias        \n",
    "    \n",
    "    def dropout_layer(self, name, tensor_before, FLAGS):\n",
    "        \"\"\"Compute dropout to tensor_before with name scoping and a placeholder for keep_prob. \n",
    "        With probability keep_prob, outputs the input element scaled up by 1 / keep_prob, otherwise outputs 0.\n",
    "\n",
    "        Args:\n",
    "            name (String): name of the scope.\n",
    "            tensor_before (Tensor): Variable Tensor.        \n",
    "        Returns:\n",
    "            Variable Tensor of the same shape of tensor_before.\n",
    "        \"\"\"   \n",
    "        if not FLAGS.dropout:\n",
    "            print('There is not dropout for' + name)\n",
    "            return tensor_before\n",
    "        with tf.name_scope(name) as scope:\n",
    "            keep_prob = tf.placeholder(DT_FLOAT, None, name='keep_proba')\n",
    "            tf.summary.scalar('keep_probability', keep_prob)\n",
    "            dropped = tf.nn.dropout(tensor_before, keep_prob=keep_prob, name=scope)\n",
    "            self.variable_summaries('input_dropped_out', dropped, FLAGS.allow_summaries)\n",
    "        return dropped\n",
    "\n",
    "    def batch_normalization(self, name, input_tensor, train_flag, FLAGS):\n",
    "        \"\"\"Perform batch normalization over the input tensor.\n",
    "        Batch normalization helps avoid overfitting and we're able to use more\n",
    "        aggressive (larger) learning rates, resulting in faster convergence.\n",
    "        training parameter: Either a Python boolean, or a TensorFlow boolean scalar tensor (e.g. a placeholder). \n",
    "        Whether to return the output in training mode (normalized with statistics of the current batch) or in \n",
    "        inference mode (normalized with moving statistics). NOTE: make sure to set this parameter correctly, \n",
    "        or else your training/inference will not work properly.\n",
    "\n",
    "        Args:\n",
    "            name (String): name of the scope and the name of the layer.\n",
    "            input_tensor (Tensor): Variable Tensor.        \n",
    "        Returns:\n",
    "            Variable Tensor # the same shape of input_tensor??.\n",
    "        \"\"\"\n",
    "        # if not FLAGS.batch_norm:\n",
    "        #     return input_tensor\n",
    "        # train_flag = tf.get_default_graph().get_tensor_by_name('train_flag:0')\n",
    "        with tf.name_scope(name):\n",
    "            normalized = tf.layers.batch_normalization(\n",
    "                input_tensor,\n",
    "                center=True,\n",
    "                scale=True,\n",
    "                training=train_flag,\n",
    "                name=name)  # renorm=True, renorm_momentum=0.99)\n",
    "            self.variable_summaries('normalized_batch', normalized, FLAGS.allow_summaries)\n",
    "        return normalized\n",
    "\n",
    "    def layer_normalization(self, name, input_tensor, FLAGS):\n",
    "        \"\"\"Perform layer normalization.\n",
    "\n",
    "        Layer normalization helps avoid overfitting and we're able to use more\n",
    "        aggressive (larger) learning rates, resulting in faster convergence.\n",
    "        Can be used as a normalizer function for conv2d and fully_connected.\n",
    "\n",
    "        Given a tensor inputs of rank R, moments are calculated and normalization \n",
    "        is performed over axes begin_norm_axis ... R - 1. \n",
    "        Scaling and centering, if requested, is performed over axes begin_params_axis .. R - 1.\n",
    "        \"\"\"\n",
    "        # if not FLAGS.batch_norm:\n",
    "        #     return input_tensor\n",
    "        with tf.name_scope(name):\n",
    "            normalized = tf.contrib.layers.layer_norm(\n",
    "                input_tensor, center=True, scale=True, scope=name)\n",
    "            self.variable_summaries('normalized_layer', normalized, FLAGS.allow_summaries)\n",
    "        return normalized\n",
    "\n",
    "\n",
    "    def normalize(self, name, input_tensor, train_flag, FLAGS):\n",
    "        \"\"\"Perform either type (batch/layer) of normalization.\"\"\"\n",
    "        if not FLAGS.batch_norm:\n",
    "            return input_tensor\n",
    "        if FLAGS.batch_type.lower() == 'batch':\n",
    "            return self.batch_normalization(name, input_tensor, train_flag, FLAGS)\n",
    "        if FLAGS.batch_type.lower() == 'layer':\n",
    "            return self.layer_normalization(name, input_tensor, FLAGS)\n",
    "        raise ValueError('Invalid value for batch_type: ' + FLAGS.batch_type)\n",
    "\n",
    "    def nn_layer(self, input_tensor, output_dim, layer_name, FLAGS, act, train_flag):\n",
    "        \"\"\"Create a simple neural net layer.\n",
    "\n",
    "        It performs the affine transformation and uses the activation function to\n",
    "        nonlinearize. It further sets up name scoping so that the resultant graph\n",
    "        is easy to read, and adds a number of summary ops.\n",
    "        \"\"\"\n",
    "        input_dim = input_tensor.shape[1].value    \n",
    "        with tf.variable_scope(layer_name): # A context manager for defining ops that creates variables (layers).\n",
    "            weights = self.create_weights('weights', [input_dim, output_dim], FLAGS.reg_rate, FLAGS.allow_summaries)\n",
    "            # This is outdated and no longer applies: Do not change the order of\n",
    "            # batch normalization and drop out. batch # normalization has to stay\n",
    "            # __before__ the drop out layer.\n",
    "            self.variable_summaries('input', input_tensor, FLAGS.allow_summaries)\n",
    "            input_tensor = self.dropout_layer('dropout', input_tensor, FLAGS)\n",
    "            with tf.name_scope('mix'):\n",
    "                mixed = tf.matmul(input_tensor, weights)\n",
    "                tf.summary.histogram('maybe_guassian', mixed)\n",
    "            # Batch or layer normalization has to stay __after__ the affine\n",
    "            # transformation (the bias term doens't really matter because of the\n",
    "            # beta term in the normalization equation).\n",
    "            # See pp. 5 of the batch normalization paper:\n",
    "            # ```We add the BN transform immediately before the nonlinearity, by\n",
    "            # normalizing x = W u + b```\n",
    "            # biases = bias_variable('biases', [output_dim], layer_name)\n",
    "            preactivate = self.normalize('layer_normalization', mixed, train_flag, FLAGS)  # + biases\n",
    "            # tf.summary.histogram('pre_activations', preactivate)\n",
    "            # preactivate = dropout_layer('dropout', preactivate)\n",
    "            with tf.name_scope('activation') as scope:\n",
    "                activations = self.activate(preactivate, funcname=act)\n",
    "                tf.summary.histogram('activations', activations)\n",
    "        return activations        \n",
    "    \n",
    "    def activate(self, input_layer, funcname=None):\n",
    "        \"\"\"Applies an activation function\"\"\"\n",
    "        if isinstance(funcname, tuple):\n",
    "            funcname = funcname[0]\n",
    "            params = funcname[1:]\n",
    "        if funcname is None:\n",
    "            funcname = self.activation_func\n",
    "        if funcname == 'LINEAR':\n",
    "            return input_layer\n",
    "        activation_map = {\n",
    "            'IDENT':   tf.identity,\n",
    "            'RELU':    tf.nn.relu,\n",
    "            'RELU6':   tf.nn.relu6,\n",
    "            'ELU':     tf.nn.elu,\n",
    "            'SIGMOID': tf.nn.sigmoid,\n",
    "            'TANH':    tf.nn.tanh,\n",
    "            'LRELU':   lambda x, name: tf.maximum(params[0]*x, x, name=name)\n",
    "        }\n",
    "        return activation_map[funcname](input_layer, name=funcname.lower())\n",
    "    \n",
    "    def add_hidden_layers(self, features, architecture, FLAGS, train_flag, act=None):\n",
    "        \"\"\"Add hidden layers to the model using the architecture parameters.\"\"\"\n",
    "        hidden_out = features\n",
    "        jit_scope = tf.contrib.compiler.jit.experimental_jit_scope #JIT compiler compiles and runs parts of TF graphs via XLA, fusing multiple operators (kernel fusion) nto a small number of compiled kernels.\n",
    "        with jit_scope(): #this operation will be compiled with XLA.\n",
    "            for hid_i in range(1, FLAGS.n_hidden + 1):\n",
    "                hidden_out = self.nn_layer(hidden_out,\n",
    "                                      architecture['n_hidden_{:1d}'.format(hid_i)],\n",
    "                                      '{:1d}_hidden'.format(hid_i), FLAGS, act, train_flag)\n",
    "        return hidden_out        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardTrainer(object):\n",
    "    def __init__(self, loss_func, nstep_per_epoch=None):        \n",
    "        self.loss_func = loss_func\n",
    "        self.nstep_per_epoch = nstep_per_epoch\n",
    "        #self.architecture = architecture\n",
    "        #self.FLAGS = FLAGS\n",
    "        with tf.device('/cpu:0'):\n",
    "            #self.global_step = tf.contrib.framework.get_or_create_global_step()\n",
    "            # tf.train.get_global_step()\n",
    "            self.global_step = tf.get_variable(\n",
    "                'global_step', [],\n",
    "                initializer=tf.constant_initializer(0),\n",
    "                dtype=tf.int64,\n",
    "                trainable=False)\n",
    "\n",
    "    def get_learning_rate(self, initial_learning_rate):\n",
    "        \"\"\"Get the learning rate.\"\"\"\n",
    "        with tf.name_scope('learning_rate') as scope:\n",
    "            if FLAGS.lr_decay_policy == 'poly':\n",
    "                return tf.train.polynomial_decay(\n",
    "                                        initial_learning_rate,\n",
    "                                        self.global_step,\n",
    "                                        decay_steps=FLAGS.epoch_num*self.nstep_per_epoch,\n",
    "                                        end_learning_rate=0.,\n",
    "                                        power=FLAGS.lr_poly_power,\n",
    "                                        cycle=False)\n",
    "            elif FLAGS.lr_decay_policy == 'exp':\n",
    "                return tf.train.exponential_decay(\n",
    "                                        initial_learning_rate,\n",
    "                                        self.global_step,\n",
    "                                        decay_steps=FLAGS.lr_decay_epochs*self.nstep_per_epoch,\n",
    "                                        decay_rate=FLAGS.lr_decay_rate,\n",
    "                                        staircase=True)\n",
    "            else:            \n",
    "                # decayed_lr = tf.train.exponential_decay(\n",
    "                #     initial_learning_rate,\n",
    "                #     global_step,\n",
    "                #     FLAGS.decay_step,\n",
    "                #     FLAGS.decay_base,\n",
    "                #     staircase=False)\n",
    "                decayed_lr = tf.train.inverse_time_decay(\n",
    "                    initial_learning_rate,\n",
    "                    self.global_step,\n",
    "                    decay_steps=FLAGS.decay_step,\n",
    "                    decay_rate=FLAGS.decay_rate)\n",
    "                final_lr = tf.clip_by_value(\n",
    "                    decayed_lr, FLAGS.rate_min, 1000, name=scope)\n",
    "                tf.summary.scalar('value', final_lr)\n",
    "                return final_lr\n",
    "        # return self.learning_rate \n",
    "\n",
    "    def get_accuracy(self, labels_int, logits, name):\n",
    "        \"\"\"Get the accuracy tensor.\"\"\"\n",
    "        with tf.name_scope(name) as scope:\n",
    "            # For a classifier model, we can use the in_top_k Op.\n",
    "            # It returns a bool tensor with shape [batch_size] that is true for\n",
    "            # the examples where the label is in the top k (here k=1)\n",
    "            # of all logits for that example.\n",
    "            correct = tf.nn.in_top_k(\n",
    "                logits, labels_int, 1, name='correct_prediction') # returns a tensor of type bool.\n",
    "            return tf.reduce_mean(tf.cast(correct, DT_FLOAT), name=scope)\n",
    "\n",
    "    # auc = get_auc(labels, probs, True, 'metrics/auc')\n",
    "    def get_auc(self, labels, scores, hist_flag, name):\n",
    "        \"\"\"Calculate the AUC of the two-way classifier for the given class.\"\"\"\n",
    "\n",
    "        def get_auc_using_histogram(labels, scores, class_, name):\n",
    "            \"\"\"Calculate the AUC.\"\"\"\n",
    "            class_ind = class_dict[class_.upper()]\n",
    "            with tf.name_scope(name) as scope:\n",
    "                auc, update_op = tf.contrib.metrics.auc_using_histogram( # his Op maintains Variables containing histograms of the scores associated with True and False labels. \n",
    "                    tf.cast(labels[:, class_ind], tf.bool),\n",
    "                    scores[:, class_ind],\n",
    "                    score_range=[0.0, 1.0],\n",
    "                    nbins=200,\n",
    "                    collections=None,\n",
    "                    name=scope)\n",
    "            ops.add_to_collections(ops.GraphKeys.UPDATE_OPS, update_op)\n",
    "            # print(update_op.name)\n",
    "            # print(auc) # it doesn't work because FailedPreconditionError (see above for traceback): Attempting to use uninitialized value metrics/auc/0//hist_accumulate/hist_true_acc\n",
    "            # aucp = tf.Print(auc,[auc], message='AUC the label: ' + class_) # it doesnt work because it doesnt run in a session\n",
    "            # print(aucp)\n",
    "            return auc\n",
    "\n",
    "        def get_auc_metric(labels, scores, class_, name):\n",
    "            \"\"\"Determine the AUC using conventional methods.\"\"\"\n",
    "            class_ind = class_dict[class_.upper()]\n",
    "            with tf.name_scope(name) as scope:\n",
    "                auc, _ = tf.metrics.auc( # Computes the approximate AUC via a Riemann sum.\n",
    "                    tf.cast(labels[:, class_ind], tf.bool), # ?? Print out!!\n",
    "                    scores[:, class_ind],\n",
    "                    weights=None,\n",
    "                    num_thresholds=200,\n",
    "                    metrics_collections=None,\n",
    "                    updates_collections=ops.GraphKeys.UPDATE_OPS,\n",
    "                    curve='ROC',\n",
    "                    name=scope)\n",
    "            # print(auc.op.name)\n",
    "            return auc\n",
    "\n",
    "        classes = ['0', '3', '6', '9', 'C', 'F', 'R']\n",
    "        class_dict = {classes[ind]: ind for ind in range(len(classes))}\n",
    "        if hist_flag:\n",
    "            auc_func = get_auc_using_histogram\n",
    "        else:\n",
    "            auc_func = get_auc_metric\n",
    "        with tf.name_scope(name) as scope:\n",
    "            aucv = [\n",
    "                    auc_func(labels, scores, class_, str(ind)) for ind, class_ in enumerate(classes) # pair (index ej. 0, value ej. '0')\n",
    "                   ]      \n",
    "            auc_values = tf.stack( # Pack along first dim\n",
    "                aucv,\n",
    "                axis=0,\n",
    "                name=scope)\n",
    "            # aucv = tf.Print(auc_values,[auc_values], message='AUC for all labels: ')\n",
    "            # print(aucv) # or maybe aucv.eval() or var = tf.Variable(aucv) and then var.eval(session=sess), or ovar = sess.run(var) but Attempting to use uninitialized value metrics/auc/Variable\n",
    "            return auc_values\n",
    "\n",
    "\n",
    "    # conf_mtx = get_confusion_matrix(labels_int, predictions, len(classes), 'metrics/confusion')\n",
    "    def get_confusion_matrix(self, labels_int, predictions, num_classes, name):\n",
    "        \"\"\"Get the confusion matrix.\n",
    "        Both prediction and labels must be 1-D arrays of the same shape in order for \n",
    "        this function to work.\n",
    "        \"\"\"\n",
    "        with tf.name_scope(name) as scope:\n",
    "            conf = tf.confusion_matrix(\n",
    "                labels_int,\n",
    "                predictions=predictions,\n",
    "                num_classes=num_classes,\n",
    "                dtype=tf.int32,\n",
    "                name=scope,\n",
    "                weights=None)\n",
    "        # print(conf.op.name)\n",
    "        return conf #return a K x K Matriz K = num_classes\n",
    "\n",
    "\n",
    "    def get_m_hand(self, labels, scores, name):\n",
    "        \"\"\"Implement the M measure described in Hand.\n",
    "\n",
    "        See ```A Simple Generalisation of the Area Under the ROC Curve for Multiple\n",
    "        Class Classification Problems``` Hand, Till 2001.    \n",
    "\n",
    "        \"\"\"\n",
    "        def get_auc_using_histogram(labels, scores, first_ind, second_ind, scope):\n",
    "            \"\"\"Calculate the AUC.\n",
    "            Calculate the AUC value by maintainig histograms of boolean variables (labels and \n",
    "            scores masked by the First-Second Individuals rule).\n",
    "            \"\"\"\n",
    "            mask = (labels[:, first_ind] + labels[:, second_ind]) > 0 #one in at least one column.\n",
    "            auc, update_op = tf.contrib.metrics.auc_using_histogram( # maintains variables containing histograms of the scores associated with True, False labels. \n",
    "                tf.cast(tf.boolean_mask(labels[:, first_ind], mask), tf.bool), # tf.boolean_mask: Apply boolean mask to tensor. Numpy equivalent is tensor[mask].\n",
    "                tf.boolean_mask(scores[:, first_ind], mask),\n",
    "                score_range=[0.0, 1.0],\n",
    "                nbins=500,\n",
    "                collections=None,\n",
    "                name=scope)\n",
    "            ops.add_to_collections(ops.GraphKeys.UPDATE_OPS, update_op)\n",
    "            # print(update_op.name)\n",
    "            return auc\n",
    "\n",
    "        temp_array = []\n",
    "        with tf.name_scope(name) as main_scope:\n",
    "            for first_ind in range(7):\n",
    "                for second_ind in range(7):\n",
    "                    if first_ind != second_ind:\n",
    "                        final_name = '{:d}{:d}'.format(first_ind, second_ind)\n",
    "                        with tf.name_scope(final_name) as scope:\n",
    "                            auc = get_auc_using_histogram(\n",
    "                                labels, scores, first_ind, second_ind, scope)\n",
    "                        temp_array.append(auc)\n",
    "            return tf.stack(temp_array, axis=0, name=main_scope) # Stacks a list of rank-R tensors into one rank-(R+1) tensor.\n",
    "\n",
    "\n",
    "    def get_auc_pr_curve(self, labels, scores, name, num_thresholds):    \n",
    "        with tf.name_scope(name) as scope:                             \n",
    "            AUC_PR = []\n",
    "            AUC_data = []\n",
    "            for i in range(7):  \n",
    "                data, update_op = tf.contrib.metrics.precision_recall_at_equal_thresholds(\n",
    "                                name='pr_data',\n",
    "                                predictions=scores[:, i],\n",
    "                                labels=tf.cast(labels[:, i], tf.bool),\n",
    "                                num_thresholds=10, use_locking=True)\n",
    "                ops.add_to_collections(ops.GraphKeys.UPDATE_OPS, update_op)\n",
    "                AUC_data.append((tf.stack(data.recall), tf.stack(data.precision), tf.stack(data.thresholds)))   # we cant use sklearn with tensorflow definition!\n",
    "                auc, _ = tf.metrics.auc(labels[:, i], scores[:, i], weights=None, num_thresholds=10, \n",
    "                                        curve='PR', updates_collections=ops.GraphKeys.UPDATE_OPS, metrics_collections=None, summation_method='careful_interpolation') # \n",
    "                # ops.add_to_collections(ops.GraphKeys.UPDATE_OPS, update_op)\n",
    "                AUC_PR.append(auc)\n",
    "            # print(AUC_data)\n",
    "            return tf.stack( # Pack the array of scalar tensor along one dim tensor\n",
    "                AUC_PR,\n",
    "                axis=0,\n",
    "                name=scope), AUC_data\n",
    "\n",
    "    def log_loss(self, labels, probs, name):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            labels: Labels tensor, int32 - [batch_size, n_classes], with one-hot\n",
    "            encoded values.\n",
    "            logits: Probabilities tensor, float32 - [batch_size, n_classes].\n",
    "        \"\"\"\n",
    "        with tf.name_scope(name) as scope:\n",
    "            total_loss = 0\n",
    "            for j in range(probs.shape[1].value):\n",
    "                loss = tf.losses.log_loss(labels[:, j], probs[:, j], loss_collection=None)\n",
    "                total_loss += loss\n",
    "\n",
    "            return tf.div(total_loss, np.float32(probs.shape[1].value), name=scope)\n",
    "    \n",
    "    def calculate_metrics(self, labels, logits):\n",
    "        \"\"\"Evaluate the quality of the logits at predicting the label.\n",
    "\n",
    "        Args:\n",
    "            labels: Labels tensor, int32 - [batch_size, n_classes], with one-hot\n",
    "            encoded values.\n",
    "            logits: Logits tensor, float32 - [batch_size, n_classes].\n",
    "        Returns:\n",
    "            A scalar float32 tensor with the fraction of examples (out of\n",
    "            batch_size) that were predicted correctly.\n",
    "        \"\"\"\n",
    "        classes = ['0', '3', '6', '9', 'C', 'F', 'R']\n",
    "        with tf.name_scope('metrics'):\n",
    "            labels_int = tf.argmax(labels, 1, name='intlabels') #tf.argmax: Returns the index with the largest value across axes=1 of a tensor.\t\t\n",
    "            predictions = tf.argmax(logits, 1, name='predictions')        \n",
    "            probs = tf.nn.softmax(logits, name='probs') # Computes softmax activations. softmax = tf.exp(logits) / tf.reduce_sum(tf.exp(logits), axis)        \n",
    "\n",
    "        m_list = self.get_m_hand(labels, probs, 'metrics/m_measure')\n",
    "        accuracy = self.get_accuracy(labels_int, logits, 'metrics/accuracy')    \n",
    "        auc = self.get_auc(labels, probs, True, 'metrics/auc')    \n",
    "        conf_mtx = self.get_confusion_matrix(labels_int, predictions,\n",
    "                                        len(classes), 'metrics/confusion')\n",
    "        loss = self.log_loss(labels, probs, 'metrics/log_loss')\n",
    "        pr_auc, pr_data = self.get_auc_pr_curve(labels, probs, 'metrics/auc_pr', 200)\n",
    "\n",
    "        # this is for the definition of the graph:\n",
    "        return accuracy, conf_mtx, auc, m_list, loss, pr_auc, pr_data\n",
    "    \n",
    "    def training_step(self, architecture, FLAGS):        \n",
    "        features = tf.placeholder(\n",
    "            DT_FLOAT, [None, architecture['n_input']], name='features')\n",
    "        labels = tf.placeholder(\n",
    "            DT_FLOAT, [None, architecture['n_classes']], name='targets')\n",
    "        # epoch_flag = tf.placeholder(tf.int32, None, name='epoch_flag')\n",
    "        example_weights = tf.placeholder(\n",
    "            DT_FLOAT, [None], name='example_weights')\n",
    "        with tf.device('/gpu:0'):\n",
    "            # Evaluate the loss and compute the gradients            \n",
    "            loss, logits = self.loss_func(features, labels, architecture, FLAGS)\n",
    "\n",
    "        with tf.device('/cpu:0'): # No in_top_k implem on GPU\n",
    "            accuracy, conf_mtx, auc_list, m_list, lloss, auc_pr, auc_data = self.calculate_metrics(labels, logits)\n",
    "            better_acc = tf.reduce_mean(tf.diag_part(conf_mtx / tf.reduce_sum(conf_mtx, axis=1, keepdims=True)))\n",
    "            auc_mean = tf.reduce_mean(auc_list)\n",
    "            m_list_mean = tf.reduce_mean(m_list)\n",
    "            auc_pr_mean = tf.reduce_mean(auc_pr)\n",
    "            \n",
    "            with tf.name_scope('0_performance'):\n",
    "                # Scalar summaries to track the loss and accuracy over time in TB.\n",
    "                tf.summary.scalar('0accuracy', accuracy)\n",
    "                tf.summary.scalar('1better_accuracy', better_acc)\n",
    "                tf.summary.scalar('2auc_aoc', auc_mean)\n",
    "                tf.summary.scalar('3m_measure', m_list_mean)\n",
    "                tf.summary.scalar('4loss', loss)\n",
    "                tf.summary.scalar('5log_loss', lloss)\n",
    "                tf.summary.scalar('6auc_pr', auc_pr_mean)\n",
    "\n",
    "        # Apply the gradients to optimize the loss function\n",
    "        with tf.device('/gpu:0'):            \n",
    "            update_ops = ops.get_collection(ops.GraphKeys.UPDATE_OPS)\n",
    "            # print(update_ops)\n",
    "            with ops.control_dependencies(update_ops):\n",
    "                with tf.name_scope('train') as scope:\n",
    "                    # print_loss = tf.Print(loss, [loss], name='print_loss') \n",
    "\n",
    "                    # Create a variable to track the global step.\n",
    "        #            global_step = tf.get_variable(\n",
    "        #                'train/global_step',\n",
    "        #                shape=[],\n",
    "        #                initializer=tf.constant_initializer(0, dtype=tf.int32),\n",
    "        #                trainable=False)            \n",
    "                    # Horovod: adjust learning rate based on number of GPUs.\n",
    "                    # optimizer = tf.train.GradientDescentOptimizer(1.0 * hvd.size())\n",
    "                    final_learning_rate = self.get_learning_rate(FLAGS.learning_rate * hvd.size())\n",
    "\n",
    "                    # optimizer = tf.train.GradientDescentOptimizer(final_learning_rate)\n",
    "                    optimizer = tf.train.MomentumOptimizer(final_learning_rate, FLAGS.momentum, use_nesterov=True)\n",
    "                    # optimizer = tf.train.AdagradOptimizer(final_learning_rate)\n",
    "\n",
    "                    # Use the optimizer to apply the gradients that minimize the loss\n",
    "                    # (and increment the global step counter) as a single training step.\n",
    "        #            return optimizer.minimize(\n",
    "        #                loss, global_step=global_step, name=scope)\n",
    "                    optimizer = hvd.DistributedOptimizer(optimizer) #HVD!!\n",
    "                    train_op = optimizer.minimize(loss, global_step=self.global_step, name=scope)\n",
    "            \n",
    "                        \n",
    "        return train_op, final_learning_rate, conf_mtx, accuracy, better_acc, auc_list, auc_mean, m_list, m_list_mean, loss, lloss, auc_pr, auc_pr_mean, auc_data\n",
    "    \n",
    "    def init(self):\n",
    "        # init_op = tf.global_variables_initializer()\n",
    "        # sess.run(init_op)        \n",
    "        \"\"\"Add an Op to the graph to initialize the global and local variables.\"\"\"\n",
    "        with tf.name_scope('init') as scope:\n",
    "            with tf.name_scope('global'):\n",
    "                global_init = tf.global_variables_initializer()\n",
    "            with tf.name_scope('local'):\n",
    "                local_init = tf.local_variables_initializer()\n",
    "                # print(local_init.name)\n",
    "            #init_op = tf.group(global_init, local_init, name=scope)\n",
    "        return global_init, local_init\n",
    "        \n",
    "    def sync(self, sess):\n",
    "        sync_op = hvd.broadcast_global_variables(0)\n",
    "        sess.run(sync_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_func(features, labels, architecture, FLAGS):\n",
    "    # Build the forward model\n",
    "    net = GPUNetworkBuilder(dtype=DT_FLOAT)\n",
    "    train_flag = tf.placeholder(tf.bool, None, name='train_flag')\n",
    "    with tf.name_scope('input_normalization') as scope:\n",
    "        feature_norm = features\n",
    "        net.variable_summaries('input_normalized', feature_norm, FLAGS.allow_summaries)\n",
    "    hidden_out = net.add_hidden_layers(feature_norm, architecture, FLAGS, train_flag)\n",
    "    # Linear output layer for the logits\n",
    "    logits = (net.nn_layer(hidden_out, architecture['n_classes'],'9_softmax_linear', FLAGS, 'IDENT', train_flag))\n",
    "    \n",
    "    with tf.name_scope('loss') as scope:\n",
    "        with tf.name_scope('regularization'):\n",
    "            penalty = tf.losses.get_regularization_loss(name='penalty') #Gets the total regularization loss from an optional scope name (sum for ol + 3h + 2h + 1h).\n",
    "            tf.summary.scalar('weight_norm', penalty / (1e-8 + FLAGS.reg_rate)) #for printing out\n",
    "        with tf.name_scope('cross_entropy') as xentropy_scope:\n",
    "            weighted_cross_entropy = tf.losses.softmax_cross_entropy(\n",
    "                onehot_labels=labels,\n",
    "                logits=logits,\n",
    "                weights=1.0,  # weights,  #\n",
    "                scope=xentropy_scope,\n",
    "                loss_collection=ops.GraphKeys.LOSSES)\n",
    "            tf.summary.scalar('weighted_cross_entropy', weighted_cross_entropy)\n",
    "        loss= tf.add(weighted_cross_entropy, penalty, name=scope) # Returns x + y element-wise.    \n",
    "            \n",
    "    return loss, logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_start_time = time.time()\n",
    "tf.set_random_seed(1234+hvd.rank())\n",
    "np.random.seed(4321+hvd.rank())\n",
    "\n",
    "# create logger:\n",
    "log_name = 'nn-real-hvd_' + str(hvd.rank())\n",
    "logger = logging.getLogger(log_name)\n",
    "logger.setLevel(logging.DEBUG)  # INFO, ERROR\n",
    "# file handler which logs debug messages\n",
    "fh = logging.FileHandler(os.path.join(FLAGS.logdir, log_name + '.log'))\n",
    "fh.setLevel(logging.DEBUG)\n",
    "# console handler\n",
    "ch = logging.StreamHandler()\n",
    "ch.setLevel(logging.INFO)\n",
    "# add formatter to the handlers\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "fh.setFormatter(formatter)\n",
    "ch.setFormatter(formatter)\n",
    "# add handlers to logger\n",
    "logger.addHandler(fh)\n",
    "logger.addHandler(ch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download the data if it has not been downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_data(rank, FLAGS):\n",
    "    server = ftplib.FTP()\n",
    "    server.connect(str(os.environ.get(\"FTP_HOST\")), int(os.environ.get(\"FTP_PORT\")))\n",
    "    server.login(os.environ.get(\"FTP_USER\"), os.environ.get(\"FTP_PASS\"))\n",
    "\n",
    "    server.cwd(FLAGS.ftp_dir)               # change into ftp_dir directory\n",
    "    logger.info(\"FTP connection stablished by worker:  {}\".format(rank))\n",
    "    \n",
    "    filenames = server.nlst() # get filenames within the directory\n",
    "    if FLAGS.eval:        \n",
    "        fname_suffix = '_test_%d.h5' % rank\n",
    "        filenames = [elem for elem in filenames if fname_suffix in elem]        \n",
    "    else:\n",
    "        train_suffix = '_train_%d.h5' % rank\n",
    "        valid_suffix = '_valid_%d.h5' % rank\n",
    "        filenames = [elem for elem in filenames if (train_suffix in elem or valid_suffix in elem)]        \n",
    "\n",
    "    for filename in filenames:        \n",
    "        if FLAGS.eval:\n",
    "            local_path = os.path.join(PRO_DIR, FLAGS.test_dir, filename)    \n",
    "        else:\n",
    "            if (str('train') in filename[-10:-5]):\n",
    "                local_path = os.path.join(PRO_DIR, FLAGS.train_dir, filename)    \n",
    "            else:\n",
    "                local_path = os.path.join(PRO_DIR, FLAGS.valid_dir, filename)    \n",
    "                \n",
    "        if not os.path.exists(local_path):            \n",
    "            file = open(local_path, 'wb')\n",
    "            server.retrbinary('RETR '+ filename, file.write, 8*1024)            \n",
    "            file.close()\n",
    "            logger.info(\"file downloaded:  {}\".format(filename))\n",
    "\n",
    "    server.quit() # This is the polite way to close a connection\n",
    "    logger.info(\"FTP connection closed by worker:  {}\".format(rank))\n",
    "\n",
    "# it is not necessary to have the datasets separates, in just one file is enough!\n",
    "def download_data_by_rank(rank, FLAGS):\n",
    "    server = ftplib.FTP()\n",
    "    server.connect(str(os.environ.get(\"FTP_HOST\")), int(os.environ.get(\"FTP_PORT\")))\n",
    "    server.login(os.environ.get(\"FTP_USER\"), os.environ.get(\"FTP_PASS\"))\n",
    "\n",
    "    server.cwd(FLAGS.ftp_dir)               # change into ftp_dir directory\n",
    "    logger.info(\"FTP connection stablished by worker:  {}\".format(rank))\n",
    "    \n",
    "    filenames = server.nlst() # get filenames within the directory\n",
    "    fname_suffix = '_%d.h5' % rank\n",
    "    filenames = [elem for elem in filenames if fname_suffix in elem]        \n",
    "    \n",
    "    for filename in filenames:            \n",
    "        local_path = os.path.join(PRO_DIR, FLAGS.train_dir, filename)                    \n",
    "        if not os.path.exists(local_path):            \n",
    "            file = open(local_path, 'wb')\n",
    "            server.retrbinary('RETR '+ filename, file.write, 8*1024)            \n",
    "            file.close()\n",
    "            logger.info(\"file downloaded:  {}\".format(filename))\n",
    "\n",
    "    server.quit() \n",
    "    logger.info(\"FTP connection closed by worker:  {}\".format(rank))\n",
    "\n",
    "\n",
    "# download_data_by_rank(hvd.rank(), FLAGS)\n",
    "#download_data(1, FLAGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_records(tf_record_pattern):\n",
    "    def count_records(file_name):\n",
    "        count = 0\n",
    "        for _ in tf.python_io.tf_record_iterator(tf_record_filename):\n",
    "            count += 1\n",
    "        return count\n",
    "    filenames = sorted(tf.gfile.Glob(tf_record_pattern))\n",
    "    nfile = len(filenames)\n",
    "    return (count_records(filenames[0])*(nfile-1) +\n",
    "            count_records(filenames[-1]))\n",
    "\n",
    "def get_files_dict(FLAGS):        \n",
    "    ext = \"*.h5\"\n",
    "        \n",
    "    files_dict = {'train': glob.glob(os.path.join(PRO_DIR, FLAGS.train_dir, ext)), \n",
    "                  'valid': glob.glob(os.path.join(PRO_DIR, FLAGS.valid_dir, ext)), \n",
    "                  'test': glob.glob(os.path.join(PRO_DIR, FLAGS.test_dir, ext))}\n",
    "    return files_dict\n",
    "\n",
    "def architecture_settings(files_dict, FLAGS):\n",
    "    architecture = {}\n",
    "    ok_inputs = True\n",
    "    for key in files_dict.keys():\n",
    "        total_records = 0\n",
    "        for file in files_dict[key]:                                \n",
    "            with pd.HDFStore(file) as dataset_file:\n",
    "                if (ok_inputs): \n",
    "                    index_length = len(dataset_file.get_storer(key+'/features').attrs.data_columns)\n",
    "                    architecture['n_input'] = dataset_file.get_storer(key+ '/features').ncols - index_length\n",
    "                    architecture['n_classes'] = dataset_file.get_storer(key+'/labels').ncols - index_length\n",
    "                    ok_inputs = False                \n",
    "                total_records += dataset_file.get_storer(key + '/features').nrows\n",
    "        architecture[key + '_num_examples'] = total_records                        \n",
    "    \n",
    "    for hid_i in range(1, FLAGS.n_hidden+1):\n",
    "        architecture['n_hidden_{:1d}'.format(hid_i)] = FLAGS.s_hidden[hid_i-1]\n",
    "    print('rank: ', hvd.rank(), 'architecture', architecture)   \n",
    "    time.sleep(5)\n",
    "    return architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rank:  0 architecture {'n_input': 258, 'n_classes': 7, 'train_num_examples': 20809545, 'valid_num_examples': 4020339, 'test_num_examples': 8635528, 'n_hidden_1': 200, 'n_hidden_2': 140, 'n_hidden_3': 140}\n"
     ]
    }
   ],
   "source": [
    "#To sum up the dataset per worker (assuming the same size of files per worker approximately):\n",
    "files_dict = get_files_dict(FLAGS)\n",
    "architecture = architecture_settings(files_dict, FLAGS)\n",
    "\n",
    "if FLAGS.eval:\n",
    "    nrecord = architecture['test_num_examples']\n",
    "else:\n",
    "    nrecord = architecture['train_num_examples']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-19 18:15:43,413 - nn-real-hvd - INFO - Num ranks:  1\n",
      "2018-10-19 18:15:43,413 - nn-real-hvd - INFO - Num ranks:  1\n",
      "2018-10-19 18:15:43,415 - nn-real-hvd - INFO - Num of records: 20809545\n",
      "2018-10-19 18:15:43,415 - nn-real-hvd - INFO - Num of records: 20809545\n",
      "2018-10-19 18:15:43,417 - nn-real-hvd - INFO - Total batch size: 4425\n",
      "2018-10-19 18:15:43,417 - nn-real-hvd - INFO - Total batch size: 4425\n",
      "2018-10-19 18:15:43,419 - nn-real-hvd - INFO - 4425, per device\n",
      "2018-10-19 18:15:43,419 - nn-real-hvd - INFO - 4425, per device\n",
      "2018-10-19 18:15:43,421 - nn-real-hvd - INFO - Data type: <dtype: 'float32'>\n",
      "2018-10-19 18:15:43,421 - nn-real-hvd - INFO - Data type: <dtype: 'float32'>\n",
      "2018-10-19 18:15:43,423 - nn-real-hvd - INFO - architecture: {'n_input': 258, 'n_classes': 7, 'train_num_examples': 20809545, 'valid_num_examples': 4020339, 'test_num_examples': 8635528, 'n_hidden_1': 200, 'n_hidden_2': 140, 'n_hidden_3': 140}\n",
      "2018-10-19 18:15:43,423 - nn-real-hvd - INFO - architecture: {'n_input': 258, 'n_classes': 7, 'train_num_examples': 20809545, 'valid_num_examples': 4020339, 'test_num_examples': 8635528, 'n_hidden_1': 200, 'n_hidden_2': 140, 'n_hidden_3': 140}\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"Num ranks:  {}\".format(hvd.size()))\n",
    "logger.info(\"Num of records: {}\".format(nrecord))\n",
    "logger.info(\"Total batch size: {}\".format(FLAGS.batch_size * hvd.size()))\n",
    "logger.info(\"{}, per device\".format(FLAGS.batch_size))\n",
    "logger.info(\"Data type: {}\".format(DT_FLOAT)) \n",
    "logger.info(\"architecture: {}\".format(architecture)) \n",
    "# time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict:  {'path': '/home/ubuntu/MLMortgage/data/processed/chuncks_random_c1millx2_train/c1mill99-01_non_index_train_0.h5', 'nrows': 20809545, 'init_index': 0, 'end_index': 20809545, 'dataset': <class 'pandas.io.pytables.HDFStore'>\n",
      "File path: /home/ubuntu/MLMortgage/data/processed/chuncks_random_c1millx2_train/c1mill99-01_non_index_train_0.h5\n",
      ", 'dataset_features': array([[0.11811024, 0.        , 1.431     , ..., 0.        , 0.        ,\n",
      "        0.        ],\n",
      "       [0.        , 0.        , 1.3775    , ..., 0.        , 0.        ,\n",
      "        0.        ],\n",
      "       [0.        , 0.        , 1.354     , ..., 0.        , 0.        ,\n",
      "        0.        ],\n",
      "       ...,\n",
      "       [0.        , 0.        , 2.125     , ..., 0.        , 0.        ,\n",
      "        0.        ],\n",
      "       [0.        , 0.        , 2.125     , ..., 0.        , 0.        ,\n",
      "        0.        ],\n",
      "       [0.        , 0.        , 2.125     , ..., 0.        , 0.        ,\n",
      "        0.        ]], dtype=float32), 'dataset_labels': array([[0, 0, 0, ..., 1, 0, 0],\n",
      "       [0, 0, 0, ..., 1, 0, 0],\n",
      "       [0, 0, 0, ..., 1, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 0, ..., 1, 0, 0],\n",
      "       [0, 0, 0, ..., 1, 0, 0],\n",
      "       [0, 0, 0, ..., 1, 0, 0]], dtype=int8)}  total rows:  20809545\n",
      "dataset_features:  (4020339, 258)\n",
      "dict:  {'path': '/home/ubuntu/MLMortgage/data/processed/chuncks_random_c1millx2_valid/c1mill99-01_non_index_valid_0.h5', 'nrows': 4020339, 'init_index': 0, 'end_index': 4020339, 'dataset': <class 'pandas.io.pytables.HDFStore'>\n",
      "File path: /home/ubuntu/MLMortgage/data/processed/chuncks_random_c1millx2_valid/c1mill99-01_non_index_valid_0.h5\n",
      ", 'dataset_features': array([[0.    , 0.    , 1.8445, ..., 0.    , 0.    , 0.    ],\n",
      "       [0.    , 0.    , 1.8535, ..., 0.    , 0.    , 0.    ],\n",
      "       [0.    , 0.    , 1.8585, ..., 0.    , 0.    , 0.    ],\n",
      "       ...,\n",
      "       [0.    , 0.    , 2.125 , ..., 0.    , 0.    , 0.    ],\n",
      "       [0.    , 0.    , 2.125 , ..., 0.    , 0.    , 0.    ],\n",
      "       [0.    , 0.    , 2.125 , ..., 0.    , 0.    , 0.    ]],\n",
      "      dtype=float32), 'dataset_labels': array([[0, 0, 0, ..., 1, 0, 0],\n",
      "       [0, 0, 0, ..., 1, 0, 0],\n",
      "       [0, 0, 0, ..., 1, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 0, ..., 1, 0, 0],\n",
      "       [0, 0, 0, ..., 1, 0, 0],\n",
      "       [0, 0, 0, ..., 1, 0, 0]], dtype=int8)}  accumulated rows:  4020339\n",
      "Features List:  ['MBA_DAYS_DELINQUENT', 'MBA_DAYS_DELINQUENT_NAN', 'CURRENT_INTEREST_RATE', 'CURRENT_INTEREST_RATE_NAN', 'LOANAGE', 'LOANAGE_NAN', 'CURRENT_BALANCE', 'CURRENT_BALANCE_NAN', 'SCHEDULED_PRINCIPAL', 'SCHEDULED_PRINCIPAL_NAN', 'SCHEDULED_MONTHLY_PANDI', 'SCHEDULED_MONTHLY_PANDI_NAN', 'LLMA2_CURRENT_INTEREST_SPREAD', 'LLMA2_CURRENT_INTEREST_SPREAD_NAN', 'LLMA2_C_IN_LAST_12_MONTHS', 'LLMA2_30_IN_LAST_12_MONTHS', 'LLMA2_60_IN_LAST_12_MONTHS', 'LLMA2_90_IN_LAST_12_MONTHS', 'LLMA2_FC_IN_LAST_12_MONTHS', 'LLMA2_REO_IN_LAST_12_MONTHS', 'LLMA2_0_IN_LAST_12_MONTHS', 'LLMA2_HIST_LAST_12_MONTHS_MIS', 'NUM_MODIF', 'NUM_MODIF_NAN', 'P_RATE_TO_MOD', 'P_RATE_TO_MOD_NAN', 'MOD_RATE', 'MOD_RATE_NAN', 'DIF_RATE', 'DIF_RATE_NAN', 'P_MONTHLY_PAY', 'P_MONTHLY_PAY_NAN', 'MOD_MONTHLY_PAY', 'MOD_MONTHLY_PAY_NAN', 'DIF_MONTHLY_PAY', 'DIF_MONTHLY_PAY_NAN', 'CAPITALIZATION_AMT', 'CAPITALIZATION_AMT_NAN', 'MORTGAGE_RATE', 'MORTGAGE_RATE_NAN', 'FICO_SCORE_ORIGINATION', 'INITIAL_INTEREST_RATE', 'ORIGINAL_LTV', 'ORIGINAL_BALANCE', 'BACKEND_RATIO', 'BACKEND_RATIO_NAN', 'ORIGINAL_TERM', 'ORIGINAL_TERM_NAN', 'SALE_PRICE', 'SALE_PRICE_NAN', 'PREPAY_PENALTY_TERM', 'PREPAY_PENALTY_TERM_NAN', 'NUMBER_OF_UNITS', 'NUMBER_OF_UNITS_NAN', 'MARGIN', 'MARGIN_NAN', 'PERIODIC_RATE_CAP', 'PERIODIC_RATE_CAP_NAN', 'PERIODIC_RATE_FLOOR', 'PERIODIC_RATE_FLOOR_NAN', 'LIFETIME_RATE_CAP', 'LIFETIME_RATE_CAP_NAN', 'LIFETIME_RATE_FLOOR', 'LIFETIME_RATE_FLOOR_NAN', 'RATE_RESET_FREQUENCY', 'RATE_RESET_FREQUENCY_NAN', 'PAY_RESET_FREQUENCY', 'PAY_RESET_FREQUENCY_NAN', 'FIRST_RATE_RESET_PERIOD', 'FIRST_RATE_RESET_PERIOD_NAN', 'LLMA2_PRIME', 'LLMA2_SUBPRIME', 'LLMA2_APPVAL_LT_SALEPRICE', 'LLMA2_ORIG_RATE_SPREAD', 'LLMA2_ORIG_RATE_SPREAD_NAN', 'AGI', 'AGI_NAN', 'UR', 'UR_NAN', 'LLMA2_ORIG_RATE_ORIG_MR_SPREAD', 'LLMA2_ORIG_RATE_ORIG_MR_SPREAD_NAN', 'COUNT_INT_RATE_LESS', 'NUM_PRIME_ZIP', 'NUM_PRIME_ZIP_NAN', 'MBA_DELINQUENCY_STATUS_0', 'MBA_DELINQUENCY_STATUS_3', 'MBA_DELINQUENCY_STATUS_6', 'MBA_DELINQUENCY_STATUS_9', 'MBA_DELINQUENCY_STATUS_C', 'MBA_DELINQUENCY_STATUS_F', 'MBA_DELINQUENCY_STATUS_R', 'BUYDOWN_FLAG_N', 'BUYDOWN_FLAG_U', 'BUYDOWN_FLAG_Y', 'NEGATIVE_AMORTIZATION_FLAG_N', 'NEGATIVE_AMORTIZATION_FLAG_U', 'NEGATIVE_AMORTIZATION_FLAG_Y', 'PREPAY_PENALTY_FLAG_N', 'PREPAY_PENALTY_FLAG_U', 'PREPAY_PENALTY_FLAG_Y', 'OCCUPANCY_TYPE_1', 'OCCUPANCY_TYPE_2', 'OCCUPANCY_TYPE_3', 'OCCUPANCY_TYPE_U', 'PRODUCT_TYPE_10', 'PRODUCT_TYPE_20', 'PRODUCT_TYPE_30', 'PRODUCT_TYPE_40', 'PRODUCT_TYPE_50', 'PRODUCT_TYPE_51', 'PRODUCT_TYPE_52', 'PRODUCT_TYPE_53', 'PRODUCT_TYPE_54', 'PRODUCT_TYPE_5A', 'PRODUCT_TYPE_5Z', 'PRODUCT_TYPE_60', 'PRODUCT_TYPE_61', 'PRODUCT_TYPE_62', 'PRODUCT_TYPE_63', 'PRODUCT_TYPE_6Z', 'PRODUCT_TYPE_70', 'PRODUCT_TYPE_80', 'PRODUCT_TYPE_81', 'PRODUCT_TYPE_82', 'PRODUCT_TYPE_83', 'PRODUCT_TYPE_84', 'PRODUCT_TYPE_8Z', 'PRODUCT_TYPE_U', 'PROPERTY_TYPE_1', 'PROPERTY_TYPE_2', 'PROPERTY_TYPE_3', 'PROPERTY_TYPE_4', 'PROPERTY_TYPE_5', 'PROPERTY_TYPE_6', 'PROPERTY_TYPE_7', 'PROPERTY_TYPE_8', 'PROPERTY_TYPE_9', 'PROPERTY_TYPE_M', 'PROPERTY_TYPE_U', 'PROPERTY_TYPE_Z', 'LOAN_PURPOSE_CATEGORY_P', 'LOAN_PURPOSE_CATEGORY_R', 'LOAN_PURPOSE_CATEGORY_U', 'DOCUMENTATION_TYPE_1', 'DOCUMENTATION_TYPE_2', 'DOCUMENTATION_TYPE_3', 'DOCUMENTATION_TYPE_U', 'CHANNEL_1', 'CHANNEL_2', 'CHANNEL_3', 'CHANNEL_4', 'CHANNEL_5', 'CHANNEL_6', 'CHANNEL_7', 'CHANNEL_8', 'CHANNEL_9', 'CHANNEL_A', 'CHANNEL_B', 'CHANNEL_C', 'CHANNEL_D', 'CHANNEL_U', 'LOAN_TYPE_1', 'LOAN_TYPE_2', 'LOAN_TYPE_3', 'LOAN_TYPE_4', 'LOAN_TYPE_5', 'LOAN_TYPE_6', 'LOAN_TYPE_U', 'IO_FLAG_N', 'IO_FLAG_U', 'IO_FLAG_Y', 'CONVERTIBLE_FLAG_N', 'CONVERTIBLE_FLAG_U', 'CONVERTIBLE_FLAG_Y', 'POOL_INSURANCE_FLAG_N', 'POOL_INSURANCE_FLAG_U', 'POOL_INSURANCE_FLAG_Y', 'STATE_AK', 'STATE_AL', 'STATE_AR', 'STATE_AZ', 'STATE_CA', 'STATE_CO', 'STATE_CT', 'STATE_DC', 'STATE_DE', 'STATE_FL', 'STATE_GA', 'STATE_HI', 'STATE_IA', 'STATE_ID', 'STATE_IL', 'STATE_IN', 'STATE_KS', 'STATE_KY', 'STATE_LA', 'STATE_MA', 'STATE_MD', 'STATE_ME', 'STATE_MI', 'STATE_MN', 'STATE_MO', 'STATE_MS', 'STATE_MT', 'STATE_NC', 'STATE_ND', 'STATE_NE', 'STATE_NH', 'STATE_NJ', 'STATE_NM', 'STATE_NV', 'STATE_NY', 'STATE_OH', 'STATE_OK', 'STATE_OR', 'STATE_PA', 'STATE_PR', 'STATE_RI', 'STATE_SC', 'STATE_SD', 'STATE_TN', 'STATE_TX', 'STATE_UT', 'STATE_VA', 'STATE_VT', 'STATE_WA', 'STATE_WI', 'STATE_WV', 'STATE_WY', 'CURRENT_INVESTOR_CODE_240', 'CURRENT_INVESTOR_CODE_250', 'CURRENT_INVESTOR_CODE_253', 'CURRENT_INVESTOR_CODE_U', 'ORIGINATION_YEAR_B1995', 'ORIGINATION_YEAR_1995', 'ORIGINATION_YEAR_1996', 'ORIGINATION_YEAR_1997', 'ORIGINATION_YEAR_1998', 'ORIGINATION_YEAR_1999', 'ORIGINATION_YEAR_2000', 'ORIGINATION_YEAR_2001', 'ORIGINATION_YEAR_2002', 'ORIGINATION_YEAR_2003', 'ORIGINATION_YEAR_2004', 'ORIGINATION_YEAR_2005', 'ORIGINATION_YEAR_2006', 'ORIGINATION_YEAR_2007', 'ORIGINATION_YEAR_2008', 'ORIGINATION_YEAR_2009', 'ORIGINATION_YEAR_2010', 'ORIGINATION_YEAR_2011', 'ORIGINATION_YEAR_2012', 'ORIGINATION_YEAR_2013', 'ORIGINATION_YEAR_2014', 'ORIGINATION_YEAR_2015', 'ORIGINATION_YEAR_2016', 'ORIGINATION_YEAR_2017', 'ORIGINATION_YEAR_2018']\n",
      "Labels List:  ['DELINQUENCY_STATUS_NEXT_0', 'DELINQUENCY_STATUS_NEXT_3', 'DELINQUENCY_STATUS_NEXT_6', 'DELINQUENCY_STATUS_NEXT_9', 'DELINQUENCY_STATUS_NEXT_C', 'DELINQUENCY_STATUS_NEXT_F', 'DELINQUENCY_STATUS_NEXT_R']\n"
     ]
    }
   ],
   "source": [
    "if not FLAGS.eval:\n",
    "    DATA = md.get_h5_data(PRO_DIR, FLAGS.train_dir, FLAGS.valid_dir, None, train_period=FLAGS.train_period, valid_period=FLAGS.valid_period, test_period=FLAGS.test_period) \n",
    "    print('Features List: ', DATA.train.features_list)\n",
    "    print('Labels List: ', DATA.train.labels_list)\n",
    "else:\n",
    "    DATA = md.get_h5_data(PRO_DIR, None, None, FLAGS.test_dir, train_period=FLAGS.train_period, valid_period=FLAGS.valid_period, test_period=FLAGS.test_period) \n",
    "    print('Features List: ', DATA.test.features_list)\n",
    "    print('Labels List: ', DATA.test.labels_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(DATA.test._dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70540\n"
     ]
    }
   ],
   "source": [
    "nstep = 0\n",
    "if FLAGS.epoch_num is not None:\n",
    "    if (nrecord <= 0):\n",
    "        logger.error(\"num_epochs requires data_dir to be specified\")\n",
    "        raise ValueError(\"num_epochs requires data_dir to be specified\")\n",
    "    nstep = nrecord * FLAGS.epoch_num //  FLAGS.batch_size # if it is kwnow the total size: (FLAGS.batch_size * hvd.size())\n",
    "print(nstep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training features - Sample [[0.11811024 0.         1.431      ... 0.         0.         0.        ]\n",
      " [0.         0.         1.3775     ... 0.         0.         0.        ]\n",
      " [0.         0.         1.354      ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.         0.         1.345      ... 0.         0.         0.        ]\n",
      " [0.         0.         1.345      ... 0.         0.         0.        ]\n",
      " [0.         0.         1.345      ... 0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print('Training features - Sample', DATA.train._dict[0]['dataset_features'][0:100]) #you can increase the sampling number of records "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-19 18:24:55,210 - nn-real-hvd - INFO - METRICS:  Namespace(allow_summaries=True, batch_layer_type='batch', batch_norm=True, batch_size=4425, batch_type='layer', data_dir='/input_data', decay_rate=1, decay_step=3520000, display_every=100, dropout=True, dropout_keep=0.9, epoch_flag=0, epoch_num=15, eval=False, ftp_dir='processed/c1mill_99-01', learning_rate=0.1, log_file=None, logdir=PosixPath('/home/ubuntu/real_summaries4425-15ep_test'), loss_tolerance=0.0001, lr_decay_epochs=30, lr_decay_policy='time', lr_decay_rate=0.1, lr_poly_power=2.0, max_epoch_size=-1, momentum=0.5, n_hidden=3, name='batch_layer_type', nstep_burnin=20, rate_min=0.0015, reg_rate=1e-05, s_hidden=[200, 140, 140], save_interval=450, stratified_flag=False, summary_interval=450, test_batch_size=100000, test_dir='chuncks_random_c1millx2_test', test_flag=True, test_period=[286, 304], train_dir='chuncks_random_c1millx2_train', train_period=[121, 279], valid_batch_size=100000, valid_dir='chuncks_random_c1millx2_valid', valid_period=[280, 285], weighted_sampling=False, xla=True)\n",
      "\n",
      "2018-10-19 18:24:55,210 - nn-real-hvd - INFO - METRICS:  Namespace(allow_summaries=True, batch_layer_type='batch', batch_norm=True, batch_size=4425, batch_type='layer', data_dir='/input_data', decay_rate=1, decay_step=3520000, display_every=100, dropout=True, dropout_keep=0.9, epoch_flag=0, epoch_num=15, eval=False, ftp_dir='processed/c1mill_99-01', learning_rate=0.1, log_file=None, logdir=PosixPath('/home/ubuntu/real_summaries4425-15ep_test'), loss_tolerance=0.0001, lr_decay_epochs=30, lr_decay_policy='time', lr_decay_rate=0.1, lr_poly_power=2.0, max_epoch_size=-1, momentum=0.5, n_hidden=3, name='batch_layer_type', nstep_burnin=20, rate_min=0.0015, reg_rate=1e-05, s_hidden=[200, 140, 140], save_interval=450, stratified_flag=False, summary_interval=450, test_batch_size=100000, test_dir='chuncks_random_c1millx2_test', test_flag=True, test_period=[286, 304], train_dir='chuncks_random_c1millx2_train', train_period=[121, 279], valid_batch_size=100000, valid_dir='chuncks_random_c1millx2_valid', valid_period=[280, 285], weighted_sampling=False, xla=True)\n",
      "\n",
      "2018-10-19 18:24:55,213 - nn-real-hvd - INFO - training files:  {0: {'path': '/home/ubuntu/MLMortgage/data/processed/chuncks_random_c1millx2_train/c1mill99-01_non_index_train_0.h5', 'nrows': 20809545, 'init_index': 0, 'end_index': 20809545, 'dataset': <class 'pandas.io.pytables.HDFStore'>\n",
      "File path: /home/ubuntu/MLMortgage/data/processed/chuncks_random_c1millx2_train/c1mill99-01_non_index_train_0.h5\n",
      ", 'dataset_features': array([[0.11811024, 0.        , 1.431     , ..., 0.        , 0.        ,\n",
      "        0.        ],\n",
      "       [0.        , 0.        , 1.3775    , ..., 0.        , 0.        ,\n",
      "        0.        ],\n",
      "       [0.        , 0.        , 1.354     , ..., 0.        , 0.        ,\n",
      "        0.        ],\n",
      "       ...,\n",
      "       [0.        , 0.        , 2.125     , ..., 0.        , 0.        ,\n",
      "        0.        ],\n",
      "       [0.        , 0.        , 2.125     , ..., 0.        , 0.        ,\n",
      "        0.        ],\n",
      "       [0.        , 0.        , 2.125     , ..., 0.        , 0.        ,\n",
      "        0.        ]], dtype=float32), 'dataset_labels': array([[0, 0, 0, ..., 1, 0, 0],\n",
      "       [0, 0, 0, ..., 1, 0, 0],\n",
      "       [0, 0, 0, ..., 1, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 0, ..., 1, 0, 0],\n",
      "       [0, 0, 0, ..., 1, 0, 0],\n",
      "       [0, 0, 0, ..., 1, 0, 0]], dtype=int8)}}\n",
      "\n",
      "2018-10-19 18:24:55,213 - nn-real-hvd - INFO - training files:  {0: {'path': '/home/ubuntu/MLMortgage/data/processed/chuncks_random_c1millx2_train/c1mill99-01_non_index_train_0.h5', 'nrows': 20809545, 'init_index': 0, 'end_index': 20809545, 'dataset': <class 'pandas.io.pytables.HDFStore'>\n",
      "File path: /home/ubuntu/MLMortgage/data/processed/chuncks_random_c1millx2_train/c1mill99-01_non_index_train_0.h5\n",
      ", 'dataset_features': array([[0.11811024, 0.        , 1.431     , ..., 0.        , 0.        ,\n",
      "        0.        ],\n",
      "       [0.        , 0.        , 1.3775    , ..., 0.        , 0.        ,\n",
      "        0.        ],\n",
      "       [0.        , 0.        , 1.354     , ..., 0.        , 0.        ,\n",
      "        0.        ],\n",
      "       ...,\n",
      "       [0.        , 0.        , 2.125     , ..., 0.        , 0.        ,\n",
      "        0.        ],\n",
      "       [0.        , 0.        , 2.125     , ..., 0.        , 0.        ,\n",
      "        0.        ],\n",
      "       [0.        , 0.        , 2.125     , ..., 0.        , 0.        ,\n",
      "        0.        ]], dtype=float32), 'dataset_labels': array([[0, 0, 0, ..., 1, 0, 0],\n",
      "       [0, 0, 0, ..., 1, 0, 0],\n",
      "       [0, 0, 0, ..., 1, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 0, ..., 1, 0, 0],\n",
      "       [0, 0, 0, ..., 1, 0, 0],\n",
      "       [0, 0, 0, ..., 1, 0, 0]], dtype=int8)}}\n",
      "\n",
      "2018-10-19 18:24:55,216 - nn-real-hvd - INFO - validation files:  {0: {'path': '/home/ubuntu/MLMortgage/data/processed/chuncks_random_c1millx2_valid/c1mill99-01_non_index_valid_0.h5', 'nrows': 4020339, 'init_index': 0, 'end_index': 4020339, 'dataset': <class 'pandas.io.pytables.HDFStore'>\n",
      "File path: /home/ubuntu/MLMortgage/data/processed/chuncks_random_c1millx2_valid/c1mill99-01_non_index_valid_0.h5\n",
      ", 'dataset_features': array([[0.    , 0.    , 1.8445, ..., 0.    , 0.    , 0.    ],\n",
      "       [0.    , 0.    , 1.8535, ..., 0.    , 0.    , 0.    ],\n",
      "       [0.    , 0.    , 1.8585, ..., 0.    , 0.    , 0.    ],\n",
      "       ...,\n",
      "       [0.    , 0.    , 2.125 , ..., 0.    , 0.    , 0.    ],\n",
      "       [0.    , 0.    , 2.125 , ..., 0.    , 0.    , 0.    ],\n",
      "       [0.    , 0.    , 2.125 , ..., 0.    , 0.    , 0.    ]],\n",
      "      dtype=float32), 'dataset_labels': array([[0, 0, 0, ..., 1, 0, 0],\n",
      "       [0, 0, 0, ..., 1, 0, 0],\n",
      "       [0, 0, 0, ..., 1, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 0, ..., 1, 0, 0],\n",
      "       [0, 0, 0, ..., 1, 0, 0],\n",
      "       [0, 0, 0, ..., 1, 0, 0]], dtype=int8)}}\n",
      "\n",
      "2018-10-19 18:24:55,216 - nn-real-hvd - INFO - validation files:  {0: {'path': '/home/ubuntu/MLMortgage/data/processed/chuncks_random_c1millx2_valid/c1mill99-01_non_index_valid_0.h5', 'nrows': 4020339, 'init_index': 0, 'end_index': 4020339, 'dataset': <class 'pandas.io.pytables.HDFStore'>\n",
      "File path: /home/ubuntu/MLMortgage/data/processed/chuncks_random_c1millx2_valid/c1mill99-01_non_index_valid_0.h5\n",
      ", 'dataset_features': array([[0.    , 0.    , 1.8445, ..., 0.    , 0.    , 0.    ],\n",
      "       [0.    , 0.    , 1.8535, ..., 0.    , 0.    , 0.    ],\n",
      "       [0.    , 0.    , 1.8585, ..., 0.    , 0.    , 0.    ],\n",
      "       ...,\n",
      "       [0.    , 0.    , 2.125 , ..., 0.    , 0.    , 0.    ],\n",
      "       [0.    , 0.    , 2.125 , ..., 0.    , 0.    , 0.    ],\n",
      "       [0.    , 0.    , 2.125 , ..., 0.    , 0.    , 0.    ]],\n",
      "      dtype=float32), 'dataset_labels': array([[0, 0, 0, ..., 1, 0, 0],\n",
      "       [0, 0, 0, ..., 1, 0, 0],\n",
      "       [0, 0, 0, ..., 1, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 0, ..., 1, 0, 0],\n",
      "       [0, 0, 0, ..., 1, 0, 0],\n",
      "       [0, 0, 0, ..., 1, 0, 0]], dtype=int8)}}\n",
      "\n",
      "2018-10-19 18:24:55,217 - nn-real-hvd - INFO - testing files:  None\n",
      "\n",
      "2018-10-19 18:24:55,217 - nn-real-hvd - INFO - testing files:  None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logger.info('METRICS:  %s\\r\\n' % str(FLAGS))\n",
    "logger.info('training files:  %s\\r\\n' % str(DATA.train._dict))\n",
    "logger.info('validation files:  %s\\r\\n' % str(DATA.validation._dict))\n",
    "logger.info('testing files:  %s\\r\\n' % str(DATA.test._dict))        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-19 18:24:58,411 - nn-real-hvd - INFO - Building training graph\n",
      "2018-10-19 18:24:58,411 - nn-real-hvd - INFO - Building training graph\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/contrib/metrics/python/ops/histogram_ops.py:83: remove_squeezable_dimensions (from tensorflow.contrib.framework.python.framework.tensor_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please switch to remove_squeezable_dimensions from tf.confusion_matrix. Note that the order of the inputs and outputs of labels and predictions have also been switched.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-19 18:25:15,574 - tensorflow - WARNING - From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/contrib/metrics/python/ops/histogram_ops.py:83: remove_squeezable_dimensions (from tensorflow.contrib.framework.python.framework.tensor_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please switch to remove_squeezable_dimensions from tf.confusion_matrix. Note that the order of the inputs and outputs of labels and predictions have also been switched.\n",
      "2018-10-19 18:25:29,449 - nn-real-hvd - INFO - Graph building completed....\n",
      "2018-10-19 18:25:29,449 - nn-real-hvd - INFO - Graph building completed....\n",
      "2018-10-19 18:25:29,451 - nn-real-hvd - INFO - Creating session\n",
      "2018-10-19 18:25:29,451 - nn-real-hvd - INFO - Creating session\n"
     ]
    }
   ],
   "source": [
    "if FLAGS.eval:\n",
    "    if FLAGS.test_dir is None:\n",
    "        logger.error(\"eval requires data_dir to be specified\")\n",
    "        raise ValueError(\"eval requires data_dir to be specified\")\n",
    "    if hvd.size() > 1:\n",
    "        logger.error(\"Multi-GPU evaluation is not supported\")\n",
    "        raise ValueError(\"Multi-GPU evaluation is not supported\")\n",
    "    #evaluator = FeedForwardEvaluator(preprocessor, eval_func)\n",
    "    #logger.info(\"Building evaluation graph\")\n",
    "    #top1_op, top5_op, enqueue_ops = evaluator.evaluation_step(batch_size)\n",
    "else:    \n",
    "    nstep_per_epoch = nrecord // FLAGS.batch_size # if it is kwnow the total size: (FLAGS.batch_size * hvd.size())\n",
    "    # model_func = lambda features, labels, architecture, FLAGS: loss_func(features, labels, architecture, FLAGS) # inference_vgg(net, images, nlayer)\n",
    "    trainer = FeedForwardTrainer(loss_func, nstep_per_epoch=nstep_per_epoch)\n",
    "    logger.info(\"Building training graph\")    \n",
    "    train_ops, learning_rate_op, conf_mtx_op, accuracy_op, better_acc_op, auc_list_op, auc_mean_op, m_list_op, m_list_mean_op, total_loss_op, lloss_op, auc_pr_op, auc_pr_mean_op, auc_data_op = trainer.training_step(architecture, FLAGS)\n",
    "    logger.info(\"Graph building completed....\")\n",
    "\n",
    "logger.info(\"Creating session\")\n",
    "config = tf.ConfigProto(allow_soft_placement = True)\n",
    "config.intra_op_parallelism_threads = 1\n",
    "config.inter_op_parallelism_threads = 10\n",
    "config.gpu_options.force_gpu_compatible = True\n",
    "config.gpu_options.visible_device_list = str(hvd.local_rank())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.FeedForwardTrainer object at 0x7f1fa012ee48>\n"
     ]
    }
   ],
   "source": [
    "print(trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining summary (writer) and checkpoint (saver) files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-19 22:11:46,315 - nn-real-hvd - INFO - Initializing variables\n",
      "2018-10-19 22:11:46,315 - nn-real-hvd - INFO - Initializing variables\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /home/ubuntu/real_summaries4425-15ep_test/checkpoint-52834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-19 22:11:46,953 - tensorflow - INFO - Restoring parameters from /home/ubuntu/real_summaries4425-15ep_test/checkpoint-52834\n",
      "2018-10-19 22:11:47,093 - nn-real-hvd - INFO - Restored session from checkpoint /home/ubuntu/real_summaries4425-15ep_test/checkpoint-52834\n",
      "2018-10-19 22:11:47,093 - nn-real-hvd - INFO - Restored session from checkpoint /home/ubuntu/real_summaries4425-15ep_test/checkpoint-52834\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session(config=config)\n",
    "global_init, local_init = trainer.init()\n",
    "\n",
    "train_writer = None\n",
    "valid_writer = None\n",
    "saver = None\n",
    "summary_ops = None\n",
    "if hvd.rank() == 0 and FLAGS.logdir is not None:\n",
    "    train_writer = tf.summary.FileWriter(os.path.join(FLAGS.logdir), sess.graph)\n",
    "    valid_writer = tf.summary.FileWriter(os.path.join(FLAGS.logdir, 'valid'), graph=None)\n",
    "    summary_ops = tf.summary.merge_all()\n",
    "    last_summary_time = time.time()\n",
    "    saver = tf.train.Saver(keep_checkpoint_every_n_hours=1)\n",
    "    last_save_time = time.time()\n",
    "\n",
    "if not FLAGS.eval:\n",
    "    logger.info(\"Initializing variables\")    \n",
    "    sess.run([global_init, local_init])\n",
    "\n",
    "restored = False\n",
    "if hvd.rank() == 0 and saver is not None:\n",
    "    ckpt = tf.train.get_checkpoint_state(FLAGS.logdir)\n",
    "    checkpoint_file = os.path.join(FLAGS.logdir, \"checkpoint\")\n",
    "    if ckpt and ckpt.model_checkpoint_path:\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "        restored = True\n",
    "        logger.info(\"Restored session from checkpoint {}\".format(ckpt.model_checkpoint_path))\n",
    "    else:\n",
    "        if not os.path.exists(FLAGS.logdir):\n",
    "            os.mkdir(FLAGS.logdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running evaluation from a checkpoint file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is lefting:\n",
    "if FLAGS.eval:\n",
    "    if not restored:\n",
    "        logger.error(\"No checkpoint found for evaluation\")\n",
    "        raise ValueError(\"No checkpoint found for evaluation\")\n",
    "    else:\n",
    "        logger.info(\"Pre-filling input pipeline\")        \n",
    "        nstep = nrecord // FLAGS.test_batch_size #??\n",
    "        run_evaluation(nstep, sess, top1_op, top5_op, enqueue_ops)\n",
    "        sys.exit(0) #the following instructiones will not be  executed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# broadcast_global_variables from hvd\n",
    "trainer.sync(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-19 22:11:57,897 - nn-real-hvd - INFO - Writing summaries to /home/ubuntu/real_summaries4425-15ep_test\n",
      "2018-10-19 22:11:57,897 - nn-real-hvd - INFO - Writing summaries to /home/ubuntu/real_summaries4425-15ep_test\n",
      "2018-10-19 22:11:57,899 - nn-real-hvd - INFO - Training\n",
      "2018-10-19 22:11:57,899 - nn-real-hvd - INFO - Training\n",
      "2018-10-19 22:11:57,902 - nn-real-hvd - INFO -   Step; Epoch; time-per-record(sec);  batchtime/worker(sec);  Loss;   Learning Rate; Accuracy; better_acc\n",
      "2018-10-19 22:11:57,902 - nn-real-hvd - INFO -   Step; Epoch; time-per-record(sec);  batchtime/worker(sec);  Loss;   Learning Rate; Accuracy; better_acc\n"
     ]
    }
   ],
   "source": [
    "# Trying to restore for training:\n",
    "if hvd.rank() == 0 and not restored:\n",
    "    if saver is not None:\n",
    "        save_path = saver.save(sess, checkpoint_file, global_step=0)\n",
    "        print(\"Checkpoint written to\", save_path)\n",
    "\n",
    "logger.info(\"Writing summaries to {}\".format(FLAGS.logdir))\n",
    "logger.info(\"Training\")\n",
    "logger.info(\"  Step; Epoch; time-per-record(sec);  batchtime/worker(sec);  Loss;   Learning Rate; Accuracy; better_acc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feed_dict(tag, DATA, FLAGS):\n",
    "    \"\"\"Create the feed dictionary for mapping data onto placeholders in the graph.\"\"\"\n",
    "    if tag == 'batch':\n",
    "        features, targets, example_weights = DATA.train.next_random_batch(FLAGS.batch_size)        \n",
    "    elif tag == 'train':\n",
    "        features = DATA.train.orig.features\n",
    "        targets = DATA.train.orig.labels\n",
    "        example_weights = np.ones_like(targets.iloc[:, 1].values)\n",
    "    elif tag == 'valid':\n",
    "        features, targets, example_weights = DATA.validation.next_sequential_batch(FLAGS.valid_batch_size)\n",
    "    else:\n",
    "        features, targets, example_weights = DATA.test.next_sequential_batch(FLAGS.test_batch_size)\n",
    "\n",
    "    # features[:, :7] = targets\n",
    "    if tag == 'batch':\n",
    "        k_prob_input = 0.9  # 0.9  # .85  # .75  # 0.8  # 0.6\n",
    "        k_prob = FLAGS.dropout_keep\n",
    "        t_flag = True\n",
    "    else:\n",
    "        k_prob_input = 1.0\n",
    "        k_prob = 1.0\n",
    "        t_flag = False\n",
    "\n",
    "    # Change the python dictionary to an io-buffer for a better performance.\n",
    "    # See here:\n",
    "    # https://www.tensorflow.org/performance/performance_guide\n",
    "    feed_d = {\n",
    "        'features:0': features,\n",
    "        'targets:0': targets,\n",
    "        'example_weights:0': example_weights,        \n",
    "        'train_flag:0': t_flag,\n",
    "        #'epoch_flag:0': FLAGS.epoch_flag,\n",
    "        #'1_hidden/dropout/keep_proba:0': k_prob_input,\n",
    "        #'2_hidden/dropout/keep_proba:0': k_prob,\n",
    "        # '3_hidden/dropout/keep_proba:0': k_prob,\n",
    "        # '4_hidden/dropout/keep_proba:0': k_prob,\n",
    "        # '5_hidden/dropout/keep_proba:0': k_prob,\n",
    "        '9_softmax_linear/dropout/keep_proba:0': k_prob\n",
    "    }\n",
    "\t\n",
    "\t# for any tag:\n",
    "    if (FLAGS.n_hidden > 0) :\n",
    "        # print ('k_prob_input', k_prob_input, type(k_prob_input))\n",
    "        feed_d['1_hidden/dropout/keep_proba:0'] = k_prob_input\n",
    "        for hid_i in range(2, FLAGS.n_hidden+1):\n",
    "            feed_d['{:1d}_hidden/dropout/keep_proba:0'.format(hid_i)] = k_prob\n",
    "    # print('feed_d', feed_d)\n",
    "    \n",
    "    return feed_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_and_update(sess, local_init, feed_dict):\n",
    "    \"\"\"Reset the local variables and update the necessary update ops.\"\"\"\n",
    "    # sess.run(local_init) # this is necesary in each batch??check out the local variables!\n",
    "        \n",
    "    update_names_list = [\n",
    "        'metrics/auc/{:d}/hist_accumulate/update_op'.format(i)\n",
    "        for i in range(7)\n",
    "    ]\n",
    "\n",
    "    update_names_list.extend([\n",
    "        'metrics/m_measure/' + str(i) + str(j) + '/hist_accumulate/update_op'\n",
    "        for i in range(7) for j in range(7) if i != j\n",
    "    ])\n",
    "\n",
    "    sess.run(update_names_list, feed_dict=feed_dict)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_m_mtx(mtx):\n",
    "    \"\"\"Reshape the python list into a np array.\"\"\"\n",
    "    new_mtx = [0]\n",
    "    for i in range(6):\n",
    "        new_mtx.extend(mtx[i * 7:(i + 1) * 7])\n",
    "        new_mtx.append(0)\n",
    "    temp = np.array(new_mtx).reshape(7, 7)\n",
    "\n",
    "    return temp\n",
    "\n",
    "def calculate_better_acc(conf_mtx):\n",
    "    cfsum = conf_mtx.sum(axis=1, keepdims=True)\n",
    "    conf_mtx1 = np.divide(conf_mtx, cfsum, out=np.zeros_like(conf_mtx, dtype=np.float32), where=(cfsum!=0), dtype=np.float32)    \n",
    "    bett_acc = conf_mtx1.diagonal().mean()\n",
    "    return bett_acc\n",
    "\n",
    "def print_stats(name, conf_mtx, accuracy, better_acc, auc_list, auc_mean, m_list, m_list_mean, lloss, auc_pr, auc_pr_mean, loss):\n",
    "    \"\"\"Print to logger the given stats.\"\"\"        \n",
    "                \n",
    "    m_mtx = np.nan_to_num(reshape_m_mtx(m_list)) \n",
    "    auc_list = np.nan_to_num(auc_list)\n",
    "    conf_mtx = np.array(conf_mtx, dtype=int)\n",
    "            \n",
    "    stdout = 'Loss in ' + name +': {:.5f}\\n'.format(loss)        \n",
    "    stdout = stdout + ' Avg Log_Loss in ' + name +': {:.5f}\\n'.format(lloss)\n",
    "    stdout = stdout +  '{:s}:'.format(name) + ' (Silly) Global-ACC={:.5f}, Better ACC={:.5f},'.format(accuracy, better_acc) + \\\n",
    "        ' Avg M-Measure={:.4f},'.format(m_list_mean) + \\\n",
    "        ' Avg AUC_AOC={:.4f}'.format(auc_mean) + ' Avg AUC_PR={:.4f}\\n'.format(auc_pr_mean)\n",
    "    stdout = stdout + (';').join(['Total Confusion Matrix', 'Total M-Measure Matrix', 'Total AUC_AOC', 'Total AUC_PR\\n'])\n",
    "    for conf_row, row, auc, auc_pr in zip(conf_mtx, m_mtx, auc_list, auc_pr):\n",
    "        for conf_value in conf_row:\n",
    "            stdout = stdout + '{}'.format(conf_value) + ';'\n",
    "        stdout = stdout + ';'\n",
    "        for value in row:\n",
    "            stdout = stdout + '{:.4f}'.format(value) + ';'\n",
    "        stdout = stdout + ';{:.4f}'.format(auc) + ' ;{:.4f}'.format(auc_pr) + '\\n'\n",
    "    stdout = stdout + '---------------------------------------------------------------------'\n",
    "              \n",
    "    logger.info('METRICS each %s (secs):  %s\\r\\n' % (FLAGS.summary_interval, stdout))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for validation set:\n",
    "def acc_metrics_init(DATA):    \n",
    "    acc_conf_mtx=np.zeros((DATA.train.num_classes, DATA.train.num_classes))\n",
    "    acc_acc = 0\n",
    "    acc_auc_list = np.zeros((DATA.train.num_classes))\n",
    "    acc_m_mtx = np.zeros((DATA.train.num_classes, DATA.train.num_classes))\n",
    "    acc_loss = 0\n",
    "    acc_log_loss = 0\n",
    "    acc_auc_pr_list = np.zeros((DATA.train.num_classes))\n",
    "    epoch_metrics = list([acc_conf_mtx, acc_acc, acc_auc_list, acc_m_mtx, acc_loss, acc_log_loss, acc_auc_pr_list])\n",
    "    \n",
    "    return epoch_metrics\n",
    "\n",
    "def init_metrics(metrics):\n",
    "    metrics[0].fill(0)\n",
    "    metrics[1] = 0\n",
    "    metrics[2].fill(0)\n",
    "    metrics[3].fill(0)\n",
    "    metrics[4] = 0\n",
    "    metrics[5] = 0\n",
    "    metrics[6].fill(0)\n",
    "\n",
    "def batch_stats(global_stats, current_stats):\n",
    "    \"\"\"Print the given stats.\"\"\"\n",
    "    acc_conf_mtx, acc_acc, acc_auc_list, acc_m_mtx, acc_loss, acc_log_loss, acc_auc_pr_list = global_stats\n",
    "    conf_mtx, acc, auc_list, m_mtx_list, loss, log_loss, auc_pr_list = current_stats          \n",
    "    m_mtx = reshape_m_mtx(m_mtx_list)\n",
    "        \n",
    "    acc_conf_mtx += conf_mtx\n",
    "    acc_acc += acc\n",
    "    auc_list = np.nan_to_num(auc_list)\n",
    "    acc_auc_list += auc_list\n",
    "    m_mtx = np.nan_to_num(m_mtx)\n",
    "    acc_m_mtx += m_mtx\n",
    "    acc_loss += loss\n",
    "    acc_log_loss += log_loss\n",
    "    acc_auc_pr_list += auc_pr_list\n",
    "    \n",
    "    return (acc_conf_mtx, acc_acc, acc_auc_list, acc_m_mtx, acc_loss, acc_log_loss, acc_auc_pr_list)\n",
    "\n",
    "def batching_dataset(sess, writer, tag, DATA, FLAGS):\n",
    "    if tag =='valid':\n",
    "        batch_num = DATA.validation.total_num_batch(FLAGS.valid_batch_size) \n",
    "        \n",
    "    # metrics = acc_metrics_init(DATA)\n",
    "    sess.run(local_init)\n",
    "    start_time = datetime.now()\n",
    "    acc_conf_mtx=np.zeros((DATA.train.num_classes, DATA.train.num_classes))        \n",
    "    metrics = [0.0] * 4\n",
    "    for batch_i in range(batch_num):\n",
    "        feed = create_feed_dict(tag, DATA, FLAGS)     \n",
    "        reset_and_update(sess, local_init, feed)\n",
    "        summary, conf_mtx, accuracy, better_acc,  lloss, loss = sess.run([summary_ops, conf_mtx_op, accuracy_op, better_acc_op, lloss_op, total_loss_op], feed_dict=feed)\n",
    "        if (math.isnan(better_acc)):\n",
    "            better_acc = calculate_better_acc(conf_mtx)\n",
    "        acc_conf_mtx = np.add(acc_conf_mtx, conf_mtx)\n",
    "        metrics = np.add(metrics, np.array([accuracy, better_acc,  lloss, loss]))\n",
    "        writer.add_summary(summary, batch_i)\n",
    "        writer.flush()\n",
    "    \n",
    "    metrics[:] = [x / batch_num for x in metrics]\n",
    "    valid_time = datetime.now() - start_time\n",
    "    logger.info('%s - Time: %s' %(tag, valid_time))\n",
    "    return acc_conf_mtx, valid_time, metrics\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       step  epoch  batch_time      Loss   LogLoss  Accuracy  Better-Accuracy  \\\n",
      "0   38000.0    9.0    0.062677  0.125673  0.031193  0.961356         0.412511   \n",
      "1   39000.0    9.0    0.049822  0.137908  0.034519  0.958418         0.554432   \n",
      "2   40000.0    9.0    0.052973  0.138902  0.034519  0.959096         0.552760   \n",
      "3   38000.0    9.0    0.062677  0.125673  0.031193  0.961356         0.412511   \n",
      "4   39000.0    9.0    0.049822  0.137908  0.034519  0.958418         0.554432   \n",
      "5   40000.0    9.0    0.052973  0.138902  0.034519  0.959096         0.552760   \n",
      "6   38000.0    9.0    0.062677  0.125673  0.031193  0.961356         0.412511   \n",
      "7   39000.0    9.0    0.049822  0.137908  0.034519  0.958418         0.554432   \n",
      "8   40000.0    9.0    0.052973  0.138902  0.034519  0.959096         0.552760   \n",
      "9   38000.0    9.0    0.062677  0.125673  0.031193  0.961356         0.412511   \n",
      "10  39000.0    9.0    0.049822  0.137908  0.034519  0.958418         0.554432   \n",
      "11  40000.0    9.0    0.052973  0.138902  0.034519  0.959096         0.552760   \n",
      "12  43189.0   10.0    0.052643  0.130560  0.033848  0.958870         0.550585   \n",
      "13  47938.0   11.0    0.053030  0.129870  0.033918  0.959322         0.463558   \n",
      "14  52834.0   12.0    0.049358  0.124331  0.032018  0.964746         0.412246   \n",
      "15  38000.0    9.0    0.062677  0.125673  0.031193  0.961356         0.412511   \n",
      "16  39000.0    9.0    0.049822  0.137908  0.034519  0.958418         0.554432   \n",
      "17  40000.0    9.0    0.052973  0.138902  0.034519  0.959096         0.552760   \n",
      "18  38000.0    9.0    0.062677  0.125673  0.031193  0.961356         0.412511   \n",
      "19  39000.0    9.0    0.049822  0.137908  0.034519  0.958418         0.554432   \n",
      "20  40000.0    9.0    0.052973  0.138902  0.034519  0.959096         0.552760   \n",
      "21  43189.0   10.0    0.052643  0.130560  0.033848  0.958870         0.550585   \n",
      "22  47938.0   11.0    0.053030  0.129870  0.033918  0.959322         0.463558   \n",
      "23  52834.0   12.0    0.049358  0.124331  0.032018  0.964746         0.412246   \n",
      "\n",
      "    M-Measure Mean  AUC_AOC Mean  AUC_PR Mean  \n",
      "0         0.879605      0.945442     0.000000  \n",
      "1         0.861049      0.931102     0.499114  \n",
      "2         0.861049      0.931102     0.499114  \n",
      "3         0.879605      0.945442     0.000000  \n",
      "4         0.861049      0.931102     0.499114  \n",
      "5         0.861049      0.931102     0.499114  \n",
      "6         0.879605      0.945442     0.000000  \n",
      "7         0.861049      0.931102     0.499114  \n",
      "8         0.861049      0.931102     0.499114  \n",
      "9         0.879605      0.945442     0.000000  \n",
      "10        0.861049      0.931102     0.499114  \n",
      "11        0.861049      0.931102     0.499114  \n",
      "12        0.863143      0.932796     0.500585  \n",
      "13        0.865680      0.935291     0.504843  \n",
      "14        0.867140      0.934568     0.504723  \n",
      "15        0.879605      0.945442     0.000000  \n",
      "16        0.861049      0.931102     0.499114  \n",
      "17        0.861049      0.931102     0.499114  \n",
      "18        0.879605      0.945442     0.000000  \n",
      "19        0.861049      0.931102     0.499114  \n",
      "20        0.861049      0.931102     0.499114  \n",
      "21        0.863143      0.932796     0.500585  \n",
      "22        0.865680      0.935291     0.504843  \n",
      "23        0.867140      0.934568     0.504723  \n",
      "     step  epoch                 batch_time      Loss   LogLoss  Accuracy  \\\n",
      "0   38000      9  0 days 00:03:27.207913000  0.161047  0.041680  0.951539   \n",
      "1   39000      9  0 days 00:03:22.380815000  0.161608  0.041749  0.951411   \n",
      "2   40000      9  0 days 00:03:20.898926000  0.161099  0.041673  0.951375   \n",
      "3   38000      9  0 days 00:03:27.207913000  0.161047  0.041680  0.951539   \n",
      "4   39000      9  0 days 00:03:22.380815000  0.161608  0.041749  0.951411   \n",
      "5   40000      9  0 days 00:03:20.898926000  0.161099  0.041673  0.951375   \n",
      "6   38000      9  0 days 00:03:27.207913000  0.161047  0.041680  0.951539   \n",
      "7   39000      9  0 days 00:03:22.380815000  0.161608  0.041749  0.951411   \n",
      "8   40000      9  0 days 00:03:20.898926000  0.161099  0.041673  0.951375   \n",
      "9   38000      9  0 days 00:03:27.207913000  0.161047  0.041680  0.951539   \n",
      "10  39000      9  0 days 00:03:22.380815000  0.161608  0.041749  0.951411   \n",
      "11  40000      9  0 days 00:03:20.898926000  0.161099  0.041673  0.951375   \n",
      "12  42318      9     0 days 00:03:25.902106  0.160937  0.041647  0.951567   \n",
      "13  47020     10     0 days 00:03:20.324956  0.161317  0.041753  0.951691   \n",
      "14  51722     11     0 days 00:03:21.247245  0.160180  0.041499  0.951297   \n",
      "15  38000      9  0 days 00:03:27.207913000  0.161047  0.041680  0.951539   \n",
      "16  39000      9  0 days 00:03:22.380815000  0.161608  0.041749  0.951411   \n",
      "17  40000      9  0 days 00:03:20.898926000  0.161099  0.041673  0.951375   \n",
      "18  38000      9  0 days 00:03:27.207913000  0.161047  0.041680  0.951539   \n",
      "19  39000      9  0 days 00:03:22.380815000  0.161608  0.041749  0.951411   \n",
      "20  40000      9  0 days 00:03:20.898926000  0.161099  0.041673  0.951375   \n",
      "21  42318      9     0 days 00:03:25.902106  0.160937  0.041647  0.951567   \n",
      "22  47020     10     0 days 00:03:20.324956  0.161317  0.041753  0.951691   \n",
      "23  51722     11     0 days 00:03:21.247245  0.160180  0.041499  0.951297   \n",
      "\n",
      "    Better-Accuracy  \n",
      "0          0.503958  \n",
      "1          0.504451  \n",
      "2          0.505558  \n",
      "3          0.503958  \n",
      "4          0.504451  \n",
      "5          0.505558  \n",
      "6          0.503958  \n",
      "7          0.504451  \n",
      "8          0.505558  \n",
      "9          0.503958  \n",
      "10         0.504451  \n",
      "11         0.505558  \n",
      "12         0.504538  \n",
      "13         0.501788  \n",
      "14         0.509064  \n",
      "15         0.503958  \n",
      "16         0.504451  \n",
      "17         0.505558  \n",
      "18         0.503958  \n",
      "19         0.504451  \n",
      "20         0.505558  \n",
      "21         0.504538  \n",
      "22         0.501788  \n",
      "23         0.509064  \n"
     ]
    }
   ],
   "source": [
    "if hvd.rank() == 0:\n",
    "    if not FLAGS.eval:\n",
    "        dtype = ['step','epoch','batch_time','Loss','LogLoss','Accuracy','Better-Accuracy','M-Measure Mean','AUC_AOC Mean','AUC_PR Mean']\n",
    "        train_file = os.path.join(FLAGS.logdir, FLAGS.name + \"_train.csv\")\n",
    "        valid_file = os.path.join(FLAGS.logdir, FLAGS.name + \"_valid.csv\")        \n",
    "        \n",
    "        if not Path(train_file).exists():\n",
    "            df_train = pd.DataFrame(columns=dtype)                \n",
    "            df_train.to_csv(train_file, sep=';', index=False)\n",
    "        else:\n",
    "            df_train = pd.read_csv(train_file, sep=';')\n",
    "\n",
    "        if not Path(valid_file).exists():\n",
    "            df_valid = pd.DataFrame(columns=dtype[:7])\n",
    "            df_valid.to_csv(valid_file, sep=';', index=False)            \n",
    "        else:\n",
    "            df_valid = pd.read_csv(valid_file, sep=';')\n",
    "        \n",
    "        print(df_train)\n",
    "        print(df_valid)\n",
    "        \n",
    "    else:  # validation set:\n",
    "        dtype = ['NN_name', 'NN_Number','Total Epochs', 'Execute Epochs', 'Total Training Time', 'Loss','LogLoss','Accuracy','Better-Accuracy','M-Measure Mean','AUC_AOC Mean','AUC_PR Mean']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-19 22:12:23,206 - nn-real-hvd - INFO -  52900;    12; 86829.9;   0.051; 0.11779; 0.09852; 0.96362; 0.41225\n",
      "2018-10-19 22:12:23,206 - nn-real-hvd - INFO -  52900;    12; 86829.9;   0.051; 0.11779; 0.09852; 0.96362; 0.41225\n",
      "2018-10-19 22:12:28,364 - nn-real-hvd - INFO -  53000;    12; 89719.4;   0.049; 0.13325; 0.09852; 0.95729; 0.41225\n",
      "2018-10-19 22:12:28,364 - nn-real-hvd - INFO -  53000;    12; 89719.4;   0.049; 0.13325; 0.09852; 0.95729; 0.41225\n",
      "2018-10-19 22:12:33,494 - nn-real-hvd - INFO -  53100;    12; 86775.1;   0.051; 0.12973; 0.09851; 0.96294; 0.57054\n",
      "2018-10-19 22:12:33,494 - nn-real-hvd - INFO -  53100;    12; 86775.1;   0.051; 0.12973; 0.09851; 0.96294; 0.57054\n",
      "2018-10-19 22:12:38,635 - nn-real-hvd - INFO -  53200;    12; 80561.7;   0.055; 0.13652; 0.09851; 0.95684; 0.41225\n",
      "2018-10-19 22:12:38,635 - nn-real-hvd - INFO -  53200;    12; 80561.7;   0.055; 0.13652; 0.09851; 0.95684; 0.41225\n",
      "2018-10-19 22:12:43,765 - nn-real-hvd - INFO -  53300;    12; 89985.1;   0.049; 0.13645; 0.09851; 0.95774; 0.40598\n",
      "2018-10-19 22:12:43,765 - nn-real-hvd - INFO -  53300;    12; 89985.1;   0.049; 0.13645; 0.09851; 0.95774; 0.40598\n",
      "2018-10-19 22:12:48,894 - nn-real-hvd - INFO -  53400;    12; 86825.0;   0.051; 0.12709; 0.09851; 0.96158; 0.56879\n",
      "2018-10-19 22:12:48,894 - nn-real-hvd - INFO -  53400;    12; 86825.0;   0.051; 0.12709; 0.09851; 0.96158; 0.56879\n",
      "2018-10-19 22:12:54,077 - nn-real-hvd - INFO -  53500;    12; 87807.6;   0.050; 0.12769; 0.09850; 0.95977; 0.48533\n",
      "2018-10-19 22:12:54,077 - nn-real-hvd - INFO -  53500;    12; 87807.6;   0.050; 0.12769; 0.09850; 0.95977; 0.48533\n",
      "2018-10-19 22:12:59,202 - nn-real-hvd - INFO -  53600;    12; 86900.2;   0.051; 0.11972; 0.09850; 0.96294; 0.47587\n",
      "2018-10-19 22:12:59,202 - nn-real-hvd - INFO -  53600;    12; 86900.2;   0.051; 0.11972; 0.09850; 0.96294; 0.47587\n",
      "2018-10-19 22:13:04,357 - nn-real-hvd - INFO -  53700;    12; 84320.9;   0.052; 0.13378; 0.09850; 0.96226; 0.52894\n",
      "2018-10-19 22:13:04,357 - nn-real-hvd - INFO -  53700;    12; 84320.9;   0.052; 0.13378; 0.09850; 0.96226; 0.52894\n",
      "2018-10-19 22:13:09,565 - nn-real-hvd - INFO -  53800;    12; 89542.3;   0.049; 0.13857; 0.09849; 0.95955; 0.41225\n",
      "2018-10-19 22:13:09,565 - nn-real-hvd - INFO -  53800;    12; 89542.3;   0.049; 0.13857; 0.09849; 0.95955; 0.41225\n",
      "2018-10-19 22:13:14,743 - nn-real-hvd - INFO -  53900;    12; 88157.9;   0.050; 0.12205; 0.09849; 0.96045; 0.44595\n",
      "2018-10-19 22:13:14,743 - nn-real-hvd - INFO -  53900;    12; 88157.9;   0.050; 0.12205; 0.09849; 0.96045; 0.44595\n",
      "2018-10-19 22:13:19,879 - nn-real-hvd - INFO -  54000;    12; 86918.1;   0.051; 0.11641; 0.09849; 0.96271; 0.59716\n",
      "2018-10-19 22:13:19,879 - nn-real-hvd - INFO -  54000;    12; 86918.1;   0.051; 0.11641; 0.09849; 0.96271; 0.59716\n",
      "2018-10-19 22:13:25,033 - nn-real-hvd - INFO -  54100;    12; 86956.4;   0.051; 0.12680; 0.09849; 0.96113; 0.44248\n",
      "2018-10-19 22:13:25,033 - nn-real-hvd - INFO -  54100;    12; 86956.4;   0.051; 0.12680; 0.09849; 0.96113; 0.44248\n",
      "2018-10-19 22:13:30,159 - nn-real-hvd - INFO -  54200;    12; 89799.2;   0.049; 0.12441; 0.09848; 0.96203; 0.53517\n",
      "2018-10-19 22:13:30,159 - nn-real-hvd - INFO -  54200;    12; 89799.2;   0.049; 0.12441; 0.09848; 0.96203; 0.53517\n",
      "2018-10-19 22:13:35,390 - nn-real-hvd - INFO -  54300;    12; 82097.6;   0.054; 0.12393; 0.09848; 0.96384; 0.41225\n",
      "2018-10-19 22:13:35,390 - nn-real-hvd - INFO -  54300;    12; 82097.6;   0.054; 0.12393; 0.09848; 0.96384; 0.41225\n",
      "2018-10-19 22:13:40,557 - nn-real-hvd - INFO -  54400;    12; 85942.5;   0.051; 0.12226; 0.09848; 0.96475; 0.58536\n",
      "2018-10-19 22:13:40,557 - nn-real-hvd - INFO -  54400;    12; 85942.5;   0.051; 0.12226; 0.09848; 0.96475; 0.58536\n",
      "2018-10-19 22:13:45,670 - nn-real-hvd - INFO -  54500;    12; 88438.1;   0.050; 0.12456; 0.09848; 0.96090; 0.55190\n",
      "2018-10-19 22:13:45,670 - nn-real-hvd - INFO -  54500;    12; 88438.1;   0.050; 0.12456; 0.09848; 0.96090; 0.55190\n",
      "2018-10-19 22:13:50,865 - nn-real-hvd - INFO -  54600;    12; 86730.4;   0.051; 0.12294; 0.09847; 0.96226; 0.47626\n",
      "2018-10-19 22:13:50,865 - nn-real-hvd - INFO -  54600;    12; 86730.4;   0.051; 0.12294; 0.09847; 0.96226; 0.47626\n",
      "2018-10-19 22:13:55,972 - nn-real-hvd - INFO -  54700;    12; 88028.7;   0.050; 0.14082; 0.09847; 0.95774; 0.57633\n",
      "2018-10-19 22:13:55,972 - nn-real-hvd - INFO -  54700;    12; 88028.7;   0.050; 0.14082; 0.09847; 0.95774; 0.57633\n",
      "2018-10-19 22:14:01,144 - nn-real-hvd - INFO -  54800;    12; 88940.7;   0.050; 0.13455; 0.09847; 0.95571; 0.41771\n",
      "2018-10-19 22:14:01,144 - nn-real-hvd - INFO -  54800;    12; 88940.7;   0.050; 0.13455; 0.09847; 0.95571; 0.41771\n",
      "2018-10-19 22:14:06,296 - nn-real-hvd - INFO -  54900;    12; 87038.8;   0.051; 0.12526; 0.09846; 0.96226; 0.49714\n",
      "2018-10-19 22:14:06,296 - nn-real-hvd - INFO -  54900;    12; 87038.8;   0.051; 0.12526; 0.09846; 0.96226; 0.49714\n",
      "2018-10-19 22:14:11,412 - nn-real-hvd - INFO -  55000;    12; 86324.6;   0.051; 0.13408; 0.09846; 0.96136; 0.45631\n",
      "2018-10-19 22:14:11,412 - nn-real-hvd - INFO -  55000;    12; 86324.6;   0.051; 0.13408; 0.09846; 0.96136; 0.45631\n",
      "2018-10-19 22:14:16,606 - nn-real-hvd - INFO -  55100;    12; 91168.9;   0.049; 0.12053; 0.09846; 0.96429; 0.52796\n",
      "2018-10-19 22:14:16,606 - nn-real-hvd - INFO -  55100;    12; 91168.9;   0.049; 0.12053; 0.09846; 0.96429; 0.52796\n",
      "2018-10-19 22:14:21,696 - nn-real-hvd - INFO -  55200;    12; 87353.8;   0.051; 0.12871; 0.09846; 0.96407; 0.59041\n",
      "2018-10-19 22:14:21,696 - nn-real-hvd - INFO -  55200;    12; 87353.8;   0.051; 0.12871; 0.09846; 0.96407; 0.59041\n",
      "2018-10-19 22:14:26,811 - nn-real-hvd - INFO -  55300;    12; 83328.3;   0.053; 0.12945; 0.09845; 0.96113; 0.41218\n",
      "2018-10-19 22:14:26,811 - nn-real-hvd - INFO -  55300;    12; 83328.3;   0.053; 0.12945; 0.09845; 0.96113; 0.41218\n",
      "2018-10-19 22:14:32,014 - nn-real-hvd - INFO -  55400;    12; 83941.5;   0.053; 0.13708; 0.09845; 0.95910; 0.41225\n",
      "2018-10-19 22:14:32,014 - nn-real-hvd - INFO -  55400;    12; 83941.5;   0.053; 0.13708; 0.09845; 0.95910; 0.41225\n",
      "2018-10-19 22:14:37,122 - nn-real-hvd - INFO -  55500;    12; 85245.4;   0.052; 0.13009; 0.09845; 0.95864; 0.45808\n",
      "2018-10-19 22:14:37,122 - nn-real-hvd - INFO -  55500;    12; 85245.4;   0.052; 0.13009; 0.09845; 0.95864; 0.45808\n",
      "2018-10-19 22:14:42,225 - nn-real-hvd - INFO -  55600;    12; 84075.7;   0.053; 0.12401; 0.09845; 0.96068; 0.44367\n",
      "2018-10-19 22:14:42,225 - nn-real-hvd - INFO -  55600;    12; 84075.7;   0.053; 0.12401; 0.09845; 0.96068; 0.44367\n",
      "2018-10-19 22:14:47,313 - nn-real-hvd - INFO -  55700;    12; 85400.7;   0.052; 0.13178; 0.09844; 0.95887; 0.40371\n",
      "2018-10-19 22:14:47,313 - nn-real-hvd - INFO -  55700;    12; 85400.7;   0.052; 0.13178; 0.09844; 0.95887; 0.40371\n",
      "2018-10-19 22:14:52,423 - nn-real-hvd - INFO -  55800;    12; 84625.4;   0.052; 0.11116; 0.09844; 0.96407; 0.59688\n",
      "2018-10-19 22:14:52,423 - nn-real-hvd - INFO -  55800;    12; 84625.4;   0.052; 0.11116; 0.09844; 0.96407; 0.59688\n",
      "2018-10-19 22:14:57,580 - nn-real-hvd - INFO -  55900;    12; 85321.4;   0.052; 0.13026; 0.09844; 0.95864; 0.48120\n",
      "2018-10-19 22:14:57,580 - nn-real-hvd - INFO -  55900;    12; 85321.4;   0.052; 0.13026; 0.09844; 0.95864; 0.48120\n",
      "2018-10-19 22:15:02,724 - nn-real-hvd - INFO -  56000;    12; 87178.2;   0.051; 0.12617; 0.09843; 0.96113; 0.40926\n",
      "2018-10-19 22:15:02,724 - nn-real-hvd - INFO -  56000;    12; 87178.2;   0.051; 0.12617; 0.09843; 0.96113; 0.40926\n",
      "2018-10-19 22:15:07,822 - nn-real-hvd - INFO -  56100;    12; 85738.8;   0.052; 0.13072; 0.09843; 0.95955; 0.39405\n",
      "2018-10-19 22:15:07,822 - nn-real-hvd - INFO -  56100;    12; 85738.8;   0.052; 0.13072; 0.09843; 0.95955; 0.39405\n",
      "2018-10-19 22:15:12,987 - nn-real-hvd - INFO -  56200;    12; 82766.8;   0.053; 0.11338; 0.09843; 0.96633; 0.44335\n",
      "2018-10-19 22:15:12,987 - nn-real-hvd - INFO -  56200;    12; 82766.8;   0.053; 0.11338; 0.09843; 0.96633; 0.44335\n",
      "2018-10-19 22:15:18,085 - nn-real-hvd - INFO -  56300;    12; 87183.9;   0.051; 0.13927; 0.09843; 0.95774; 0.41225\n",
      "2018-10-19 22:15:18,085 - nn-real-hvd - INFO -  56300;    12; 87183.9;   0.051; 0.13927; 0.09843; 0.95774; 0.41225\n",
      "2018-10-19 22:15:23,162 - nn-real-hvd - INFO -  56400;    12; 85412.1;   0.052; 0.12668; 0.09842; 0.96520; 0.41225\n",
      "2018-10-19 22:15:23,162 - nn-real-hvd - INFO -  56400;    12; 85412.1;   0.052; 0.12668; 0.09842; 0.96520; 0.41225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-19 22:18:52,628 - nn-real-hvd - INFO - valid - Time: 0:03:28.007130\n",
      "2018-10-19 22:18:52,628 - nn-real-hvd - INFO - valid - Time: 0:03:28.007130\n",
      "2018-10-19 22:18:52,630 - nn-real-hvd - INFO - Confusion Matrix in Validation 56424; 12: [[    313     466      12     467   28494     720       0]\n",
      " [      0   38484    2777    4957   80121     521       0]\n",
      " [      0   15747    3722    6054    5613     168       0]\n",
      " [      0     546    2087   32108     768    2572       0]\n",
      " [      0   26312    1567    6107 3723153    1510       0]\n",
      " [      0     273      76    5953     412   26473       0]\n",
      " [      0       2       0      55       4     871     854]]\n",
      "2018-10-19 22:18:52,630 - nn-real-hvd - INFO - Confusion Matrix in Validation 56424; 12: [[    313     466      12     467   28494     720       0]\n",
      " [      0   38484    2777    4957   80121     521       0]\n",
      " [      0   15747    3722    6054    5613     168       0]\n",
      " [      0     546    2087   32108     768    2572       0]\n",
      " [      0   26312    1567    6107 3723153    1510       0]\n",
      " [      0     273      76    5953     412   26473       0]\n",
      " [      0       2       0      55       4     871     854]]\n",
      "2018-10-19 22:18:52,634 - nn-real-hvd - INFO - (step, epoch, loss, accuracy, better accuracy) in Validation:  56424;    12; 0.16093; 0.95159; 0.50455\n",
      "2018-10-19 22:18:52,634 - nn-real-hvd - INFO - (step, epoch, loss, accuracy, better accuracy) in Validation:  56424;    12; 0.16093; 0.95159; 0.50455\n",
      "2018-10-19 22:18:56,338 - nn-real-hvd - INFO -  56500;    13; 87900.7;   0.050; 0.13717; 0.09842; 0.95932; 0.48382\n",
      "2018-10-19 22:18:56,338 - nn-real-hvd - INFO -  56500;    13; 87900.7;   0.050; 0.13717; 0.09842; 0.95932; 0.48382\n",
      "2018-10-19 22:19:01,262 - nn-real-hvd - INFO -  56600;    13; 94201.2;   0.047; 0.12604; 0.09842; 0.96045; 0.41225\n",
      "2018-10-19 22:19:01,262 - nn-real-hvd - INFO -  56600;    13; 94201.2;   0.047; 0.12604; 0.09842; 0.96045; 0.41225\n",
      "2018-10-19 22:19:06,141 - nn-real-hvd - INFO -  56700;    13; 91081.2;   0.049; 0.12339; 0.09841; 0.96136; 0.48246\n",
      "2018-10-19 22:19:06,141 - nn-real-hvd - INFO -  56700;    13; 91081.2;   0.049; 0.12339; 0.09841; 0.96136; 0.48246\n",
      "2018-10-19 22:19:11,016 - nn-real-hvd - INFO -  56800;    13; 94054.1;   0.047; 0.11675; 0.09841; 0.96271; 0.42077\n",
      "2018-10-19 22:19:11,016 - nn-real-hvd - INFO -  56800;    13; 94054.1;   0.047; 0.11675; 0.09841; 0.96271; 0.42077\n",
      "2018-10-19 22:19:15,967 - nn-real-hvd - INFO -  56900;    13; 90697.1;   0.049; 0.13255; 0.09841; 0.96203; 0.41225\n",
      "2018-10-19 22:19:15,967 - nn-real-hvd - INFO -  56900;    13; 90697.1;   0.049; 0.13255; 0.09841; 0.96203; 0.41225\n",
      "2018-10-19 22:19:23,169 - nn-real-hvd - INFO - METRICS each 450 (secs):  Loss in ---Training in Summary---: 0.13719\n",
      " Avg Log_Loss in ---Training in Summary---: 0.03544\n",
      "---Training in Summary---: (Silly) Global-ACC=0.95729, Better ACC=0.45574, Avg M-Measure=0.8672, Avg AUC_AOC=0.9351 Avg AUC_PR=0.5055\n",
      "Total Confusion Matrix;Total M-Measure Matrix;Total AUC_AOC;Total AUC_PR\n",
      "1;0;0;1;16;0;0;;0.0000;0.7997;0.7604;0.6755;0.8460;0.4145;0.2869;;0.7947 ;0.0457\n",
      "0;24;3;2;97;0;0;;0.8693;0.0000;0.3372;0.7763;0.9181;0.8417;0.8998;;0.9097 ;0.3300\n",
      "0;10;3;3;8;0;0;;0.9746;0.8189;0.0000;0.8083;0.9946;0.9527;0.9934;;0.9869 ;0.2356\n",
      "0;0;1;22;2;1;0;;0.9870;0.9868;0.9306;0.0000;0.9981;0.8783;0.8911;;0.9962 ;0.7572\n",
      "0;22;1;7;4163;1;0;;0.7576;0.9135;0.9954;0.9992;0.0000;0.9990;0.9989;;0.9323 ;0.9863\n",
      "0;1;1;8;2;22;0;;0.9759;0.9899;0.9756;0.8905;0.9952;0.0000;0.7119;;0.9918 ;0.7346\n",
      "0;0;0;1;0;1;1;;0.9611;0.9701;0.9696;0.9149;0.9704;0.7943;0.0000;;0.9340 ;0.4490\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "2018-10-19 22:19:23,169 - nn-real-hvd - INFO - METRICS each 450 (secs):  Loss in ---Training in Summary---: 0.13719\n",
      " Avg Log_Loss in ---Training in Summary---: 0.03544\n",
      "---Training in Summary---: (Silly) Global-ACC=0.95729, Better ACC=0.45574, Avg M-Measure=0.8672, Avg AUC_AOC=0.9351 Avg AUC_PR=0.5055\n",
      "Total Confusion Matrix;Total M-Measure Matrix;Total AUC_AOC;Total AUC_PR\n",
      "1;0;0;1;16;0;0;;0.0000;0.7997;0.7604;0.6755;0.8460;0.4145;0.2869;;0.7947 ;0.0457\n",
      "0;24;3;2;97;0;0;;0.8693;0.0000;0.3372;0.7763;0.9181;0.8417;0.8998;;0.9097 ;0.3300\n",
      "0;10;3;3;8;0;0;;0.9746;0.8189;0.0000;0.8083;0.9946;0.9527;0.9934;;0.9869 ;0.2356\n",
      "0;0;1;22;2;1;0;;0.9870;0.9868;0.9306;0.0000;0.9981;0.8783;0.8911;;0.9962 ;0.7572\n",
      "0;22;1;7;4163;1;0;;0.7576;0.9135;0.9954;0.9992;0.0000;0.9990;0.9989;;0.9323 ;0.9863\n",
      "0;1;1;8;2;22;0;;0.9759;0.9899;0.9756;0.8905;0.9952;0.0000;0.7119;;0.9918 ;0.7346\n",
      "0;0;0;1;0;1;1;;0.9611;0.9701;0.9696;0.9149;0.9704;0.7943;0.0000;;0.9340 ;0.4490\n",
      "---------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint written to /home/ubuntu/real_summaries4425-15ep_test/checkpoint-56908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-19 22:19:29,132 - nn-real-hvd - INFO -  57000;    13; 80598.0;   0.055; 0.13207; 0.09841; 0.96113; 0.50578\n",
      "2018-10-19 22:19:29,132 - nn-real-hvd - INFO -  57000;    13; 80598.0;   0.055; 0.13207; 0.09841; 0.96113; 0.50578\n",
      "2018-10-19 22:19:34,327 - nn-real-hvd - INFO -  57100;    13; 87729.1;   0.050; 0.12688; 0.09840; 0.96113; 0.41434\n",
      "2018-10-19 22:19:34,327 - nn-real-hvd - INFO -  57100;    13; 87729.1;   0.050; 0.12688; 0.09840; 0.96113; 0.41434\n",
      "2018-10-19 22:19:39,496 - nn-real-hvd - INFO -  57200;    13; 84974.1;   0.052; 0.12016; 0.09840; 0.96249; 0.48301\n",
      "2018-10-19 22:19:39,496 - nn-real-hvd - INFO -  57200;    13; 84974.1;   0.052; 0.12016; 0.09840; 0.96249; 0.48301\n",
      "2018-10-19 22:19:44,687 - nn-real-hvd - INFO -  57300;    13; 85609.9;   0.052; 0.11747; 0.09840; 0.96520; 0.42000\n",
      "2018-10-19 22:19:44,687 - nn-real-hvd - INFO -  57300;    13; 85609.9;   0.052; 0.11747; 0.09840; 0.96520; 0.42000\n",
      "2018-10-19 22:19:49,864 - nn-real-hvd - INFO -  57400;    13; 85973.1;   0.051; 0.12063; 0.09840; 0.96271; 0.43313\n",
      "2018-10-19 22:19:49,864 - nn-real-hvd - INFO -  57400;    13; 85973.1;   0.051; 0.12063; 0.09840; 0.96271; 0.43313\n",
      "2018-10-19 22:19:55,047 - nn-real-hvd - INFO -  57500;    13; 85024.3;   0.052; 0.11472; 0.09839; 0.96339; 0.47104\n",
      "2018-10-19 22:19:55,047 - nn-real-hvd - INFO -  57500;    13; 85024.3;   0.052; 0.11472; 0.09839; 0.96339; 0.47104\n",
      "2018-10-19 22:20:00,189 - nn-real-hvd - INFO -  57600;    13; 82022.1;   0.054; 0.10912; 0.09839; 0.96542; 0.40228\n",
      "2018-10-19 22:20:00,189 - nn-real-hvd - INFO -  57600;    13; 82022.1;   0.054; 0.10912; 0.09839; 0.96542; 0.40228\n",
      "2018-10-19 22:20:05,404 - nn-real-hvd - INFO -  57700;    13; 87559.0;   0.051; 0.12468; 0.09839; 0.96203; 0.55881\n",
      "2018-10-19 22:20:05,404 - nn-real-hvd - INFO -  57700;    13; 87559.0;   0.051; 0.12468; 0.09839; 0.96203; 0.55881\n",
      "2018-10-19 22:20:10,580 - nn-real-hvd - INFO -  57800;    13; 85478.2;   0.052; 0.12202; 0.09838; 0.96429; 0.52665\n",
      "2018-10-19 22:20:10,580 - nn-real-hvd - INFO -  57800;    13; 85478.2;   0.052; 0.12202; 0.09838; 0.96429; 0.52665\n",
      "2018-10-19 22:20:15,810 - nn-real-hvd - INFO -  57900;    13; 87934.0;   0.050; 0.12930; 0.09838; 0.96136; 0.58405\n",
      "2018-10-19 22:20:15,810 - nn-real-hvd - INFO -  57900;    13; 87934.0;   0.050; 0.12930; 0.09838; 0.96136; 0.58405\n",
      "2018-10-19 22:20:21,011 - nn-real-hvd - INFO -  58000;    13; 86146.7;   0.051; 0.14069; 0.09838; 0.95842; 0.45913\n",
      "2018-10-19 22:20:21,011 - nn-real-hvd - INFO -  58000;    13; 86146.7;   0.051; 0.14069; 0.09838; 0.95842; 0.45913\n",
      "2018-10-19 22:20:26,189 - nn-real-hvd - INFO -  58100;    13; 87338.6;   0.051; 0.12838; 0.09838; 0.95977; 0.50779\n",
      "2018-10-19 22:20:26,189 - nn-real-hvd - INFO -  58100;    13; 87338.6;   0.051; 0.12838; 0.09838; 0.95977; 0.50779\n",
      "2018-10-19 22:20:31,391 - nn-real-hvd - INFO -  58200;    13; 86015.8;   0.051; 0.13850; 0.09837; 0.95819; 0.43435\n",
      "2018-10-19 22:20:31,391 - nn-real-hvd - INFO -  58200;    13; 86015.8;   0.051; 0.13850; 0.09837; 0.95819; 0.43435\n",
      "2018-10-19 22:20:36,628 - nn-real-hvd - INFO -  58300;    13; 88433.9;   0.050; 0.11790; 0.09837; 0.96226; 0.43877\n",
      "2018-10-19 22:20:36,628 - nn-real-hvd - INFO -  58300;    13; 88433.9;   0.050; 0.11790; 0.09837; 0.96226; 0.43877\n",
      "2018-10-19 22:20:41,786 - nn-real-hvd - INFO -  58400;    13; 91570.1;   0.048; 0.10791; 0.09837; 0.96949; 0.45574\n",
      "2018-10-19 22:20:41,786 - nn-real-hvd - INFO -  58400;    13; 91570.1;   0.048; 0.10791; 0.09837; 0.96949; 0.45574\n",
      "2018-10-19 22:20:46,986 - nn-real-hvd - INFO -  58500;    13; 85408.6;   0.052; 0.12432; 0.09837; 0.96181; 0.45574\n",
      "2018-10-19 22:20:46,986 - nn-real-hvd - INFO -  58500;    13; 85408.6;   0.052; 0.12432; 0.09837; 0.96181; 0.45574\n",
      "2018-10-19 22:20:52,133 - nn-real-hvd - INFO -  58600;    13; 86816.0;   0.051; 0.13882; 0.09836; 0.95684; 0.56515\n",
      "2018-10-19 22:20:52,133 - nn-real-hvd - INFO -  58600;    13; 86816.0;   0.051; 0.13882; 0.09836; 0.95684; 0.56515\n",
      "2018-10-19 22:20:57,286 - nn-real-hvd - INFO -  58700;    13; 87496.7;   0.051; 0.13516; 0.09836; 0.96023; 0.42418\n",
      "2018-10-19 22:20:57,286 - nn-real-hvd - INFO -  58700;    13; 87496.7;   0.051; 0.13516; 0.09836; 0.96023; 0.42418\n",
      "2018-10-19 22:21:02,408 - nn-real-hvd - INFO -  58800;    13; 87296.7;   0.051; 0.13278; 0.09836; 0.95638; 0.45574\n",
      "2018-10-19 22:21:02,408 - nn-real-hvd - INFO -  58800;    13; 87296.7;   0.051; 0.13278; 0.09836; 0.95638; 0.45574\n",
      "2018-10-19 22:21:07,642 - nn-real-hvd - INFO -  58900;    13; 87017.1;   0.051; 0.13977; 0.09835; 0.95932; 0.38682\n",
      "2018-10-19 22:21:07,642 - nn-real-hvd - INFO -  58900;    13; 87017.1;   0.051; 0.13977; 0.09835; 0.95932; 0.38682\n",
      "2018-10-19 22:21:12,789 - nn-real-hvd - INFO -  59000;    13; 89477.1;   0.049; 0.12952; 0.09835; 0.96407; 0.45574\n",
      "2018-10-19 22:21:12,789 - nn-real-hvd - INFO -  59000;    13; 89477.1;   0.049; 0.12952; 0.09835; 0.96407; 0.45574\n",
      "2018-10-19 22:21:17,893 - nn-real-hvd - INFO -  59100;    13; 87337.7;   0.051; 0.12413; 0.09835; 0.96136; 0.43193\n",
      "2018-10-19 22:21:17,893 - nn-real-hvd - INFO -  59100;    13; 87337.7;   0.051; 0.12413; 0.09835; 0.96136; 0.43193\n",
      "2018-10-19 22:21:23,044 - nn-real-hvd - INFO -  59200;    13; 89095.7;   0.050; 0.13588; 0.09835; 0.95910; 0.45574\n",
      "2018-10-19 22:21:23,044 - nn-real-hvd - INFO -  59200;    13; 89095.7;   0.050; 0.13588; 0.09835; 0.95910; 0.45574\n",
      "2018-10-19 22:21:28,205 - nn-real-hvd - INFO -  59300;    13; 86154.7;   0.051; 0.10827; 0.09834; 0.96588; 0.43271\n",
      "2018-10-19 22:21:28,205 - nn-real-hvd - INFO -  59300;    13; 86154.7;   0.051; 0.10827; 0.09834; 0.96588; 0.43271\n",
      "2018-10-19 22:21:33,362 - nn-real-hvd - INFO -  59400;    13; 80777.3;   0.055; 0.12774; 0.09834; 0.96249; 0.44362\n",
      "2018-10-19 22:21:33,362 - nn-real-hvd - INFO -  59400;    13; 80777.3;   0.055; 0.12774; 0.09834; 0.96249; 0.44362\n",
      "2018-10-19 22:21:38,528 - nn-real-hvd - INFO -  59500;    13; 84481.0;   0.052; 0.12507; 0.09834; 0.95932; 0.51911\n",
      "2018-10-19 22:21:38,528 - nn-real-hvd - INFO -  59500;    13; 84481.0;   0.052; 0.12507; 0.09834; 0.95932; 0.51911\n",
      "2018-10-19 22:21:43,678 - nn-real-hvd - INFO -  59600;    13; 86192.3;   0.051; 0.13243; 0.09834; 0.96136; 0.45574\n",
      "2018-10-19 22:21:43,678 - nn-real-hvd - INFO -  59600;    13; 86192.3;   0.051; 0.13243; 0.09834; 0.96136; 0.45574\n",
      "2018-10-19 22:21:48,825 - nn-real-hvd - INFO -  59700;    13; 85199.2;   0.052; 0.13259; 0.09833; 0.95661; 0.45574\n",
      "2018-10-19 22:21:48,825 - nn-real-hvd - INFO -  59700;    13; 85199.2;   0.052; 0.13259; 0.09833; 0.95661; 0.45574\n",
      "2018-10-19 22:21:53,969 - nn-real-hvd - INFO -  59800;    13; 84700.4;   0.052; 0.13149; 0.09833; 0.95910; 0.48547\n",
      "2018-10-19 22:21:53,969 - nn-real-hvd - INFO -  59800;    13; 84700.4;   0.052; 0.13149; 0.09833; 0.95910; 0.48547\n",
      "2018-10-19 22:21:59,089 - nn-real-hvd - INFO -  59900;    13; 89793.2;   0.049; 0.12949; 0.09833; 0.96181; 0.57251\n",
      "2018-10-19 22:21:59,089 - nn-real-hvd - INFO -  59900;    13; 89793.2;   0.049; 0.12949; 0.09833; 0.96181; 0.57251\n",
      "2018-10-19 22:22:04,187 - nn-real-hvd - INFO -  60000;    13; 88402.3;   0.050; 0.13886; 0.09832; 0.96316; 0.56917\n",
      "2018-10-19 22:22:04,187 - nn-real-hvd - INFO -  60000;    13; 88402.3;   0.050; 0.13886; 0.09832; 0.96316; 0.56917\n",
      "2018-10-19 22:22:09,273 - nn-real-hvd - INFO -  60100;    13; 88114.4;   0.050; 0.12445; 0.09832; 0.96023; 0.53411\n",
      "2018-10-19 22:22:09,273 - nn-real-hvd - INFO -  60100;    13; 88114.4;   0.050; 0.12445; 0.09832; 0.96023; 0.53411\n",
      "2018-10-19 22:22:14,375 - nn-real-hvd - INFO -  60200;    13; 79335.0;   0.056; 0.13182; 0.09832; 0.95729; 0.42378\n",
      "2018-10-19 22:22:14,375 - nn-real-hvd - INFO -  60200;    13; 79335.0;   0.056; 0.13182; 0.09832; 0.95729; 0.42378\n",
      "2018-10-19 22:22:19,494 - nn-real-hvd - INFO -  60300;    13; 85204.7;   0.052; 0.12894; 0.09832; 0.96000; 0.45574\n",
      "2018-10-19 22:22:19,494 - nn-real-hvd - INFO -  60300;    13; 85204.7;   0.052; 0.12894; 0.09832; 0.96000; 0.45574\n",
      "2018-10-19 22:22:24,555 - nn-real-hvd - INFO -  60400;    13; 84640.5;   0.052; 0.12492; 0.09831; 0.96249; 0.56868\n",
      "2018-10-19 22:22:24,555 - nn-real-hvd - INFO -  60400;    13; 84640.5;   0.052; 0.12492; 0.09831; 0.96249; 0.56868\n",
      "2018-10-19 22:22:29,677 - nn-real-hvd - INFO -  60500;    13; 88755.3;   0.050; 0.12891; 0.09831; 0.95729; 0.51659\n",
      "2018-10-19 22:22:29,677 - nn-real-hvd - INFO -  60500;    13; 88755.3;   0.050; 0.12891; 0.09831; 0.95729; 0.51659\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-19 22:22:34,823 - nn-real-hvd - INFO -  60600;    13; 88968.9;   0.050; 0.13034; 0.09831; 0.95932; 0.46332\n",
      "2018-10-19 22:22:34,823 - nn-real-hvd - INFO -  60600;    13; 88968.9;   0.050; 0.13034; 0.09831; 0.95932; 0.46332\n",
      "2018-10-19 22:22:39,920 - nn-real-hvd - INFO -  60700;    13; 86214.8;   0.051; 0.12316; 0.09830; 0.96181; 0.45574\n",
      "2018-10-19 22:22:39,920 - nn-real-hvd - INFO -  60700;    13; 86214.8;   0.051; 0.12316; 0.09830; 0.96181; 0.45574\n",
      "2018-10-19 22:22:45,038 - nn-real-hvd - INFO -  60800;    13; 85601.6;   0.052; 0.12507; 0.09830; 0.96339; 0.45574\n",
      "2018-10-19 22:22:45,038 - nn-real-hvd - INFO -  60800;    13; 85601.6;   0.052; 0.12507; 0.09830; 0.96339; 0.45574\n",
      "2018-10-19 22:22:50,129 - nn-real-hvd - INFO -  60900;    13; 88961.2;   0.050; 0.13276; 0.09830; 0.95842; 0.38377\n",
      "2018-10-19 22:22:50,129 - nn-real-hvd - INFO -  60900;    13; 88961.2;   0.050; 0.13276; 0.09830; 0.95842; 0.38377\n",
      "2018-10-19 22:22:55,225 - nn-real-hvd - INFO -  61000;    13; 86838.4;   0.051; 0.11414; 0.09830; 0.96203; 0.45574\n",
      "2018-10-19 22:22:55,225 - nn-real-hvd - INFO -  61000;    13; 86838.4;   0.051; 0.11414; 0.09830; 0.96203; 0.45574\n",
      "2018-10-19 22:23:00,308 - nn-real-hvd - INFO -  61100;    13; 82561.7;   0.054; 0.13491; 0.09829; 0.96068; 0.42936\n",
      "2018-10-19 22:23:00,308 - nn-real-hvd - INFO -  61100;    13; 82561.7;   0.054; 0.13491; 0.09829; 0.96068; 0.42936\n",
      "2018-10-19 22:26:22,606 - nn-real-hvd - INFO - valid - Time: 0:03:20.938704\n",
      "2018-10-19 22:26:22,606 - nn-real-hvd - INFO - valid - Time: 0:03:20.938704\n",
      "2018-10-19 22:26:22,608 - nn-real-hvd - INFO - Confusion Matrix in Validation 61126; 13: [[    316     340      17     457   28625     717       0]\n",
      " [      0   33177    3187    4773   85196     527       0]\n",
      " [      0   13288    4209    5814    7823     170       0]\n",
      " [      0     203    2322   31798    1182    2576       0]\n",
      " [      0   20400    1834    5834 3729064    1517       0]\n",
      " [      0     170     121    5883     533   26480       0]\n",
      " [      0       1       1      55       4     869     856]]\n",
      "2018-10-19 22:26:22,608 - nn-real-hvd - INFO - Confusion Matrix in Validation 61126; 13: [[    316     340      17     457   28625     717       0]\n",
      " [      0   33177    3187    4773   85196     527       0]\n",
      " [      0   13288    4209    5814    7823     170       0]\n",
      " [      0     203    2322   31798    1182    2576       0]\n",
      " [      0   20400    1834    5834 3729064    1517       0]\n",
      " [      0     170     121    5883     533   26480       0]\n",
      " [      0       1       1      55       4     869     856]]\n",
      "2018-10-19 22:26:22,614 - nn-real-hvd - INFO - (step, epoch, loss, accuracy, better accuracy) in Validation:  61126;    13; 0.16013; 0.95178; 0.50003\n",
      "2018-10-19 22:26:22,614 - nn-real-hvd - INFO - (step, epoch, loss, accuracy, better accuracy) in Validation:  61126;    13; 0.16013; 0.95178; 0.50003\n",
      "2018-10-19 22:26:26,271 - nn-real-hvd - INFO -  61200;    14; 92984.0;   0.048; 0.14915; 0.09829; 0.95390; 0.44629\n",
      "2018-10-19 22:26:26,271 - nn-real-hvd - INFO -  61200;    14; 92984.0;   0.048; 0.14915; 0.09829; 0.95390; 0.44629\n",
      "2018-10-19 22:26:31,188 - nn-real-hvd - INFO -  61300;    14; 94232.7;   0.047; 0.13460; 0.09829; 0.95910; 0.48636\n",
      "2018-10-19 22:26:31,188 - nn-real-hvd - INFO -  61300;    14; 94232.7;   0.047; 0.13460; 0.09829; 0.95910; 0.48636\n",
      "2018-10-19 22:26:36,104 - nn-real-hvd - INFO -  61400;    14; 91639.3;   0.048; 0.11840; 0.09829; 0.96520; 0.56816\n",
      "2018-10-19 22:26:36,104 - nn-real-hvd - INFO -  61400;    14; 91639.3;   0.048; 0.11840; 0.09829; 0.96520; 0.56816\n",
      "2018-10-19 22:26:41,006 - nn-real-hvd - INFO -  61500;    14; 94545.7;   0.047; 0.13970; 0.09828; 0.96023; 0.57208\n",
      "2018-10-19 22:26:41,006 - nn-real-hvd - INFO -  61500;    14; 94545.7;   0.047; 0.13970; 0.09828; 0.96023; 0.57208\n",
      "2018-10-19 22:26:45,876 - nn-real-hvd - INFO -  61600;    14; 94387.0;   0.047; 0.14051; 0.09828; 0.95638; 0.50411\n",
      "2018-10-19 22:26:45,876 - nn-real-hvd - INFO -  61600;    14; 94387.0;   0.047; 0.14051; 0.09828; 0.95638; 0.50411\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint written to /home/ubuntu/real_summaries4425-15ep_test/checkpoint-61609\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-19 22:26:47,855 - nn-real-hvd - INFO - METRICS each 450 (secs):  Loss in ---Training in Summary---: 0.12212\n",
      " Avg Log_Loss in ---Training in Summary---: 0.03195\n",
      "---Training in Summary---: (Silly) Global-ACC=0.96316, Better ACC=0.43633, Avg M-Measure=0.8693, Avg AUC_AOC=0.9357 Avg AUC_PR=0.5076\n",
      "Total Confusion Matrix;Total M-Measure Matrix;Total AUC_AOC;Total AUC_PR\n",
      "2;1;0;0;15;0;0;;0.0000;0.7925;0.7487;0.6782;0.8479;0.4139;0.3027;;0.7992 ;0.0489\n",
      "0;33;2;1;77;0;0;;0.8630;0.0000;0.3412;0.7804;0.9183;0.8435;0.9005;;0.9096 ;0.3287\n",
      "0;10;1;6;7;0;0;;0.9724;0.8196;0.0000;0.8063;0.9947;0.9539;0.9911;;0.9866 ;0.2370\n",
      "0;0;7;27;1;1;0;;0.9887;0.9875;0.9316;0.0000;0.9983;0.8862;0.9038;;0.9963 ;0.7650\n",
      "0;21;1;8;4173;0;0;;0.7627;0.9138;0.9954;0.9992;0.0000;0.9988;0.9997;;0.9330 ;0.9863\n",
      "0;0;1;3;0;26;0;;0.9778;0.9899;0.9763;0.8945;0.9955;0.0000;0.7103;;0.9919 ;0.7385\n",
      "0;0;0;0;0;1;0;;0.9698;0.9780;0.9775;0.9223;0.9784;0.8041;0.0000;;0.9333 ;0.4490\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "2018-10-19 22:26:47,855 - nn-real-hvd - INFO - METRICS each 450 (secs):  Loss in ---Training in Summary---: 0.12212\n",
      " Avg Log_Loss in ---Training in Summary---: 0.03195\n",
      "---Training in Summary---: (Silly) Global-ACC=0.96316, Better ACC=0.43633, Avg M-Measure=0.8693, Avg AUC_AOC=0.9357 Avg AUC_PR=0.5076\n",
      "Total Confusion Matrix;Total M-Measure Matrix;Total AUC_AOC;Total AUC_PR\n",
      "2;1;0;0;15;0;0;;0.0000;0.7925;0.7487;0.6782;0.8479;0.4139;0.3027;;0.7992 ;0.0489\n",
      "0;33;2;1;77;0;0;;0.8630;0.0000;0.3412;0.7804;0.9183;0.8435;0.9005;;0.9096 ;0.3287\n",
      "0;10;1;6;7;0;0;;0.9724;0.8196;0.0000;0.8063;0.9947;0.9539;0.9911;;0.9866 ;0.2370\n",
      "0;0;7;27;1;1;0;;0.9887;0.9875;0.9316;0.0000;0.9983;0.8862;0.9038;;0.9963 ;0.7650\n",
      "0;21;1;8;4173;0;0;;0.7627;0.9138;0.9954;0.9992;0.0000;0.9988;0.9997;;0.9330 ;0.9863\n",
      "0;0;1;3;0;26;0;;0.9778;0.9899;0.9763;0.8945;0.9955;0.0000;0.7103;;0.9919 ;0.7385\n",
      "0;0;0;0;0;1;0;;0.9698;0.9780;0.9775;0.9223;0.9784;0.8041;0.0000;;0.9333 ;0.4490\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "2018-10-19 22:26:52,337 - nn-real-hvd - INFO -  61700;    14; 86870.1;   0.051; 0.15139; 0.09828; 0.95480; 0.43633\n",
      "2018-10-19 22:26:52,337 - nn-real-hvd - INFO -  61700;    14; 86870.1;   0.051; 0.15139; 0.09828; 0.95480; 0.43633\n",
      "2018-10-19 22:26:57,477 - nn-real-hvd - INFO -  61800;    14; 87485.1;   0.051; 0.14530; 0.09827; 0.95842; 0.57570\n",
      "2018-10-19 22:26:57,477 - nn-real-hvd - INFO -  61800;    14; 87485.1;   0.051; 0.14530; 0.09827; 0.95842; 0.57570\n",
      "2018-10-19 22:27:02,631 - nn-real-hvd - INFO -  61900;    14; 89326.0;   0.050; 0.13452; 0.09827; 0.95797; 0.43633\n",
      "2018-10-19 22:27:02,631 - nn-real-hvd - INFO -  61900;    14; 89326.0;   0.050; 0.13452; 0.09827; 0.95797; 0.43633\n",
      "2018-10-19 22:27:07,754 - nn-real-hvd - INFO -  62000;    14; 84821.5;   0.052; 0.11603; 0.09827; 0.96339; 0.59176\n",
      "2018-10-19 22:27:07,754 - nn-real-hvd - INFO -  62000;    14; 84821.5;   0.052; 0.11603; 0.09827; 0.96339; 0.59176\n",
      "2018-10-19 22:27:12,841 - nn-real-hvd - INFO -  62100;    14; 87092.7;   0.051; 0.13264; 0.09827; 0.95684; 0.56000\n",
      "2018-10-19 22:27:12,841 - nn-real-hvd - INFO -  62100;    14; 87092.7;   0.051; 0.13264; 0.09827; 0.95684; 0.56000\n",
      "2018-10-19 22:27:17,942 - nn-real-hvd - INFO -  62200;    14; 88079.7;   0.050; 0.12832; 0.09826; 0.96158; 0.58789\n",
      "2018-10-19 22:27:17,942 - nn-real-hvd - INFO -  62200;    14; 88079.7;   0.050; 0.12832; 0.09826; 0.96158; 0.58789\n",
      "2018-10-19 22:27:23,137 - nn-real-hvd - INFO -  62300;    14; 86123.9;   0.051; 0.12565; 0.09826; 0.95887; 0.53129\n",
      "2018-10-19 22:27:23,137 - nn-real-hvd - INFO -  62300;    14; 86123.9;   0.051; 0.12565; 0.09826; 0.95887; 0.53129\n",
      "2018-10-19 22:27:28,264 - nn-real-hvd - INFO -  62400;    14; 91028.0;   0.049; 0.12003; 0.09826; 0.96226; 0.56639\n",
      "2018-10-19 22:27:28,264 - nn-real-hvd - INFO -  62400;    14; 91028.0;   0.049; 0.12003; 0.09826; 0.96226; 0.56639\n",
      "2018-10-19 22:27:33,420 - nn-real-hvd - INFO -  62500;    14; 87829.6;   0.050; 0.14848; 0.09826; 0.95729; 0.41464\n",
      "2018-10-19 22:27:33,420 - nn-real-hvd - INFO -  62500;    14; 87829.6;   0.050; 0.14848; 0.09826; 0.95729; 0.41464\n",
      "2018-10-19 22:27:38,540 - nn-real-hvd - INFO -  62600;    14; 89794.5;   0.049; 0.12980; 0.09825; 0.95842; 0.40762\n",
      "2018-10-19 22:27:38,540 - nn-real-hvd - INFO -  62600;    14; 89794.5;   0.049; 0.12980; 0.09825; 0.95842; 0.40762\n",
      "2018-10-19 22:27:43,654 - nn-real-hvd - INFO -  62700;    14; 87876.6;   0.050; 0.12762; 0.09825; 0.96000; 0.48584\n",
      "2018-10-19 22:27:43,654 - nn-real-hvd - INFO -  62700;    14; 87876.6;   0.050; 0.12762; 0.09825; 0.96000; 0.48584\n",
      "2018-10-19 22:27:48,843 - nn-real-hvd - INFO -  62800;    14; 88084.7;   0.050; 0.12045; 0.09825; 0.96339; 0.43633\n",
      "2018-10-19 22:27:48,843 - nn-real-hvd - INFO -  62800;    14; 88084.7;   0.050; 0.12045; 0.09825; 0.96339; 0.43633\n",
      "2018-10-19 22:27:53,980 - nn-real-hvd - INFO -  62900;    14; 87667.0;   0.050; 0.12030; 0.09824; 0.96475; 0.43633\n",
      "2018-10-19 22:27:53,980 - nn-real-hvd - INFO -  62900;    14; 87667.0;   0.050; 0.12030; 0.09824; 0.96475; 0.43633\n",
      "2018-10-19 22:27:59,120 - nn-real-hvd - INFO -  63000;    14; 84031.5;   0.053; 0.12612; 0.09824; 0.95977; 0.57413\n",
      "2018-10-19 22:27:59,120 - nn-real-hvd - INFO -  63000;    14; 84031.5;   0.053; 0.12612; 0.09824; 0.95977; 0.57413\n",
      "2018-10-19 22:28:04,323 - nn-real-hvd - INFO -  63100;    14; 85294.0;   0.052; 0.13762; 0.09824; 0.95910; 0.55350\n",
      "2018-10-19 22:28:04,323 - nn-real-hvd - INFO -  63100;    14; 85294.0;   0.052; 0.13762; 0.09824; 0.95910; 0.55350\n",
      "2018-10-19 22:28:09,441 - nn-real-hvd - INFO -  63200;    14; 82786.4;   0.053; 0.12623; 0.09824; 0.96045; 0.43633\n",
      "2018-10-19 22:28:09,441 - nn-real-hvd - INFO -  63200;    14; 82786.4;   0.053; 0.12623; 0.09824; 0.96045; 0.43633\n",
      "2018-10-19 22:28:14,559 - nn-real-hvd - INFO -  63300;    14; 87292.2;   0.051; 0.12153; 0.09823; 0.96768; 0.47336\n",
      "2018-10-19 22:28:14,559 - nn-real-hvd - INFO -  63300;    14; 87292.2;   0.051; 0.12153; 0.09823; 0.96768; 0.47336\n",
      "2018-10-19 22:28:19,683 - nn-real-hvd - INFO -  63400;    14; 86005.0;   0.051; 0.14576; 0.09823; 0.95458; 0.50296\n",
      "2018-10-19 22:28:19,683 - nn-real-hvd - INFO -  63400;    14; 86005.0;   0.051; 0.14576; 0.09823; 0.95458; 0.50296\n",
      "2018-10-19 22:28:24,833 - nn-real-hvd - INFO -  63500;    14; 76820.0;   0.058; 0.12146; 0.09823; 0.96339; 0.43633\n",
      "2018-10-19 22:28:24,833 - nn-real-hvd - INFO -  63500;    14; 76820.0;   0.058; 0.12146; 0.09823; 0.96339; 0.43633\n",
      "2018-10-19 22:28:29,963 - nn-real-hvd - INFO -  63600;    14; 89507.8;   0.049; 0.12136; 0.09823; 0.96362; 0.42326\n",
      "2018-10-19 22:28:29,963 - nn-real-hvd - INFO -  63600;    14; 89507.8;   0.049; 0.12136; 0.09823; 0.96362; 0.42326\n",
      "2018-10-19 22:28:35,067 - nn-real-hvd - INFO -  63700;    14; 90032.7;   0.049; 0.12565; 0.09822; 0.96362; 0.49870\n",
      "2018-10-19 22:28:35,067 - nn-real-hvd - INFO -  63700;    14; 90032.7;   0.049; 0.12565; 0.09822; 0.96362; 0.49870\n",
      "2018-10-19 22:28:38,577 - nn-real-hvd - INFO - Keyboard interrupt\n",
      "2018-10-19 22:28:38,577 - nn-real-hvd - INFO - Keyboard interrupt\n",
      "2018-10-19 22:28:38,583 - nn-real-hvd - INFO - Time used in total: 15181.5 seconds\n",
      "2018-10-19 22:28:38,583 - nn-real-hvd - INFO - Time used in total: 15181.5 seconds\n"
     ]
    }
   ],
   "source": [
    "ops_to_run = [learning_rate_op, train_ops]\n",
    "ops_stats = [conf_mtx_op, accuracy_op, better_acc_op, auc_list_op, auc_mean_op, m_list_op, m_list_mean_op, \n",
    "             lloss_op, auc_pr_op, auc_pr_mean_op, total_loss_op]\n",
    "                    \n",
    "oom = False\n",
    "step0 = int(sess.run(trainer.global_step))\n",
    "for step in range(step0, nstep):    \n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        epoch = step*FLAGS.batch_size*hvd.size() // nrecord\n",
    "        batch_dict= create_feed_dict('batch', DATA, FLAGS)        \n",
    "        \n",
    "        if (hvd.rank() == 0 and summary_ops is not None and\n",
    "            (step == 0 or step+1 == nstep or\n",
    "             time.time() - last_summary_time > FLAGS.summary_interval)):\n",
    "            \n",
    "            if step != 0:\n",
    "                last_summary_time += FLAGS.summary_interval                        \n",
    "                \n",
    "            reset_and_update(sess, local_init, batch_dict)\n",
    "            summary, conf_mtx, accuracy, better_acc, auc_list, auc_mean, m_list, m_list_mean, lloss, auc_pr, auc_pr_mean, loss, lr, _ = sess.run([summary_ops] + ops_stats + ops_to_run, feed_dict=batch_dict)                        \n",
    "            train_writer.add_summary(summary, step)            \n",
    "            train_writer.flush()\n",
    "            if (math.isnan(better_acc)):\n",
    "                better_acc = calculate_better_acc(conf_mtx)        \n",
    "                \n",
    "        else:\n",
    "            accuracy, conf_mtx, better_acc, loss, lr, _ = sess.run([accuracy_op, conf_mtx_op, better_acc_op, total_loss_op] + ops_to_run, feed_dict=batch_dict)\n",
    "            if (math.isnan(better_acc)):\n",
    "                better_acc = calculate_better_acc(conf_mtx)        \n",
    "\n",
    "        elapsed = time.time() - start_time                \n",
    "        feature_per_sec = FLAGS.batch_size / elapsed            \n",
    "            \n",
    "        if step == 0 or (step+1) % FLAGS.display_every == 0:                    \n",
    "            logger.info(\"%6i; %5i; %7.1f; %7.3f; %7.5f; %7.5f; %7.5f; %7.5f\" % (\n",
    "                step+1, epoch+1, feature_per_sec*hvd.size(), elapsed, loss, lr, accuracy, better_acc))        \n",
    "\n",
    "        if (hvd.rank() == 0 and (step == 0 or (step+1) % nstep_per_epoch == 0)):        \n",
    "\n",
    "            #this not necessarily matches with the display_every because of the summary_interval!\n",
    "            print_stats('---Training in Summary---', conf_mtx, accuracy, better_acc, auc_list, auc_mean, m_list, m_list_mean, lloss, auc_pr, auc_pr_mean, loss)                         \n",
    "            df_train.loc[len(df_train)] = [step+1, epoch+1, elapsed, loss, lloss, accuracy, better_acc, m_list_mean, auc_mean, auc_pr_mean]                        \n",
    "            # Adding validation set:\n",
    "            valid_conf_mtx, valid_time, metrics = batching_dataset(sess, valid_writer, 'valid', DATA, FLAGS)\n",
    "            #valid_conf_mtx = np.array2string(valid_conf_mtx, formatter={'int_type':lambda x: \"int(%)\" % x})\n",
    "            valid_conf_mtx = np.array(valid_conf_mtx, dtype=int)\n",
    "            logger.info(\"Confusion Matrix in Validation %d; %d: %s\" % (step+1, epoch+1, str(valid_conf_mtx)))\n",
    "            df_valid.loc[len(df_valid)] = [step+1, epoch+1, valid_time, metrics[3], metrics[2], metrics[0], metrics[1]]            \n",
    "            logger.info(\"(step, epoch, loss, accuracy, better accuracy) in Validation: %6i; %5i; %7.5f; %7.5f; %7.5f\" % (\n",
    "                step+1, epoch+1, metrics[3], metrics[0], metrics[1]))    \n",
    "            sess.run(local_init)\n",
    "\n",
    "                    \n",
    "    except KeyboardInterrupt:\n",
    "        if hvd.rank() == 0:\n",
    "            df_train.to_csv(train_file, index=False, mode='a', sep =';', header=False)\n",
    "            df_valid.to_csv(valid_file, index=False, mode='a', sep =';', header=False)\n",
    "        logger.info(\"Keyboard interrupt\")\n",
    "        break\n",
    "    except tf.errors.ResourceExhaustedError:\n",
    "        elapsed = -1.\n",
    "        loss    = 0.\n",
    "        lr      = -1\n",
    "        if hvd.rank() == 0:\n",
    "            df_train.to_csv(train_file, index=False, mode='a', sep =';', header=False)\n",
    "            df_valid.to_csv(valid_file, index=False, mode='a', sep =';', header=False)\n",
    "        oom = True\n",
    "    \n",
    "    if (hvd.rank() == 0 and saver is not None and\n",
    "        (time.time() - last_save_time > FLAGS.save_interval or step+1 == nstep)):\n",
    "        last_save_time += FLAGS.save_interval\n",
    "        save_path = saver.save(sess, checkpoint_file, global_step=trainer.global_step)\n",
    "        print(\"Checkpoint written to\", save_path)\n",
    "    \n",
    "    if oom:\n",
    "        break\n",
    "\n",
    "if hvd.rank() == 0:                               \n",
    "    df_train.to_csv(train_file, index=False, mode='a', sep =';', header=False)\n",
    "    df_valid.to_csv(valid_file, index=False, mode='a', sep =';', header=False)\n",
    "                               \n",
    "if train_writer is not None:\n",
    "    train_writer.close()\n",
    "\n",
    "if valid_writer is not None:\n",
    "    valid_writer.close()    \n",
    "    \n",
    "global_end_time = time.time()\n",
    "#logger.info(\"start time is {}, end time is {}\".format(global_start_time, global_end_time))\n",
    "logger.info('Time used in total: %.1f seconds' % (global_end_time - global_start_time))\n",
    "\n",
    "if oom:\n",
    "    print(\"Out of memory error detected, exiting\")\n",
    "    sys.exit(-2)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook nn_real_hvd-ntb-v3.ipynb to python\n",
      "[NbConvertApp] Writing 61952 bytes to nn_real_hvd-ntb-v3.py\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to=python  nn_real_hvd-ntb-v3.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "nn_real_hvd-ntb-v3.py                           0%    0     0.0KB/s   --:-- ETA\r",
      "nn_real_hvd-ntb-v3.py                         100%   61KB  60.5KB/s   00:00    \r\n"
     ]
    }
   ],
   "source": [
    "!scp nn_real_hvd-ntb-v3.py ubuntu@ec2-52-90-176-235.compute-1.amazonaws.com:/home/ubuntu/MLMortgage/notebooks/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/ubuntu/MLMortgage/src/data', '/home/ubuntu/MLMortgage/notebooks', '/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python36.zip', '/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6', '/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/lib-dynload', '/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages']\n",
      "['/home/ubuntu/MLMortgage/src/data', '/home/ubuntu/MLMortgage/notebooks', '/home/ubuntu/src/cntk/bindings/python', '/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python36.zip', '/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6', '/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/lib-dynload', '/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages']\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel package will be renamed from release 2.8; in order to keep installing from binary please use \"pip install psycopg2-binary\" instead. For details see: <http://initd.org/psycopg/docs/install.html#binary-install-from-pypi>.\n",
      "  \"\"\")\n",
      "2018-10-20 00:04:06,178 - matplotlib - DEBUG - CACHEDIR=/home/ubuntu/.cache/matplotlib\n",
      "2018-10-20 00:04:06,181 - matplotlib.font_manager - DEBUG - Using fontManager instance from /home/ubuntu/.cache/matplotlib/fontList.json\n",
      "2018-10-20 00:04:06,259 - matplotlib.backends - DEBUG - backend agg version v2.2\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel package will be renamed from release 2.8; in order to keep installing from binary please use \"pip install psycopg2-binary\" instead. For details see: <http://initd.org/psycopg/docs/install.html#binary-install-from-pypi>.\n",
      "  \"\"\")\n",
      "2018-10-20 00:04:06,275 - matplotlib - DEBUG - CACHEDIR=/home/ubuntu/.cache/matplotlib\n",
      "2018-10-20 00:04:06,277 - matplotlib.font_manager - DEBUG - Using fontManager instance from /home/ubuntu/.cache/matplotlib/fontList.json\n",
      "/home/ubuntu/MLMortgage/data/raw /home/ubuntu/MLMortgage/data/processed\n",
      "2018-10-20 00:04:06,554 - matplotlib.backends - DEBUG - backend module://ipykernel.pylab.backend_inline version unknown\n",
      "/home/ubuntu/MLMortgage/data/raw /home/ubuntu/MLMortgage/data/processed\n",
      "UNPARSED []\n",
      "FLAGS Namespace(allow_summaries=True, batch_layer_type='batch', batch_norm=True, batch_size=4425, batch_type='layer', data_dir='/input_data', decay_rate=1, decay_step=3520000, display_every=100, dropout=True, dropout_keep=0.9, epoch_flag=0, epoch_num=15, eval=False, ftp_dir='processed/c1mill_99-01', learning_rate=0.1, log_file=None, logdir=PosixPath('/home/ubuntu/real_summaries4425-15ep_test'), loss_tolerance=0.0001, lr_decay_epochs=30, lr_decay_policy='time', lr_decay_rate=0.1, lr_poly_power=2.0, max_epoch_size=-1, momentum=0.5, n_hidden=3, name='batch_layer_type', nstep_burnin=20, rate_min=0.0015, reg_rate=1e-05, s_hidden=[200, 140, 140], save_interval=450, stratified_flag=False, summary_interval=1800, test_batch_size=100000, test_dir='chuncks_random_c1millx2_test', test_flag=True, test_period=[286, 304], train_dir='chuncks_random_c1millx2_train', train_period=[121, 279], valid_batch_size=100000, valid_dir='chuncks_random_c1millx2_valid', valid_period=[280, 285], weighted_sampling=False, xla=True)\n",
      "UNPARSED []\n",
      "FLAGS Namespace(allow_summaries=True, batch_layer_type='batch', batch_norm=True, batch_size=4425, batch_type='layer', data_dir='/input_data', decay_rate=1, decay_step=3520000, display_every=100, dropout=True, dropout_keep=0.9, epoch_flag=0, epoch_num=15, eval=False, ftp_dir='processed/c1mill_99-01', learning_rate=0.1, log_file=None, logdir=PosixPath('/home/ubuntu/real_summaries4425-15ep_test'), loss_tolerance=0.0001, lr_decay_epochs=30, lr_decay_policy='time', lr_decay_rate=0.1, lr_poly_power=2.0, max_epoch_size=-1, momentum=0.5, n_hidden=3, name='batch_layer_type', nstep_burnin=20, rate_min=0.0015, reg_rate=1e-05, s_hidden=[200, 140, 140], save_interval=450, stratified_flag=False, summary_interval=1800, test_batch_size=100000, test_dir='chuncks_random_c1millx2_test', test_flag=True, test_period=[286, 304], train_dir='chuncks_random_c1millx2_train', train_period=[121, 279], valid_batch_size=100000, valid_dir='chuncks_random_c1millx2_valid', valid_period=[280, 285], weighted_sampling=False, xla=True)\n",
      "rank:  1 architecture {'n_input': 258, 'n_classes': 7, 'train_num_examples': 20809545, 'valid_num_examples': 4020339, 'test_num_examples': 8635528, 'n_hidden_1': 200, 'n_hidden_2': 140, 'n_hidden_3': 140}\n",
      "rank:  0 architecture {'n_input': 258, 'n_classes': 7, 'train_num_examples': 20809545, 'valid_num_examples': 4020339, 'test_num_examples': 8635528, 'n_hidden_1': 200, 'n_hidden_2': 140, 'n_hidden_3': 140}\n",
      "2018-10-20 00:04:11,915 - nn-real-hvd_0 - INFO - Num ranks:  2\n",
      "2018-10-20 00:04:11,915 - nn-real-hvd_0 - INFO - Num ranks:  2\n",
      "2018-10-20 00:04:11,915 - nn-real-hvd_0 - INFO - Num of records: 20809545\n",
      "2018-10-20 00:04:11,915 - nn-real-hvd_0 - INFO - Num of records: 20809545\n",
      "2018-10-20 00:04:11,915 - nn-real-hvd_0 - INFO - Total batch size: 8850\n",
      "2018-10-20 00:04:11,915 - nn-real-hvd_0 - INFO - Total batch size: 8850\n",
      "2018-10-20 00:04:11,915 - nn-real-hvd_0 - INFO - 4425, per device\n",
      "2018-10-20 00:04:11,915 - nn-real-hvd_0 - INFO - 4425, per device\n",
      "2018-10-20 00:04:11,915 - nn-real-hvd_0 - INFO - Data type: <dtype: 'float32'>\n",
      "2018-10-20 00:04:11,915 - nn-real-hvd_0 - INFO - Data type: <dtype: 'float32'>\n",
      "2018-10-20 00:04:11,915 - nn-real-hvd_0 - INFO - architecture: {'n_input': 258, 'n_classes': 7, 'train_num_examples': 20809545, 'valid_num_examples': 4020339, 'test_num_examples': 8635528, 'n_hidden_1': 200, 'n_hidden_2': 140, 'n_hidden_3': 140}\n",
      "2018-10-20 00:04:11,915 - nn-real-hvd_0 - INFO - architecture: {'n_input': 258, 'n_classes': 7, 'train_num_examples': 20809545, 'valid_num_examples': 4020339, 'test_num_examples': 8635528, 'n_hidden_1': 200, 'n_hidden_2': 140, 'n_hidden_3': 140}\n",
      "2018-10-20 00:04:11,919 - nn-real-hvd_1 - INFO - Num ranks:  2\n",
      "2018-10-20 00:04:11,919 - nn-real-hvd_1 - INFO - Num ranks:  2\n",
      "2018-10-20 00:04:11,919 - nn-real-hvd_1 - INFO - Num of records: 20809545\n",
      "2018-10-20 00:04:11,919 - nn-real-hvd_1 - INFO - Num of records: 20809545\n",
      "2018-10-20 00:04:11,919 - nn-real-hvd_1 - INFO - Total batch size: 8850\n",
      "2018-10-20 00:04:11,919 - nn-real-hvd_1 - INFO - Total batch size: 8850\n",
      "2018-10-20 00:04:11,919 - nn-real-hvd_1 - INFO - 4425, per device\n",
      "2018-10-20 00:04:11,919 - nn-real-hvd_1 - INFO - 4425, per device\n",
      "2018-10-20 00:04:11,919 - nn-real-hvd_1 - INFO - Data type: <dtype: 'float32'>\n",
      "2018-10-20 00:04:11,919 - nn-real-hvd_1 - INFO - Data type: <dtype: 'float32'>\n",
      "2018-10-20 00:04:11,919 - nn-real-hvd_1 - INFO - architecture: {'n_input': 258, 'n_classes': 7, 'train_num_examples': 20809545, 'valid_num_examples': 4020339, 'test_num_examples': 8635528, 'n_hidden_1': 200, 'n_hidden_2': 140, 'n_hidden_3': 140}\n",
      "2018-10-20 00:04:11,919 - nn-real-hvd_1 - INFO - architecture: {'n_input': 258, 'n_classes': 7, 'train_num_examples': 20809545, 'valid_num_examples': 4020339, 'test_num_examples': 8635528, 'n_hidden_1': 200, 'n_hidden_2': 140, 'n_hidden_3': 140}\n",
      "dict:  {'path': '/home/ubuntu/MLMortgage/data/processed/chuncks_random_c1millx2_train/c1mill99-01_non_index_train_0.h5', 'nrows': 20809545, 'init_index': 0, 'end_index': 20809545, 'dataset': <class 'pandas.io.pytables.HDFStore'>\n",
      "File path: /home/ubuntu/MLMortgage/data/processed/chuncks_random_c1millx2_train/c1mill99-01_non_index_train_0.h5\n",
      ", 'dataset_features': array([[0.11811024, 0.        , 1.431     , ..., 0.        , 0.        ,\n",
      "        0.        ],\n",
      "       [0.        , 0.        , 1.3775    , ..., 0.        , 0.        ,\n",
      "        0.        ],\n",
      "       [0.        , 0.        , 1.354     , ..., 0.        , 0.        ,\n",
      "        0.        ],\n",
      "       ...,\n",
      "       [0.        , 0.        , 2.125     , ..., 0.        , 0.        ,\n",
      "        0.        ],\n",
      "       [0.        , 0.        , 2.125     , ..., 0.        , 0.        ,\n",
      "        0.        ],\n",
      "       [0.        , 0.        , 2.125     , ..., 0.        , 0.        ,\n",
      "        0.        ]], dtype=float32), 'dataset_labels': array([[0, 0, 0, ..., 1, 0, 0],\n",
      "       [0, 0, 0, ..., 1, 0, 0],\n",
      "       [0, 0, 0, ..., 1, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 0, ..., 1, 0, 0],\n",
      "       [0, 0, 0, ..., 1, 0, 0],\n",
      "       [0, 0, 0, ..., 1, 0, 0]], dtype=int8)}  total rows:  20809545\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict:  {'path': '/home/ubuntu/MLMortgage/data/processed/chuncks_random_c1millx2_train/c1mill99-01_train_non_index_0.h5', 'nrows': 20809545, 'init_index': 0, 'end_index': 20809545, 'dataset': <class 'pandas.io.pytables.HDFStore'>\n",
      "File path: /home/ubuntu/MLMortgage/data/processed/chuncks_random_c1millx2_train/c1mill99-01_train_non_index_0.h5\n",
      ", 'dataset_features': array([[0.11811024, 0.        , 1.431     , ..., 0.        , 0.        ,\n",
      "        0.        ],\n",
      "       [0.        , 0.        , 1.3775    , ..., 0.        , 0.        ,\n",
      "        0.        ],\n",
      "       [0.        , 0.        , 1.354     , ..., 0.        , 0.        ,\n",
      "        0.        ],\n",
      "       ...,\n",
      "       [0.        , 0.        , 2.125     , ..., 0.        , 0.        ,\n",
      "        0.        ],\n",
      "       [0.        , 0.        , 2.125     , ..., 0.        , 0.        ,\n",
      "        0.        ],\n",
      "       [0.        , 0.        , 2.125     , ..., 0.        , 0.        ,\n",
      "        0.        ]], dtype=float32), 'dataset_labels': array([[0, 0, 0, ..., 1, 0, 0],\n",
      "       [0, 0, 0, ..., 1, 0, 0],\n",
      "       [0, 0, 0, ..., 1, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 0, ..., 1, 0, 0],\n",
      "       [0, 0, 0, ..., 1, 0, 0],\n",
      "       [0, 0, 0, ..., 1, 0, 0]], dtype=int8)}  total rows:  20809545\n",
      "dataset_features:  (4020339, 258)\n",
      "dict:  {'path': '/home/ubuntu/MLMortgage/data/processed/chuncks_random_c1millx2_valid/c1mill99-01_non_index_valid_0.h5', 'nrows': 4020339, 'init_index': 0, 'end_index': 4020339, 'dataset': <class 'pandas.io.pytables.HDFStore'>\n",
      "File path: /home/ubuntu/MLMortgage/data/processed/chuncks_random_c1millx2_valid/c1mill99-01_non_index_valid_0.h5\n",
      ", 'dataset_features': array([[0.    , 0.    , 1.8445, ..., 0.    , 0.    , 0.    ],\n",
      "       [0.    , 0.    , 1.8535, ..., 0.    , 0.    , 0.    ],\n",
      "       [0.    , 0.    , 1.8585, ..., 0.    , 0.    , 0.    ],\n",
      "       ...,\n",
      "       [0.    , 0.    , 2.125 , ..., 0.    , 0.    , 0.    ],\n",
      "       [0.    , 0.    , 2.125 , ..., 0.    , 0.    , 0.    ],\n",
      "       [0.    , 0.    , 2.125 , ..., 0.    , 0.    , 0.    ]],\n",
      "      dtype=float32), 'dataset_labels': array([[0, 0, 0, ..., 1, 0, 0],\n",
      "       [0, 0, 0, ..., 1, 0, 0],\n",
      "       [0, 0, 0, ..., 1, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 0, ..., 1, 0, 0],\n",
      "       [0, 0, 0, ..., 1, 0, 0],\n",
      "       [0, 0, 0, ..., 1, 0, 0]], dtype=int8)}  accumulated rows:  4020339\n",
      "Features List:  ['MBA_DAYS_DELINQUENT', 'MBA_DAYS_DELINQUENT_NAN', 'CURRENT_INTEREST_RATE', 'CURRENT_INTEREST_RATE_NAN', 'LOANAGE', 'LOANAGE_NAN', 'CURRENT_BALANCE', 'CURRENT_BALANCE_NAN', 'SCHEDULED_PRINCIPAL', 'SCHEDULED_PRINCIPAL_NAN', 'SCHEDULED_MONTHLY_PANDI', 'SCHEDULED_MONTHLY_PANDI_NAN', 'LLMA2_CURRENT_INTEREST_SPREAD', 'LLMA2_CURRENT_INTEREST_SPREAD_NAN', 'LLMA2_C_IN_LAST_12_MONTHS', 'LLMA2_30_IN_LAST_12_MONTHS', 'LLMA2_60_IN_LAST_12_MONTHS', 'LLMA2_90_IN_LAST_12_MONTHS', 'LLMA2_FC_IN_LAST_12_MONTHS', 'LLMA2_REO_IN_LAST_12_MONTHS', 'LLMA2_0_IN_LAST_12_MONTHS', 'LLMA2_HIST_LAST_12_MONTHS_MIS', 'NUM_MODIF', 'NUM_MODIF_NAN', 'P_RATE_TO_MOD', 'P_RATE_TO_MOD_NAN', 'MOD_RATE', 'MOD_RATE_NAN', 'DIF_RATE', 'DIF_RATE_NAN', 'P_MONTHLY_PAY', 'P_MONTHLY_PAY_NAN', 'MOD_MONTHLY_PAY', 'MOD_MONTHLY_PAY_NAN', 'DIF_MONTHLY_PAY', 'DIF_MONTHLY_PAY_NAN', 'CAPITALIZATION_AMT', 'CAPITALIZATION_AMT_NAN', 'MORTGAGE_RATE', 'MORTGAGE_RATE_NAN', 'FICO_SCORE_ORIGINATION', 'INITIAL_INTEREST_RATE', 'ORIGINAL_LTV', 'ORIGINAL_BALANCE', 'BACKEND_RATIO', 'BACKEND_RATIO_NAN', 'ORIGINAL_TERM', 'ORIGINAL_TERM_NAN', 'SALE_PRICE', 'SALE_PRICE_NAN', 'PREPAY_PENALTY_TERM', 'PREPAY_PENALTY_TERM_NAN', 'NUMBER_OF_UNITS', 'NUMBER_OF_UNITS_NAN', 'MARGIN', 'MARGIN_NAN', 'PERIODIC_RATE_CAP', 'PERIODIC_RATE_CAP_NAN', 'PERIODIC_RATE_FLOOR', 'PERIODIC_RATE_FLOOR_NAN', 'LIFETIME_RATE_CAP', 'LIFETIME_RATE_CAP_NAN', 'LIFETIME_RATE_FLOOR', 'LIFETIME_RATE_FLOOR_NAN', 'RATE_RESET_FREQUENCY', 'RATE_RESET_FREQUENCY_NAN', 'PAY_RESET_FREQUENCY', 'PAY_RESET_FREQUENCY_NAN', 'FIRST_RATE_RESET_PERIOD', 'FIRST_RATE_RESET_PERIOD_NAN', 'LLMA2_PRIME', 'LLMA2_SUBPRIME', 'LLMA2_APPVAL_LT_SALEPRICE', 'LLMA2_ORIG_RATE_SPREAD', 'LLMA2_ORIG_RATE_SPREAD_NAN', 'AGI', 'AGI_NAN', 'UR', 'UR_NAN', 'LLMA2_ORIG_RATE_ORIG_MR_SPREAD', 'LLMA2_ORIG_RATE_ORIG_MR_SPREAD_NAN', 'COUNT_INT_RATE_LESS', 'NUM_PRIME_ZIP', 'NUM_PRIME_ZIP_NAN', 'MBA_DELINQUENCY_STATUS_0', 'MBA_DELINQUENCY_STATUS_3', 'MBA_DELINQUENCY_STATUS_6', 'MBA_DELINQUENCY_STATUS_9', 'MBA_DELINQUENCY_STATUS_C', 'MBA_DELINQUENCY_STATUS_F', 'MBA_DELINQUENCY_STATUS_R', 'BUYDOWN_FLAG_N', 'BUYDOWN_FLAG_U', 'BUYDOWN_FLAG_Y', 'NEGATIVE_AMORTIZATION_FLAG_N', 'NEGATIVE_AMORTIZATION_FLAG_U', 'NEGATIVE_AMORTIZATION_FLAG_Y', 'PREPAY_PENALTY_FLAG_N', 'PREPAY_PENALTY_FLAG_U', 'PREPAY_PENALTY_FLAG_Y', 'OCCUPANCY_TYPE_1', 'OCCUPANCY_TYPE_2', 'OCCUPANCY_TYPE_3', 'OCCUPANCY_TYPE_U', 'PRODUCT_TYPE_10', 'PRODUCT_TYPE_20', 'PRODUCT_TYPE_30', 'PRODUCT_TYPE_40', 'PRODUCT_TYPE_50', 'PRODUCT_TYPE_51', 'PRODUCT_TYPE_52', 'PRODUCT_TYPE_53', 'PRODUCT_TYPE_54', 'PRODUCT_TYPE_5A', 'PRODUCT_TYPE_5Z', 'PRODUCT_TYPE_60', 'PRODUCT_TYPE_61', 'PRODUCT_TYPE_62', 'PRODUCT_TYPE_63', 'PRODUCT_TYPE_6Z', 'PRODUCT_TYPE_70', 'PRODUCT_TYPE_80', 'PRODUCT_TYPE_81', 'PRODUCT_TYPE_82', 'PRODUCT_TYPE_83', 'PRODUCT_TYPE_84', 'PRODUCT_TYPE_8Z', 'PRODUCT_TYPE_U', 'PROPERTY_TYPE_1', 'PROPERTY_TYPE_2', 'PROPERTY_TYPE_3', 'PROPERTY_TYPE_4', 'PROPERTY_TYPE_5', 'PROPERTY_TYPE_6', 'PROPERTY_TYPE_7', 'PROPERTY_TYPE_8', 'PROPERTY_TYPE_9', 'PROPERTY_TYPE_M', 'PROPERTY_TYPE_U', 'PROPERTY_TYPE_Z', 'LOAN_PURPOSE_CATEGORY_P', 'LOAN_PURPOSE_CATEGORY_R', 'LOAN_PURPOSE_CATEGORY_U', 'DOCUMENTATION_TYPE_1', 'DOCUMENTATION_TYPE_2', 'DOCUMENTATION_TYPE_3', 'DOCUMENTATION_TYPE_U', 'CHANNEL_1', 'CHANNEL_2', 'CHANNEL_3', 'CHANNEL_4', 'CHANNEL_5', 'CHANNEL_6', 'CHANNEL_7', 'CHANNEL_8', 'CHANNEL_9', 'CHANNEL_A', 'CHANNEL_B', 'CHANNEL_C', 'CHANNEL_D', 'CHANNEL_U', 'LOAN_TYPE_1', 'LOAN_TYPE_2', 'LOAN_TYPE_3', 'LOAN_TYPE_4', 'LOAN_TYPE_5', 'LOAN_TYPE_6', 'LOAN_TYPE_U', 'IO_FLAG_N', 'IO_FLAG_U', 'IO_FLAG_Y', 'CONVERTIBLE_FLAG_N', 'CONVERTIBLE_FLAG_U', 'CONVERTIBLE_FLAG_Y', 'POOL_INSURANCE_FLAG_N', 'POOL_INSURANCE_FLAG_U', 'POOL_INSURANCE_FLAG_Y', 'STATE_AK', 'STATE_AL', 'STATE_AR', 'STATE_AZ', 'STATE_CA', 'STATE_CO', 'STATE_CT', 'STATE_DC', 'STATE_DE', 'STATE_FL', 'STATE_GA', 'STATE_HI', 'STATE_IA', 'STATE_ID', 'STATE_IL', 'STATE_IN', 'STATE_KS', 'STATE_KY', 'STATE_LA', 'STATE_MA', 'STATE_MD', 'STATE_ME', 'STATE_MI', 'STATE_MN', 'STATE_MO', 'STATE_MS', 'STATE_MT', 'STATE_NC', 'STATE_ND', 'STATE_NE', 'STATE_NH', 'STATE_NJ', 'STATE_NM', 'STATE_NV', 'STATE_NY', 'STATE_OH', 'STATE_OK', 'STATE_OR', 'STATE_PA', 'STATE_PR', 'STATE_RI', 'STATE_SC', 'STATE_SD', 'STATE_TN', 'STATE_TX', 'STATE_UT', 'STATE_VA', 'STATE_VT', 'STATE_WA', 'STATE_WI', 'STATE_WV', 'STATE_WY', 'CURRENT_INVESTOR_CODE_240', 'CURRENT_INVESTOR_CODE_250', 'CURRENT_INVESTOR_CODE_253', 'CURRENT_INVESTOR_CODE_U', 'ORIGINATION_YEAR_B1995', 'ORIGINATION_YEAR_1995', 'ORIGINATION_YEAR_1996', 'ORIGINATION_YEAR_1997', 'ORIGINATION_YEAR_1998', 'ORIGINATION_YEAR_1999', 'ORIGINATION_YEAR_2000', 'ORIGINATION_YEAR_2001', 'ORIGINATION_YEAR_2002', 'ORIGINATION_YEAR_2003', 'ORIGINATION_YEAR_2004', 'ORIGINATION_YEAR_2005', 'ORIGINATION_YEAR_2006', 'ORIGINATION_YEAR_2007', 'ORIGINATION_YEAR_2008', 'ORIGINATION_YEAR_2009', 'ORIGINATION_YEAR_2010', 'ORIGINATION_YEAR_2011', 'ORIGINATION_YEAR_2012', 'ORIGINATION_YEAR_2013', 'ORIGINATION_YEAR_2014', 'ORIGINATION_YEAR_2015', 'ORIGINATION_YEAR_2016', 'ORIGINATION_YEAR_2017', 'ORIGINATION_YEAR_2018']\n",
      "Labels List:  ['DELINQUENCY_STATUS_NEXT_0', 'DELINQUENCY_STATUS_NEXT_3', 'DELINQUENCY_STATUS_NEXT_6', 'DELINQUENCY_STATUS_NEXT_9', 'DELINQUENCY_STATUS_NEXT_C', 'DELINQUENCY_STATUS_NEXT_F', 'DELINQUENCY_STATUS_NEXT_R']\n",
      "None\n",
      "70540\n",
      "Training features - Sample [[0.11811024 0.         1.431      ... 0.         0.         0.        ]\n",
      " [0.         0.         1.3775     ... 0.         0.         0.        ]\n",
      " [0.         0.         1.354      ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.         0.         1.345      ... 0.         0.         0.        ]\n",
      " [0.         0.         1.345      ... 0.         0.         0.        ]\n",
      " [0.         0.         1.345      ... 0.         0.         0.        ]]\n",
      "2018-10-20 00:05:07,805 - nn-real-hvd_0 - INFO - METRICS:  Namespace(allow_summaries=True, batch_layer_type='batch', batch_norm=True, batch_size=4425, batch_type='layer', data_dir='/input_data', decay_rate=1, decay_step=3520000, display_every=100, dropout=True, dropout_keep=0.9, epoch_flag=0, epoch_num=15, eval=False, ftp_dir='processed/c1mill_99-01', learning_rate=0.1, log_file=None, logdir=PosixPath('/home/ubuntu/real_summaries4425-15ep_test'), loss_tolerance=0.0001, lr_decay_epochs=30, lr_decay_policy='time', lr_decay_rate=0.1, lr_poly_power=2.0, max_epoch_size=-1, momentum=0.5, n_hidden=3, name='batch_layer_type', nstep_burnin=20, rate_min=0.0015, reg_rate=1e-05, s_hidden=[200, 140, 140], save_interval=450, stratified_flag=False, summary_interval=1800, test_batch_size=100000, test_dir='chuncks_random_c1millx2_test', test_flag=True, test_period=[286, 304], train_dir='chuncks_random_c1millx2_train', train_period=[121, 279], valid_batch_size=100000, valid_dir='chuncks_random_c1millx2_valid', valid_period=[280, 285], weighted_sampling=False, xla=True)\n",
      "\n",
      "2018-10-20 00:05:07,805 - nn-real-hvd_0 - INFO - METRICS:  Namespace(allow_summaries=True, batch_layer_type='batch', batch_norm=True, batch_size=4425, batch_type='layer', data_dir='/input_data', decay_rate=1, decay_step=3520000, display_every=100, dropout=True, dropout_keep=0.9, epoch_flag=0, epoch_num=15, eval=False, ftp_dir='processed/c1mill_99-01', learning_rate=0.1, log_file=None, logdir=PosixPath('/home/ubuntu/real_summaries4425-15ep_test'), loss_tolerance=0.0001, lr_decay_epochs=30, lr_decay_policy='time', lr_decay_rate=0.1, lr_poly_power=2.0, max_epoch_size=-1, momentum=0.5, n_hidden=3, name='batch_layer_type', nstep_burnin=20, rate_min=0.0015, reg_rate=1e-05, s_hidden=[200, 140, 140], save_interval=450, stratified_flag=False, summary_interval=1800, test_batch_size=100000, test_dir='chuncks_random_c1millx2_test', test_flag=True, test_period=[286, 304], train_dir='chuncks_random_c1millx2_train', train_period=[121, 279], valid_batch_size=100000, valid_dir='chuncks_random_c1millx2_valid', valid_period=[280, 285], weighted_sampling=False, xla=True)\n",
      "\n",
      "2018-10-20 00:05:07,806 - nn-real-hvd_0 - INFO - training files:  {0: {'path': '/home/ubuntu/MLMortgage/data/processed/chuncks_random_c1millx2_train/c1mill99-01_non_index_train_0.h5', 'nrows': 20809545, 'init_index': 0, 'end_index': 20809545, 'dataset': <class 'pandas.io.pytables.HDFStore'>\n",
      "File path: /home/ubuntu/MLMortgage/data/processed/chuncks_random_c1millx2_train/c1mill99-01_non_index_train_0.h5\n",
      ", 'dataset_features': array([[0.11811024, 0.        , 1.431     , ..., 0.        , 0.        ,\n",
      "        0.        ],\n",
      "       [0.        , 0.        , 1.3775    , ..., 0.        , 0.        ,\n",
      "        0.        ],\n",
      "       [0.        , 0.        , 1.354     , ..., 0.        , 0.        ,\n",
      "        0.        ],\n",
      "       ...,\n",
      "       [0.        , 0.        , 2.125     , ..., 0.        , 0.        ,\n",
      "        0.        ],\n",
      "       [0.        , 0.        , 2.125     , ..., 0.        , 0.        ,\n",
      "        0.        ],\n",
      "       [0.        , 0.        , 2.125     , ..., 0.        , 0.        ,\n",
      "        0.        ]], dtype=float32), 'dataset_labels': array([[0, 0, 0, ..., 1, 0, 0],\n",
      "       [0, 0, 0, ..., 1, 0, 0],\n",
      "       [0, 0, 0, ..., 1, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 0, ..., 1, 0, 0],\n",
      "       [0, 0, 0, ..., 1, 0, 0],\n",
      "       [0, 0, 0, ..., 1, 0, 0]], dtype=int8)}}\n",
      "\n",
      "2018-10-20 00:05:07,806 - nn-real-hvd_0 - INFO - training files:  {0: {'path': '/home/ubuntu/MLMortgage/data/processed/chuncks_random_c1millx2_train/c1mill99-01_non_index_train_0.h5', 'nrows': 20809545, 'init_index': 0, 'end_index': 20809545, 'dataset': <class 'pandas.io.pytables.HDFStore'>\n",
      "File path: /home/ubuntu/MLMortgage/data/processed/chuncks_random_c1millx2_train/c1mill99-01_non_index_train_0.h5\n",
      ", 'dataset_features': array([[0.11811024, 0.        , 1.431     , ..., 0.        , 0.        ,\n",
      "        0.        ],\n",
      "       [0.        , 0.        , 1.3775    , ..., 0.        , 0.        ,\n",
      "        0.        ],\n",
      "       [0.        , 0.        , 1.354     , ..., 0.        , 0.        ,\n",
      "        0.        ],\n",
      "       ...,\n",
      "       [0.        , 0.        , 2.125     , ..., 0.        , 0.        ,\n",
      "        0.        ],\n",
      "       [0.        , 0.        , 2.125     , ..., 0.        , 0.        ,\n",
      "        0.        ],\n",
      "       [0.        , 0.        , 2.125     , ..., 0.        , 0.        ,\n",
      "        0.        ]], dtype=float32), 'dataset_labels': array([[0, 0, 0, ..., 1, 0, 0],\n",
      "       [0, 0, 0, ..., 1, 0, 0],\n",
      "       [0, 0, 0, ..., 1, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 0, ..., 1, 0, 0],\n",
      "       [0, 0, 0, ..., 1, 0, 0],\n",
      "       [0, 0, 0, ..., 1, 0, 0]], dtype=int8)}}\n",
      "\n",
      "2018-10-20 00:05:07,807 - nn-real-hvd_0 - INFO - validation files:  {0: {'path': '/home/ubuntu/MLMortgage/data/processed/chuncks_random_c1millx2_valid/c1mill99-01_non_index_valid_0.h5', 'nrows': 4020339, 'init_index': 0, 'end_index': 4020339, 'dataset': <class 'pandas.io.pytables.HDFStore'>\n",
      "File path: /home/ubuntu/MLMortgage/data/processed/chuncks_random_c1millx2_valid/c1mill99-01_non_index_valid_0.h5\n",
      ", 'dataset_features': array([[0.    , 0.    , 1.8445, ..., 0.    , 0.    , 0.    ],\n",
      "       [0.    , 0.    , 1.8535, ..., 0.    , 0.    , 0.    ],\n",
      "       [0.    , 0.    , 1.8585, ..., 0.    , 0.    , 0.    ],\n",
      "       ...,\n",
      "       [0.    , 0.    , 2.125 , ..., 0.    , 0.    , 0.    ],\n",
      "       [0.    , 0.    , 2.125 , ..., 0.    , 0.    , 0.    ],\n",
      "       [0.    , 0.    , 2.125 , ..., 0.    , 0.    , 0.    ]],\n",
      "      dtype=float32), 'dataset_labels': array([[0, 0, 0, ..., 1, 0, 0],\n",
      "       [0, 0, 0, ..., 1, 0, 0],\n",
      "       [0, 0, 0, ..., 1, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 0, ..., 1, 0, 0],\n",
      "       [0, 0, 0, ..., 1, 0, 0],\n",
      "       [0, 0, 0, ..., 1, 0, 0]], dtype=int8)}}\n",
      "\n",
      "2018-10-20 00:05:07,807 - nn-real-hvd_0 - INFO - validation files:  {0: {'path': '/home/ubuntu/MLMortgage/data/processed/chuncks_random_c1millx2_valid/c1mill99-01_non_index_valid_0.h5', 'nrows': 4020339, 'init_index': 0, 'end_index': 4020339, 'dataset': <class 'pandas.io.pytables.HDFStore'>\n",
      "File path: /home/ubuntu/MLMortgage/data/processed/chuncks_random_c1millx2_valid/c1mill99-01_non_index_valid_0.h5\n",
      ", 'dataset_features': array([[0.    , 0.    , 1.8445, ..., 0.    , 0.    , 0.    ],\n",
      "       [0.    , 0.    , 1.8535, ..., 0.    , 0.    , 0.    ],\n",
      "       [0.    , 0.    , 1.8585, ..., 0.    , 0.    , 0.    ],\n",
      "       ...,\n",
      "       [0.    , 0.    , 2.125 , ..., 0.    , 0.    , 0.    ],\n",
      "       [0.    , 0.    , 2.125 , ..., 0.    , 0.    , 0.    ],\n",
      "       [0.    , 0.    , 2.125 , ..., 0.    , 0.    , 0.    ]],\n",
      "      dtype=float32), 'dataset_labels': array([[0, 0, 0, ..., 1, 0, 0],\n",
      "       [0, 0, 0, ..., 1, 0, 0],\n",
      "       [0, 0, 0, ..., 1, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 0, ..., 1, 0, 0],\n",
      "       [0, 0, 0, ..., 1, 0, 0],\n",
      "       [0, 0, 0, ..., 1, 0, 0]], dtype=int8)}}\n",
      "\n",
      "2018-10-20 00:05:07,807 - nn-real-hvd_0 - INFO - testing files:  None\n",
      "\n",
      "2018-10-20 00:05:07,807 - nn-real-hvd_0 - INFO - testing files:  None\n",
      "\n",
      "2018-10-20 00:05:07,811 - nn-real-hvd_0 - INFO - Building training graph\n",
      "2018-10-20 00:05:07,811 - nn-real-hvd_0 - INFO - Building training graph\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_features:  (4020339, 258)\n",
      "dict:  {'path': '/home/ubuntu/MLMortgage/data/processed/chuncks_random_c1millx2_valid/c1mill99-01_valid_non_index_0.h5', 'nrows': 4020339, 'init_index': 0, 'end_index': 4020339, 'dataset': <class 'pandas.io.pytables.HDFStore'>\n",
      "File path: /home/ubuntu/MLMortgage/data/processed/chuncks_random_c1millx2_valid/c1mill99-01_valid_non_index_0.h5\n",
      ", 'dataset_features': array([[0.    , 0.    , 1.8445, ..., 0.    , 0.    , 0.    ],\n",
      "       [0.    , 0.    , 1.8535, ..., 0.    , 0.    , 0.    ],\n",
      "       [0.    , 0.    , 1.8585, ..., 0.    , 0.    , 0.    ],\n",
      "       ...,\n",
      "       [0.    , 0.    , 2.125 , ..., 0.    , 0.    , 0.    ],\n",
      "       [0.    , 0.    , 2.125 , ..., 0.    , 0.    , 0.    ],\n",
      "       [0.    , 0.    , 2.125 , ..., 0.    , 0.    , 0.    ]],\n",
      "      dtype=float32), 'dataset_labels': array([[0, 0, 0, ..., 1, 0, 0],\n",
      "       [0, 0, 0, ..., 1, 0, 0],\n",
      "       [0, 0, 0, ..., 1, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 0, ..., 1, 0, 0],\n",
      "       [0, 0, 0, ..., 1, 0, 0],\n",
      "       [0, 0, 0, ..., 1, 0, 0]], dtype=int8)}  accumulated rows:  4020339\n",
      "Features List:  ['MBA_DAYS_DELINQUENT', 'MBA_DAYS_DELINQUENT_NAN', 'CURRENT_INTEREST_RATE', 'CURRENT_INTEREST_RATE_NAN', 'LOANAGE', 'LOANAGE_NAN', 'CURRENT_BALANCE', 'CURRENT_BALANCE_NAN', 'SCHEDULED_PRINCIPAL', 'SCHEDULED_PRINCIPAL_NAN', 'SCHEDULED_MONTHLY_PANDI', 'SCHEDULED_MONTHLY_PANDI_NAN', 'LLMA2_CURRENT_INTEREST_SPREAD', 'LLMA2_CURRENT_INTEREST_SPREAD_NAN', 'LLMA2_C_IN_LAST_12_MONTHS', 'LLMA2_30_IN_LAST_12_MONTHS', 'LLMA2_60_IN_LAST_12_MONTHS', 'LLMA2_90_IN_LAST_12_MONTHS', 'LLMA2_FC_IN_LAST_12_MONTHS', 'LLMA2_REO_IN_LAST_12_MONTHS', 'LLMA2_0_IN_LAST_12_MONTHS', 'LLMA2_HIST_LAST_12_MONTHS_MIS', 'NUM_MODIF', 'NUM_MODIF_NAN', 'P_RATE_TO_MOD', 'P_RATE_TO_MOD_NAN', 'MOD_RATE', 'MOD_RATE_NAN', 'DIF_RATE', 'DIF_RATE_NAN', 'P_MONTHLY_PAY', 'P_MONTHLY_PAY_NAN', 'MOD_MONTHLY_PAY', 'MOD_MONTHLY_PAY_NAN', 'DIF_MONTHLY_PAY', 'DIF_MONTHLY_PAY_NAN', 'CAPITALIZATION_AMT', 'CAPITALIZATION_AMT_NAN', 'MORTGAGE_RATE', 'MORTGAGE_RATE_NAN', 'FICO_SCORE_ORIGINATION', 'INITIAL_INTEREST_RATE', 'ORIGINAL_LTV', 'ORIGINAL_BALANCE', 'BACKEND_RATIO', 'BACKEND_RATIO_NAN', 'ORIGINAL_TERM', 'ORIGINAL_TERM_NAN', 'SALE_PRICE', 'SALE_PRICE_NAN', 'PREPAY_PENALTY_TERM', 'PREPAY_PENALTY_TERM_NAN', 'NUMBER_OF_UNITS', 'NUMBER_OF_UNITS_NAN', 'MARGIN', 'MARGIN_NAN', 'PERIODIC_RATE_CAP', 'PERIODIC_RATE_CAP_NAN', 'PERIODIC_RATE_FLOOR', 'PERIODIC_RATE_FLOOR_NAN', 'LIFETIME_RATE_CAP', 'LIFETIME_RATE_CAP_NAN', 'LIFETIME_RATE_FLOOR', 'LIFETIME_RATE_FLOOR_NAN', 'RATE_RESET_FREQUENCY', 'RATE_RESET_FREQUENCY_NAN', 'PAY_RESET_FREQUENCY', 'PAY_RESET_FREQUENCY_NAN', 'FIRST_RATE_RESET_PERIOD', 'FIRST_RATE_RESET_PERIOD_NAN', 'LLMA2_PRIME', 'LLMA2_SUBPRIME', 'LLMA2_APPVAL_LT_SALEPRICE', 'LLMA2_ORIG_RATE_SPREAD', 'LLMA2_ORIG_RATE_SPREAD_NAN', 'AGI', 'AGI_NAN', 'UR', 'UR_NAN', 'LLMA2_ORIG_RATE_ORIG_MR_SPREAD', 'LLMA2_ORIG_RATE_ORIG_MR_SPREAD_NAN', 'COUNT_INT_RATE_LESS', 'NUM_PRIME_ZIP', 'NUM_PRIME_ZIP_NAN', 'MBA_DELINQUENCY_STATUS_0', 'MBA_DELINQUENCY_STATUS_3', 'MBA_DELINQUENCY_STATUS_6', 'MBA_DELINQUENCY_STATUS_9', 'MBA_DELINQUENCY_STATUS_C', 'MBA_DELINQUENCY_STATUS_F', 'MBA_DELINQUENCY_STATUS_R', 'BUYDOWN_FLAG_N', 'BUYDOWN_FLAG_U', 'BUYDOWN_FLAG_Y', 'NEGATIVE_AMORTIZATION_FLAG_N', 'NEGATIVE_AMORTIZATION_FLAG_U', 'NEGATIVE_AMORTIZATION_FLAG_Y', 'PREPAY_PENALTY_FLAG_N', 'PREPAY_PENALTY_FLAG_U', 'PREPAY_PENALTY_FLAG_Y', 'OCCUPANCY_TYPE_1', 'OCCUPANCY_TYPE_2', 'OCCUPANCY_TYPE_3', 'OCCUPANCY_TYPE_U', 'PRODUCT_TYPE_10', 'PRODUCT_TYPE_20', 'PRODUCT_TYPE_30', 'PRODUCT_TYPE_40', 'PRODUCT_TYPE_50', 'PRODUCT_TYPE_51', 'PRODUCT_TYPE_52', 'PRODUCT_TYPE_53', 'PRODUCT_TYPE_54', 'PRODUCT_TYPE_5A', 'PRODUCT_TYPE_5Z', 'PRODUCT_TYPE_60', 'PRODUCT_TYPE_61', 'PRODUCT_TYPE_62', 'PRODUCT_TYPE_63', 'PRODUCT_TYPE_6Z', 'PRODUCT_TYPE_70', 'PRODUCT_TYPE_80', 'PRODUCT_TYPE_81', 'PRODUCT_TYPE_82', 'PRODUCT_TYPE_83', 'PRODUCT_TYPE_84', 'PRODUCT_TYPE_8Z', 'PRODUCT_TYPE_U', 'PROPERTY_TYPE_1', 'PROPERTY_TYPE_2', 'PROPERTY_TYPE_3', 'PROPERTY_TYPE_4', 'PROPERTY_TYPE_5', 'PROPERTY_TYPE_6', 'PROPERTY_TYPE_7', 'PROPERTY_TYPE_8', 'PROPERTY_TYPE_9', 'PROPERTY_TYPE_M', 'PROPERTY_TYPE_U', 'PROPERTY_TYPE_Z', 'LOAN_PURPOSE_CATEGORY_P', 'LOAN_PURPOSE_CATEGORY_R', 'LOAN_PURPOSE_CATEGORY_U', 'DOCUMENTATION_TYPE_1', 'DOCUMENTATION_TYPE_2', 'DOCUMENTATION_TYPE_3', 'DOCUMENTATION_TYPE_U', 'CHANNEL_1', 'CHANNEL_2', 'CHANNEL_3', 'CHANNEL_4', 'CHANNEL_5', 'CHANNEL_6', 'CHANNEL_7', 'CHANNEL_8', 'CHANNEL_9', 'CHANNEL_A', 'CHANNEL_B', 'CHANNEL_C', 'CHANNEL_D', 'CHANNEL_U', 'LOAN_TYPE_1', 'LOAN_TYPE_2', 'LOAN_TYPE_3', 'LOAN_TYPE_4', 'LOAN_TYPE_5', 'LOAN_TYPE_6', 'LOAN_TYPE_U', 'IO_FLAG_N', 'IO_FLAG_U', 'IO_FLAG_Y', 'CONVERTIBLE_FLAG_N', 'CONVERTIBLE_FLAG_U', 'CONVERTIBLE_FLAG_Y', 'POOL_INSURANCE_FLAG_N', 'POOL_INSURANCE_FLAG_U', 'POOL_INSURANCE_FLAG_Y', 'STATE_AK', 'STATE_AL', 'STATE_AR', 'STATE_AZ', 'STATE_CA', 'STATE_CO', 'STATE_CT', 'STATE_DC', 'STATE_DE', 'STATE_FL', 'STATE_GA', 'STATE_HI', 'STATE_IA', 'STATE_ID', 'STATE_IL', 'STATE_IN', 'STATE_KS', 'STATE_KY', 'STATE_LA', 'STATE_MA', 'STATE_MD', 'STATE_ME', 'STATE_MI', 'STATE_MN', 'STATE_MO', 'STATE_MS', 'STATE_MT', 'STATE_NC', 'STATE_ND', 'STATE_NE', 'STATE_NH', 'STATE_NJ', 'STATE_NM', 'STATE_NV', 'STATE_NY', 'STATE_OH', 'STATE_OK', 'STATE_OR', 'STATE_PA', 'STATE_PR', 'STATE_RI', 'STATE_SC', 'STATE_SD', 'STATE_TN', 'STATE_TX', 'STATE_UT', 'STATE_VA', 'STATE_VT', 'STATE_WA', 'STATE_WI', 'STATE_WV', 'STATE_WY', 'CURRENT_INVESTOR_CODE_240', 'CURRENT_INVESTOR_CODE_250', 'CURRENT_INVESTOR_CODE_253', 'CURRENT_INVESTOR_CODE_U', 'ORIGINATION_YEAR_B1995', 'ORIGINATION_YEAR_1995', 'ORIGINATION_YEAR_1996', 'ORIGINATION_YEAR_1997', 'ORIGINATION_YEAR_1998', 'ORIGINATION_YEAR_1999', 'ORIGINATION_YEAR_2000', 'ORIGINATION_YEAR_2001', 'ORIGINATION_YEAR_2002', 'ORIGINATION_YEAR_2003', 'ORIGINATION_YEAR_2004', 'ORIGINATION_YEAR_2005', 'ORIGINATION_YEAR_2006', 'ORIGINATION_YEAR_2007', 'ORIGINATION_YEAR_2008', 'ORIGINATION_YEAR_2009', 'ORIGINATION_YEAR_2010', 'ORIGINATION_YEAR_2011', 'ORIGINATION_YEAR_2012', 'ORIGINATION_YEAR_2013', 'ORIGINATION_YEAR_2014', 'ORIGINATION_YEAR_2015', 'ORIGINATION_YEAR_2016', 'ORIGINATION_YEAR_2017', 'ORIGINATION_YEAR_2018']\n",
      "Labels List:  ['DELINQUENCY_STATUS_NEXT_0', 'DELINQUENCY_STATUS_NEXT_3', 'DELINQUENCY_STATUS_NEXT_6', 'DELINQUENCY_STATUS_NEXT_9', 'DELINQUENCY_STATUS_NEXT_C', 'DELINQUENCY_STATUS_NEXT_F', 'DELINQUENCY_STATUS_NEXT_R']\n",
      "None\n",
      "70540\n",
      "Training features - Sample [[0.11811024 0.         1.431      ... 0.         0.         0.        ]\n",
      " [0.         0.         1.3775     ... 0.         0.         0.        ]\n",
      " [0.         0.         1.354      ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.         0.         1.345      ... 0.         0.         0.        ]\n",
      " [0.         0.         1.345      ... 0.         0.         0.        ]\n",
      " [0.         0.         1.345      ... 0.         0.         0.        ]]\n",
      "2018-10-20 00:05:08,010 - nn-real-hvd_1 - INFO - METRICS:  Namespace(allow_summaries=True, batch_layer_type='batch', batch_norm=True, batch_size=4425, batch_type='layer', data_dir='/input_data', decay_rate=1, decay_step=3520000, display_every=100, dropout=True, dropout_keep=0.9, epoch_flag=0, epoch_num=15, eval=False, ftp_dir='processed/c1mill_99-01', learning_rate=0.1, log_file=None, logdir=PosixPath('/home/ubuntu/real_summaries4425-15ep_test'), loss_tolerance=0.0001, lr_decay_epochs=30, lr_decay_policy='time', lr_decay_rate=0.1, lr_poly_power=2.0, max_epoch_size=-1, momentum=0.5, n_hidden=3, name='batch_layer_type', nstep_burnin=20, rate_min=0.0015, reg_rate=1e-05, s_hidden=[200, 140, 140], save_interval=450, stratified_flag=False, summary_interval=1800, test_batch_size=100000, test_dir='chuncks_random_c1millx2_test', test_flag=True, test_period=[286, 304], train_dir='chuncks_random_c1millx2_train', train_period=[121, 279], valid_batch_size=100000, valid_dir='chuncks_random_c1millx2_valid', valid_period=[280, 285], weighted_sampling=False, xla=True)\n",
      "\n",
      "2018-10-20 00:05:08,010 - nn-real-hvd_1 - INFO - METRICS:  Namespace(allow_summaries=True, batch_layer_type='batch', batch_norm=True, batch_size=4425, batch_type='layer', data_dir='/input_data', decay_rate=1, decay_step=3520000, display_every=100, dropout=True, dropout_keep=0.9, epoch_flag=0, epoch_num=15, eval=False, ftp_dir='processed/c1mill_99-01', learning_rate=0.1, log_file=None, logdir=PosixPath('/home/ubuntu/real_summaries4425-15ep_test'), loss_tolerance=0.0001, lr_decay_epochs=30, lr_decay_policy='time', lr_decay_rate=0.1, lr_poly_power=2.0, max_epoch_size=-1, momentum=0.5, n_hidden=3, name='batch_layer_type', nstep_burnin=20, rate_min=0.0015, reg_rate=1e-05, s_hidden=[200, 140, 140], save_interval=450, stratified_flag=False, summary_interval=1800, test_batch_size=100000, test_dir='chuncks_random_c1millx2_test', test_flag=True, test_period=[286, 304], train_dir='chuncks_random_c1millx2_train', train_period=[121, 279], valid_batch_size=100000, valid_dir='chuncks_random_c1millx2_valid', valid_period=[280, 285], weighted_sampling=False, xla=True)\n",
      "\n",
      "2018-10-20 00:05:08,011 - nn-real-hvd_1 - INFO - training files:  {0: {'path': '/home/ubuntu/MLMortgage/data/processed/chuncks_random_c1millx2_train/c1mill99-01_train_non_index_0.h5', 'nrows': 20809545, 'init_index': 0, 'end_index': 20809545, 'dataset': <class 'pandas.io.pytables.HDFStore'>\n",
      "File path: /home/ubuntu/MLMortgage/data/processed/chuncks_random_c1millx2_train/c1mill99-01_train_non_index_0.h5\n",
      ", 'dataset_features': array([[0.11811024, 0.        , 1.431     , ..., 0.        , 0.        ,\n",
      "        0.        ],\n",
      "       [0.        , 0.        , 1.3775    , ..., 0.        , 0.        ,\n",
      "        0.        ],\n",
      "       [0.        , 0.        , 1.354     , ..., 0.        , 0.        ,\n",
      "        0.        ],\n",
      "       ...,\n",
      "       [0.        , 0.        , 2.125     , ..., 0.        , 0.        ,\n",
      "        0.        ],\n",
      "       [0.        , 0.        , 2.125     , ..., 0.        , 0.        ,\n",
      "        0.        ],\n",
      "       [0.        , 0.        , 2.125     , ..., 0.        , 0.        ,\n",
      "        0.        ]], dtype=float32), 'dataset_labels': array([[0, 0, 0, ..., 1, 0, 0],\n",
      "       [0, 0, 0, ..., 1, 0, 0],\n",
      "       [0, 0, 0, ..., 1, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 0, ..., 1, 0, 0],\n",
      "       [0, 0, 0, ..., 1, 0, 0],\n",
      "       [0, 0, 0, ..., 1, 0, 0]], dtype=int8)}}\n",
      "\n",
      "2018-10-20 00:05:08,011 - nn-real-hvd_1 - INFO - training files:  {0: {'path': '/home/ubuntu/MLMortgage/data/processed/chuncks_random_c1millx2_train/c1mill99-01_train_non_index_0.h5', 'nrows': 20809545, 'init_index': 0, 'end_index': 20809545, 'dataset': <class 'pandas.io.pytables.HDFStore'>\n",
      "File path: /home/ubuntu/MLMortgage/data/processed/chuncks_random_c1millx2_train/c1mill99-01_train_non_index_0.h5\n",
      ", 'dataset_features': array([[0.11811024, 0.        , 1.431     , ..., 0.        , 0.        ,\n",
      "        0.        ],\n",
      "       [0.        , 0.        , 1.3775    , ..., 0.        , 0.        ,\n",
      "        0.        ],\n",
      "       [0.        , 0.        , 1.354     , ..., 0.        , 0.        ,\n",
      "        0.        ],\n",
      "       ...,\n",
      "       [0.        , 0.        , 2.125     , ..., 0.        , 0.        ,\n",
      "        0.        ],\n",
      "       [0.        , 0.        , 2.125     , ..., 0.        , 0.        ,\n",
      "        0.        ],\n",
      "       [0.        , 0.        , 2.125     , ..., 0.        , 0.        ,\n",
      "        0.        ]], dtype=float32), 'dataset_labels': array([[0, 0, 0, ..., 1, 0, 0],\n",
      "       [0, 0, 0, ..., 1, 0, 0],\n",
      "       [0, 0, 0, ..., 1, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 0, ..., 1, 0, 0],\n",
      "       [0, 0, 0, ..., 1, 0, 0],\n",
      "       [0, 0, 0, ..., 1, 0, 0]], dtype=int8)}}\n",
      "\n",
      "2018-10-20 00:05:08,012 - nn-real-hvd_1 - INFO - validation files:  {0: {'path': '/home/ubuntu/MLMortgage/data/processed/chuncks_random_c1millx2_valid/c1mill99-01_valid_non_index_0.h5', 'nrows': 4020339, 'init_index': 0, 'end_index': 4020339, 'dataset': <class 'pandas.io.pytables.HDFStore'>\n",
      "File path: /home/ubuntu/MLMortgage/data/processed/chuncks_random_c1millx2_valid/c1mill99-01_valid_non_index_0.h5\n",
      ", 'dataset_features': array([[0.    , 0.    , 1.8445, ..., 0.    , 0.    , 0.    ],\n",
      "       [0.    , 0.    , 1.8535, ..., 0.    , 0.    , 0.    ],\n",
      "       [0.    , 0.    , 1.8585, ..., 0.    , 0.    , 0.    ],\n",
      "       ...,\n",
      "       [0.    , 0.    , 2.125 , ..., 0.    , 0.    , 0.    ],\n",
      "       [0.    , 0.    , 2.125 , ..., 0.    , 0.    , 0.    ],\n",
      "       [0.    , 0.    , 2.125 , ..., 0.    , 0.    , 0.    ]],\n",
      "      dtype=float32), 'dataset_labels': array([[0, 0, 0, ..., 1, 0, 0],\n",
      "       [0, 0, 0, ..., 1, 0, 0],\n",
      "       [0, 0, 0, ..., 1, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 0, ..., 1, 0, 0],\n",
      "       [0, 0, 0, ..., 1, 0, 0],\n",
      "       [0, 0, 0, ..., 1, 0, 0]], dtype=int8)}}\n",
      "\n",
      "2018-10-20 00:05:08,012 - nn-real-hvd_1 - INFO - validation files:  {0: {'path': '/home/ubuntu/MLMortgage/data/processed/chuncks_random_c1millx2_valid/c1mill99-01_valid_non_index_0.h5', 'nrows': 4020339, 'init_index': 0, 'end_index': 4020339, 'dataset': <class 'pandas.io.pytables.HDFStore'>\n",
      "File path: /home/ubuntu/MLMortgage/data/processed/chuncks_random_c1millx2_valid/c1mill99-01_valid_non_index_0.h5\n",
      ", 'dataset_features': array([[0.    , 0.    , 1.8445, ..., 0.    , 0.    , 0.    ],\n",
      "       [0.    , 0.    , 1.8535, ..., 0.    , 0.    , 0.    ],\n",
      "       [0.    , 0.    , 1.8585, ..., 0.    , 0.    , 0.    ],\n",
      "       ...,\n",
      "       [0.    , 0.    , 2.125 , ..., 0.    , 0.    , 0.    ],\n",
      "       [0.    , 0.    , 2.125 , ..., 0.    , 0.    , 0.    ],\n",
      "       [0.    , 0.    , 2.125 , ..., 0.    , 0.    , 0.    ]],\n",
      "      dtype=float32), 'dataset_labels': array([[0, 0, 0, ..., 1, 0, 0],\n",
      "       [0, 0, 0, ..., 1, 0, 0],\n",
      "       [0, 0, 0, ..., 1, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 0, ..., 1, 0, 0],\n",
      "       [0, 0, 0, ..., 1, 0, 0],\n",
      "       [0, 0, 0, ..., 1, 0, 0]], dtype=int8)}}\n",
      "\n",
      "2018-10-20 00:05:08,012 - nn-real-hvd_1 - INFO - testing files:  None\n",
      "\n",
      "2018-10-20 00:05:08,012 - nn-real-hvd_1 - INFO - testing files:  None\n",
      "\n",
      "2018-10-20 00:05:08,017 - nn-real-hvd_1 - INFO - Building training graph\n",
      "2018-10-20 00:05:08,017 - nn-real-hvd_1 - INFO - Building training graph\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/contrib/metrics/python/ops/histogram_ops.py:83: remove_squeezable_dimensions (from tensorflow.contrib.framework.python.framework.tensor_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please switch to remove_squeezable_dimensions from tf.confusion_matrix. Note that the order of the inputs and outputs of labels and predictions have also been switched.\n",
      "2018-10-20 00:05:11,396 - tensorflow - WARNING - From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/contrib/metrics/python/ops/histogram_ops.py:83: remove_squeezable_dimensions (from tensorflow.contrib.framework.python.framework.tensor_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please switch to remove_squeezable_dimensions from tf.confusion_matrix. Note that the order of the inputs and outputs of labels and predictions have also been switched.\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/contrib/metrics/python/ops/histogram_ops.py:83: remove_squeezable_dimensions (from tensorflow.contrib.framework.python.framework.tensor_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please switch to remove_squeezable_dimensions from tf.confusion_matrix. Note that the order of the inputs and outputs of labels and predictions have also been switched.\n",
      "2018-10-20 00:05:12,228 - tensorflow - WARNING - From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/contrib/metrics/python/ops/histogram_ops.py:83: remove_squeezable_dimensions (from tensorflow.contrib.framework.python.framework.tensor_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please switch to remove_squeezable_dimensions from tf.confusion_matrix. Note that the order of the inputs and outputs of labels and predictions have also been switched.\n",
      "2018-10-20 00:05:23,483 - nn-real-hvd_1 - INFO - Graph building completed....\n",
      "2018-10-20 00:05:23,483 - nn-real-hvd_1 - INFO - Graph building completed....\n",
      "2018-10-20 00:05:23,483 - nn-real-hvd_1 - INFO - Creating session\n",
      "2018-10-20 00:05:23,483 - nn-real-hvd_1 - INFO - Creating session\n",
      "<__main__.FeedForwardTrainer object at 0x7f5d8c10a780>\n",
      "2018-10-20 00:05:23.573910: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2018-10-20 00:05:23.574320: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1356] Found device 0 with properties: \n",
      "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
      "pciBusID: 0000:00:1e.0\n",
      "totalMemory: 11.17GiB freeMemory: 11.10GiB\n",
      "2018-10-20 00:05:23.574348: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1435] Adding visible gpu devices: 0\n",
      "2018-10-20 00:05:23.823986: I tensorflow/core/common_runtime/gpu/gpu_device.cc:923] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2018-10-20 00:05:23.824035: I tensorflow/core/common_runtime/gpu/gpu_device.cc:929]      0 \n",
      "2018-10-20 00:05:23.824044: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 0:   N \n",
      "2018-10-20 00:05:23.824354: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10760 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:1e.0, compute capability: 3.7)\n",
      "2018-10-20 00:05:23,885 - nn-real-hvd_1 - INFO - Initializing variables\n",
      "2018-10-20 00:05:23,885 - nn-real-hvd_1 - INFO - Initializing variables\n",
      "2018-10-20 00:05:24,790 - nn-real-hvd_0 - INFO - Graph building completed....\n",
      "2018-10-20 00:05:24,790 - nn-real-hvd_0 - INFO - Graph building completed....\n",
      "2018-10-20 00:05:24,791 - nn-real-hvd_0 - INFO - Creating session\n",
      "2018-10-20 00:05:24,791 - nn-real-hvd_0 - INFO - Creating session\n",
      "<__main__.FeedForwardTrainer object at 0x7efe2c0168d0>\n",
      "2018-10-20 00:05:24.857825: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2018-10-20 00:05:24.858108: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1356] Found device 0 with properties: \n",
      "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
      "pciBusID: 0000:00:1e.0\n",
      "totalMemory: 11.17GiB freeMemory: 482.00MiB\n",
      "2018-10-20 00:05:24.858141: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1435] Adding visible gpu devices: 0\n",
      "2018-10-20 00:05:25.112472: I tensorflow/core/common_runtime/gpu/gpu_device.cc:923] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2018-10-20 00:05:25.112524: I tensorflow/core/common_runtime/gpu/gpu_device.cc:929]      0 \n",
      "2018-10-20 00:05:25.112539: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 0:   N \n",
      "2018-10-20 00:05:25.112718: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 215 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:1e.0, compute capability: 3.7)\n",
      "2018-10-20 00:05:27,867 - nn-real-hvd_0 - INFO - Initializing variables\n",
      "2018-10-20 00:05:27,867 - nn-real-hvd_0 - INFO - Initializing variables\n",
      "INFO:tensorflow:Restoring parameters from /home/ubuntu/real_summaries4425-15ep_test/checkpoint-0\n",
      "2018-10-20 00:05:28,527 - tensorflow - INFO - Restoring parameters from /home/ubuntu/real_summaries4425-15ep_test/checkpoint-0\n",
      "2018-10-20 00:05:28,671 - nn-real-hvd_0 - INFO - Restored session from checkpoint /home/ubuntu/real_summaries4425-15ep_test/checkpoint-0\n",
      "2018-10-20 00:05:28,671 - nn-real-hvd_0 - INFO - Restored session from checkpoint /home/ubuntu/real_summaries4425-15ep_test/checkpoint-0\n",
      "2018-10-20 00:05:29,223 - nn-real-hvd_0 - INFO - Writing summaries to /home/ubuntu/real_summaries4425-15ep_test\n",
      "2018-10-20 00:05:29,223 - nn-real-hvd_0 - INFO - Writing summaries to /home/ubuntu/real_summaries4425-15ep_test\n",
      "2018-10-20 00:05:29,223 - nn-real-hvd_0 - INFO - Training\n",
      "2018-10-20 00:05:29,223 - nn-real-hvd_0 - INFO - Training\n",
      "2018-10-20 00:05:29,223 - nn-real-hvd_0 - INFO -   Step; Epoch; time-per-record(sec);  batchtime/worker(sec);  Loss;   Learning Rate; Accuracy; better_acc\n",
      "2018-10-20 00:05:29,223 - nn-real-hvd_0 - INFO -   Step; Epoch; time-per-record(sec);  batchtime/worker(sec);  Loss;   Learning Rate; Accuracy; better_acc\n",
      "2018-10-20 00:05:29,226 - nn-real-hvd_1 - INFO - Writing summaries to /home/ubuntu/real_summaries4425-15ep_test\n",
      "2018-10-20 00:05:29,226 - nn-real-hvd_1 - INFO - Writing summaries to /home/ubuntu/real_summaries4425-15ep_test\n",
      "2018-10-20 00:05:29,226 - nn-real-hvd_1 - INFO - Training\n",
      "2018-10-20 00:05:29,226 - nn-real-hvd_1 - INFO - Training\n",
      "2018-10-20 00:05:29,226 - nn-real-hvd_1 - INFO -   Step; Epoch; time-per-record(sec);  batchtime/worker(sec);  Loss;   Learning Rate; Accuracy; better_acc\n",
      "2018-10-20 00:05:29,226 - nn-real-hvd_1 - INFO -   Step; Epoch; time-per-record(sec);  batchtime/worker(sec);  Loss;   Learning Rate; Accuracy; better_acc\n",
      "Empty DataFrame\n",
      "Columns: [step, epoch, batch_time, Loss, LogLoss, Accuracy, Better-Accuracy, M-Measure Mean, AUC_AOC Mean, AUC_PR Mean]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [step, epoch, batch_time, Loss, LogLoss, Accuracy, Better-Accuracy]\n",
      "Index: []\n",
      "ip-172-31-34-84:4701:4712 [0] INFO NET : Using interface eth0:172.31.34.84<0>\n",
      "ip-172-31-34-84:4701:4712 [0] INFO NET/IB : Using interface eth0 for sideband communication\n",
      "ip-172-31-34-84:4701:4712 [0] INFO Using internal Network Socket\n",
      "ip-172-31-34-84:4701:4712 [0] INFO Using NCCL Low-latency algorithm for sizes below 16384\n",
      "ip-172-31-34-84:4701:4712 [0] INFO NET : Using interface eth0:172.31.34.84<0>\n",
      "ip-172-31-34-84:4701:4712 [0] INFO NET/Socket : 1 interfaces found\n",
      "NCCL version 2.1.2+cuda9.0\n",
      "ip-172-31-33-117:8209:8220 [0] INFO NET : Using interface eth0:172.31.33.117<0>\n",
      "ip-172-31-33-117:8209:8220 [0] INFO NET/IB : Using interface eth0 for sideband communication\n",
      "ip-172-31-33-117:8209:8220 [0] INFO Using internal Network Socket\n",
      "ip-172-31-33-117:8209:8220 [0] INFO Using NCCL Low-latency algorithm for sizes below 16384\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ip-172-31-33-117:8209:8220 [0] INFO NET : Using interface eth0:172.31.33.117<0>\n",
      "ip-172-31-33-117:8209:8220 [0] INFO NET/Socket : 1 interfaces found\n",
      "ip-172-31-34-84:4701:4712 [0] INFO Using 512 threads\n",
      "ip-172-31-34-84:4701:4712 [0] INFO Min Comp Cap 3\n",
      "ip-172-31-34-84:4701:4712 [0] INFO NCCL_SINGLE_RING_THRESHOLD=131072\n",
      "ip-172-31-34-84:4701:4712 [0] INFO [0] Ring 0 :    0   1\n",
      "ip-172-31-34-84:4701:4712 [0] INFO 1 -> 0 via NET/Socket/0\n",
      "ip-172-31-33-117:8209:8220 [0] INFO 0 -> 1 via NET/Socket/0\n",
      "ip-172-31-34-84:4701:4712 [0] INFO Launch mode Parallel\n",
      "2018-10-20 00:05:37,994 - nn-real-hvd_1 - INFO -      1;     1;  1020.6;   8.671; 2.23040; 0.20000; 0.04249; 0.15616\n",
      "2018-10-20 00:05:37,994 - nn-real-hvd_1 - INFO -      1;     1;  1020.6;   8.671; 2.23040; 0.20000; 0.04249; 0.15616\n",
      "2018-10-20 00:05:37,995 - nn-real-hvd_0 - INFO -      1;     1;  1021.1;   8.667; 2.21642; 0.20000; 0.04475; 0.13823\n",
      "2018-10-20 00:05:37,995 - nn-real-hvd_0 - INFO -      1;     1;  1021.1;   8.667; 2.21642; 0.20000; 0.04475; 0.13823\n",
      "2018-10-20 00:05:37,995 - nn-real-hvd_0 - INFO - METRICS each 1800 (secs):  Loss in ---Training in Summary---: 2.21642\n",
      " Avg Log_Loss in ---Training in Summary---: 0.46401\n",
      "---Training in Summary---: (Silly) Global-ACC=0.04475, Better ACC=0.13823, Avg M-Measure=nan, Avg AUC_AOC=nan Avg AUC_PR=0.0000\n",
      "Total Confusion Matrix;Total M-Measure Matrix;Total AUC_AOC;Total AUC_PR\n",
      "0;12;3;6;1;0;0;;0.0000;0.4676;0.5176;0.4069;0.3691;0.5666;0.0000;;0.3722 ;0.0000\n",
      "0;107;16;25;6;0;1;;0.5543;0.0000;0.5230;0.4661;0.4740;0.3441;0.0000;;0.4733 ;0.0000\n",
      "0;17;2;3;0;0;0;;0.3306;0.4265;0.0000;0.5346;0.3922;0.5211;0.0000;;0.3963 ;0.0000\n",
      "0;28;6;7;0;0;1;;0.4621;0.4866;0.4118;0.0000;0.4596;0.5230;0.0000;;0.4602 ;0.0000\n",
      "0;2806;456;796;82;0;16;;0.3865;0.4248;0.5177;0.5206;0.0000;0.5653;0.0000;;0.4543 ;0.0000\n",
      "0;18;3;4;3;0;0;;0.6607;0.6828;0.6291;0.6675;0.7036;0.0000;0.0000;;0.6851 ;0.0000\n",
      "0;0;0;0;0;0;0;;0.0000;0.0000;0.0000;0.0000;0.0000;0.0000;0.0000;;0.0000 ;0.0000\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "2018-10-20 00:05:37,995 - nn-real-hvd_0 - INFO - METRICS each 1800 (secs):  Loss in ---Training in Summary---: 2.21642\n",
      " Avg Log_Loss in ---Training in Summary---: 0.46401\n",
      "---Training in Summary---: (Silly) Global-ACC=0.04475, Better ACC=0.13823, Avg M-Measure=nan, Avg AUC_AOC=nan Avg AUC_PR=0.0000\n",
      "Total Confusion Matrix;Total M-Measure Matrix;Total AUC_AOC;Total AUC_PR\n",
      "0;12;3;6;1;0;0;;0.0000;0.4676;0.5176;0.4069;0.3691;0.5666;0.0000;;0.3722 ;0.0000\n",
      "0;107;16;25;6;0;1;;0.5543;0.0000;0.5230;0.4661;0.4740;0.3441;0.0000;;0.4733 ;0.0000\n",
      "0;17;2;3;0;0;0;;0.3306;0.4265;0.0000;0.5346;0.3922;0.5211;0.0000;;0.3963 ;0.0000\n",
      "0;28;6;7;0;0;1;;0.4621;0.4866;0.4118;0.0000;0.4596;0.5230;0.0000;;0.4602 ;0.0000\n",
      "0;2806;456;796;82;0;16;;0.3865;0.4248;0.5177;0.5206;0.0000;0.5653;0.0000;;0.4543 ;0.0000\n",
      "0;18;3;4;3;0;0;;0.6607;0.6828;0.6291;0.6675;0.7036;0.0000;0.0000;;0.6851 ;0.0000\n",
      "0;0;0;0;0;0;0;;0.0000;0.0000;0.0000;0.0000;0.0000;0.0000;0.0000;;0.0000 ;0.0000\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "2018-10-20 00:05:48.286651: W tensorflow/core/common_runtime/bfc_allocator.cc:275] Allocator (GPU_0_bfc) ran out of memory trying to allocate 98.42MiB.  Current allocation summary follows.\n",
      "2018-10-20 00:05:48.286724: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (256): \tTotal Chunks: 48, Chunks in use: 48. 12.0KiB allocated for chunks. 12.0KiB in use in bin. 360B client-requested in use in bin.\n",
      "2018-10-20 00:05:48.286753: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (512): \tTotal Chunks: 16, Chunks in use: 16. 12.0KiB allocated for chunks. 12.0KiB in use in bin. 8.8KiB client-requested in use in bin.\n",
      "2018-10-20 00:05:48.286766: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (1024): \tTotal Chunks: 9, Chunks in use: 9. 9.2KiB allocated for chunks. 9.2KiB in use in bin. 7.3KiB client-requested in use in bin.\n",
      "2018-10-20 00:05:48.286777: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (2048): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2018-10-20 00:05:48.286790: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (4096): \tTotal Chunks: 3, Chunks in use: 3. 12.0KiB allocated for chunks. 12.0KiB in use in bin. 11.5KiB client-requested in use in bin.\n",
      "2018-10-20 00:05:48.286801: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (8192): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2018-10-20 00:05:48.286811: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (16384): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2018-10-20 00:05:48.286821: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (32768): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2018-10-20 00:05:48.286834: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (65536): \tTotal Chunks: 6, Chunks in use: 5. 549.5KiB allocated for chunks. 482.0KiB in use in bin. 448.4KiB client-requested in use in bin.\n",
      "2018-10-20 00:05:48.286846: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (131072): \tTotal Chunks: 4, Chunks in use: 4. 807.0KiB allocated for chunks. 807.0KiB in use in bin. 714.1KiB client-requested in use in bin.\n",
      "2018-10-20 00:05:48.286856: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (262144): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2018-10-20 00:05:48.286866: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (524288): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2018-10-20 00:05:48.286876: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (1048576): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2018-10-20 00:05:48.286886: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (2097152): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2018-10-20 00:05:48.286895: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (4194304): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2018-10-20 00:05:48.286905: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (8388608): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2018-10-20 00:05:48.286915: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (16777216): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2018-10-20 00:05:48.286926: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (33554432): \tTotal Chunks: 1, Chunks in use: 0. 62.48MiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2018-10-20 00:05:48.286937: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (67108864): \tTotal Chunks: 2, Chunks in use: 1. 151.15MiB allocated for chunks. 64.00MiB in use in bin. 64.00MiB client-requested in use in bin.\n",
      "2018-10-20 00:05:48.286948: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (134217728): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2018-10-20 00:05:48.286958: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (268435456): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2018-10-20 00:05:48.286969: I tensorflow/core/common_runtime/bfc_allocator.cc:646] Bin for 98.42MiB was 64.00MiB, Chunk State: \n",
      "2018-10-20 00:05:48.286986: I tensorflow/core/common_runtime/bfc_allocator.cc:652]   Size: 87.15MiB | Requested Size: 2.36MiB | in_use: 0, prev:   Size: 64.00MiB | Requested Size: 64.00MiB | in_use: 1\n",
      "2018-10-20 00:05:48.287001: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x12051c0000 of size 1280\n",
      "2018-10-20 00:05:48.287009: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x12051c0500 of size 1024\n",
      "2018-10-20 00:05:48.287017: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x12051c0900 of size 1024\n",
      "2018-10-20 00:05:48.287025: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x12051c0d00 of size 1024\n",
      "2018-10-20 00:05:48.287032: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x12051c1100 of size 1024\n",
      "2018-10-20 00:05:48.287041: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x12051c1500 of size 768\n",
      "2018-10-20 00:05:48.287048: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x12051c1800 of size 768\n",
      "2018-10-20 00:05:48.287056: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x12051c1b00 of size 768\n",
      "2018-10-20 00:05:48.287064: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x12051c1e00 of size 768\n",
      "2018-10-20 00:05:48.287072: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x12051c2100 of size 768\n",
      "2018-10-20 00:05:48.287079: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x12051c2400 of size 768\n",
      "2018-10-20 00:05:48.287087: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x12051c2700 of size 768\n",
      "2018-10-20 00:05:48.287095: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x12051c2a00 of size 768\n",
      "2018-10-20 00:05:48.287103: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x12051c2d00 of size 256\n",
      "2018-10-20 00:05:48.287111: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x12051c2e00 of size 256\n",
      "2018-10-20 00:05:48.287118: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x12051c2f00 of size 256\n",
      "2018-10-20 00:05:48.287126: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x12051c3000 of size 256\n",
      "2018-10-20 00:05:48.287135: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x12051c3100 of size 206592\n",
      "2018-10-20 00:05:48.287143: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x12051f5800 of size 256\n",
      "2018-10-20 00:05:48.287151: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x12051f5900 of size 112128\n",
      "2018-10-20 00:05:48.287159: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x1205210f00 of size 256\n",
      "2018-10-20 00:05:48.287167: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x1205211000 of size 78592\n",
      "2018-10-20 00:05:48.287174: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x1205224300 of size 256\n",
      "2018-10-20 00:05:48.287183: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x1205224400 of size 4096\n",
      "2018-10-20 00:05:48.287190: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x1205225400 of size 1024\n",
      "2018-10-20 00:05:48.287198: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x1205225800 of size 1024\n",
      "2018-10-20 00:05:48.287206: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x1205225c00 of size 1024\n",
      "2018-10-20 00:05:48.287214: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x1205226000 of size 1024\n",
      "2018-10-20 00:05:48.287221: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x1205226400 of size 768\n",
      "2018-10-20 00:05:48.287228: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x1205226700 of size 768\n",
      "2018-10-20 00:05:48.287236: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x1205226a00 of size 768\n",
      "2018-10-20 00:05:48.287244: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x1205226d00 of size 768\n",
      "2018-10-20 00:05:48.287251: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x1205227000 of size 768\n",
      "2018-10-20 00:05:48.287259: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x1205227300 of size 768\n",
      "2018-10-20 00:05:48.287266: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x1205227600 of size 768\n",
      "2018-10-20 00:05:48.287274: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x1205227900 of size 768\n",
      "2018-10-20 00:05:48.287281: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x1205227c00 of size 256\n",
      "2018-10-20 00:05:48.287289: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x1205227d00 of size 256\n",
      "2018-10-20 00:05:48.287297: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x1205227e00 of size 256\n",
      "2018-10-20 00:05:48.287304: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x1205227f00 of size 256\n",
      "2018-10-20 00:05:48.287311: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x1205228000 of size 206592\n",
      "2018-10-20 00:05:48.287319: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x120525a700 of size 206592\n",
      "2018-10-20 00:05:48.287327: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x120528ce00 of size 112128\n",
      "2018-10-20 00:05:48.287334: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x12052a8400 of size 112128\n",
      "2018-10-20 00:05:48.287342: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x12052c3a00 of size 4096\n",
      "2018-10-20 00:05:48.287350: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x12052c4a00 of size 256\n",
      "2018-10-20 00:05:48.287358: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x12052c4b00 of size 256\n",
      "2018-10-20 00:05:48.287365: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x12052c4c00 of size 256\n",
      "2018-10-20 00:05:48.287372: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x12052c4d00 of size 256\n",
      "2018-10-20 00:05:48.287380: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x12052c4e00 of size 256\n",
      "2018-10-20 00:05:48.287388: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x12052c4f00 of size 256\n",
      "2018-10-20 00:05:48.287395: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x12052c5000 of size 256\n",
      "2018-10-20 00:05:48.287403: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x12052c5100 of size 256\n",
      "2018-10-20 00:05:48.287410: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x12052c5200 of size 256\n",
      "2018-10-20 00:05:48.287418: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x12052c5300 of size 256\n",
      "2018-10-20 00:05:48.287425: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x12052c5400 of size 256\n",
      "2018-10-20 00:05:48.287433: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x12052c5500 of size 256\n",
      "2018-10-20 00:05:48.287440: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x12052c5600 of size 256\n",
      "2018-10-20 00:05:48.287448: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x12052c5700 of size 256\n",
      "2018-10-20 00:05:48.287456: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x12052c5800 of size 256\n",
      "2018-10-20 00:05:48.287464: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x12052c5900 of size 256\n",
      "2018-10-20 00:05:48.287471: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x12052c5a00 of size 256\n",
      "2018-10-20 00:05:48.287479: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x12052c5b00 of size 256\n",
      "2018-10-20 00:05:48.287486: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x12052c5c00 of size 256\n",
      "2018-10-20 00:05:48.287494: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x12052c5d00 of size 256\n",
      "2018-10-20 00:05:48.287502: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x12052c5e00 of size 256\n",
      "2018-10-20 00:05:48.287509: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Free  at 0x12052c5f00 of size 69120\n",
      "2018-10-20 00:05:48.287517: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x12052d6d00 of size 78592\n",
      "2018-10-20 00:05:48.287525: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x12052ea000 of size 256\n",
      "2018-10-20 00:05:48.287532: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x12052ea100 of size 256\n",
      "2018-10-20 00:05:48.287540: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x12052ea200 of size 256\n",
      "2018-10-20 00:05:48.287547: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x12052ea300 of size 256\n",
      "2018-10-20 00:05:48.287555: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x12052ea400 of size 256\n",
      "2018-10-20 00:05:48.287563: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x12052ea500 of size 256\n",
      "2018-10-20 00:05:48.287571: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x12052ea600 of size 256\n",
      "2018-10-20 00:05:48.287578: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x12052ea700 of size 256\n",
      "2018-10-20 00:05:48.287586: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x12052ea800 of size 256\n",
      "2018-10-20 00:05:48.287593: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x12052ea900 of size 256\n",
      "2018-10-20 00:05:48.287601: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x12052eaa00 of size 256\n",
      "2018-10-20 00:05:48.287609: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x12052eab00 of size 256\n",
      "2018-10-20 00:05:48.287616: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x12052eac00 of size 256\n",
      "2018-10-20 00:05:48.287624: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x12052ead00 of size 256\n",
      "2018-10-20 00:05:48.287632: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x12052eae00 of size 256\n",
      "2018-10-20 00:05:48.287639: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x12052eaf00 of size 256\n",
      "2018-10-20 00:05:48.287647: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x12052eb000 of size 4096\n",
      "2018-10-20 00:05:48.287655: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x12052ec000 of size 206592\n",
      "2018-10-20 00:05:48.287663: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Free  at 0x120531e700 of size 65511168\n",
      "2018-10-20 00:05:48.287671: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x1209198600 of size 67108864\n",
      "2018-10-20 00:05:48.287679: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Free  at 0x120d198600 of size 91388416\n",
      "2018-10-20 00:05:48.287686: I tensorflow/core/common_runtime/bfc_allocator.cc:671]      Summary of in-use Chunks by size: \n",
      "2018-10-20 00:05:48.287697: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 48 Chunks of size 256 totalling 12.0KiB\n",
      "2018-10-20 00:05:48.287707: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 16 Chunks of size 768 totalling 12.0KiB\n",
      "2018-10-20 00:05:48.287716: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 8 Chunks of size 1024 totalling 8.0KiB\n",
      "2018-10-20 00:05:48.287725: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 1 Chunks of size 1280 totalling 1.2KiB\n",
      "2018-10-20 00:05:48.287734: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 3 Chunks of size 4096 totalling 12.0KiB\n",
      "2018-10-20 00:05:48.287744: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 2 Chunks of size 78592 totalling 153.5KiB\n",
      "2018-10-20 00:05:48.287753: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 3 Chunks of size 112128 totalling 328.5KiB\n",
      "2018-10-20 00:05:48.287763: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 4 Chunks of size 206592 totalling 807.0KiB\n",
      "2018-10-20 00:05:48.287772: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 1 Chunks of size 67108864 totalling 64.00MiB\n",
      "2018-10-20 00:05:48.287782: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Sum Total of in-use chunks: 65.30MiB\n",
      "2018-10-20 00:05:48.287794: I tensorflow/core/common_runtime/bfc_allocator.cc:680] Stats: \n",
      "Limit:                   225443840\n",
      "InUse:                    68475136\n",
      "MaxInUse:                117901312\n",
      "NumAllocs:                     536\n",
      "MaxAllocSize:             67108864\n",
      "\n",
      "2018-10-20 00:05:48.287811: W tensorflow/core/common_runtime/bfc_allocator.cc:279] *____________________________*******************************________________________________________\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1322, in _do_call\n",
      "    return fn(*args)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1307, in _run_fn\n",
      "    options, feed_dict, fetch_list, target_list, run_metadata)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1409, in _call_tf_sessionrun\n",
      "    run_metadata)\n",
      "tensorflow.python.framework.errors_impl.InternalError: Dst tensor is not initialized.\n",
      "\t [[Node: _arg_features_0_5/_157 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device_incarnation=1, tensor_name=\"edge_2342__arg_features_0_5\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"nn_real_hvd-ntb-v3.py\", line 1405, in <module>\n",
      "    valid_conf_mtx, valid_time, metrics = batching_dataset(sess, valid_writer, 'valid', DATA, FLAGS)\n",
      "  File \"nn_real_hvd-ntb-v3.py\", line 1314, in batching_dataset\n",
      "    reset_and_update(sess, local_init, feed)\n",
      "  File \"nn_real_hvd-ntb-v3.py\", line 1214, in reset_and_update\n",
      "    sess.run(update_names_list, feed_dict=feed_dict)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 900, in run\n",
      "    run_metadata_ptr)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1135, in _run\n",
      "    feed_dict_tensor, options, run_metadata)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1316, in _do_run\n",
      "    run_metadata)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1335, in _do_call\n",
      "    raise type(e)(node_def, op, message)\n",
      "tensorflow.python.framework.errors_impl.InternalError: Dst tensor is not initialized.\n",
      "\t [[Node: _arg_features_0_5/_157 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device_incarnation=1, tensor_name=\"edge_2342__arg_features_0_5\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\n",
      "Closing remaining open files:/home/ubuntu/MLMortgage/data/processed/chuncks_random_c1millx2_valid/c1mill99-01_non_index_valid_0.h5...done/home/ubuntu/MLMortgage/data/processed/chuncks_random_c1millx2_train/c1mill99-01_non_index_train_0.h5...done\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1322, in _do_call\n",
      "    return fn(*args)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1307, in _run_fn\n",
      "    options, feed_dict, fetch_list, target_list, run_metadata)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1409, in _call_tf_sessionrun\n",
      "    run_metadata)\n",
      "tensorflow.python.framework.errors_impl.AbortedError: Horovod has been shut down. This has been caused by an exception on one of the rank or an attempt to allreduce, allgather or broadcast a tensor after one of the ranks has finished execution.\n",
      "\t [[Node: train/DistributedMomentumOptimizer_Allreduce/HorovodAllreduce_train_gradients_AddN_11_0 = HorovodAllreduce[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](train/gradients/AddN_11)]]\n",
      "\t [[Node: train/update/_472 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_8695_train/update\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"nn_real_hvd-ntb-v3.py\", line 1388, in <module>\n",
      "    accuracy, conf_mtx, better_acc, loss, lr, _ = sess.run([accuracy_op, conf_mtx_op, better_acc_op, total_loss_op] + ops_to_run, feed_dict=batch_dict)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 900, in run\n",
      "    run_metadata_ptr)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1135, in _run\n",
      "    feed_dict_tensor, options, run_metadata)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1316, in _do_run\n",
      "    run_metadata)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1335, in _do_call\n",
      "    raise type(e)(node_def, op, message)\n",
      "tensorflow.python.framework.errors_impl.AbortedError: Horovod has been shut down. This has been caused by an exception on one of the rank or an attempt to allreduce, allgather or broadcast a tensor after one of the ranks has finished execution.\n",
      "\t [[Node: train/DistributedMomentumOptimizer_Allreduce/HorovodAllreduce_train_gradients_AddN_11_0 = HorovodAllreduce[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](train/gradients/AddN_11)]]\n",
      "\t [[Node: train/update/_472 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_8695_train/update\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n",
      "\n",
      "Caused by op 'train/DistributedMomentumOptimizer_Allreduce/HorovodAllreduce_train_gradients_AddN_11_0', defined at:\n",
      "  File \"nn_real_hvd-ntb-v3.py\", line 1050, in <module>\n",
      "    train_ops, learning_rate_op, conf_mtx_op, accuracy_op, better_acc_op, auc_list_op, auc_mean_op, m_list_op, m_list_mean_op, total_loss_op, lloss_op, auc_pr_op, auc_pr_mean_op, auc_data_op = trainer.training_step(architecture, FLAGS)\n",
      "  File \"nn_real_hvd-ntb-v3.py\", line 762, in training_step\n",
      "    train_op = optimizer.minimize(loss, global_step=self.global_step, name=scope)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py\", line 414, in minimize\n",
      "    grad_loss=grad_loss)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/horovod/tensorflow/__init__.py\", line 185, in compute_gradients\n",
      "    device_sparse=self._device_sparse)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/horovod/tensorflow/__init__.py\", line 82, in allreduce\n",
      "    summed_tensor = _allreduce(tensor)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/horovod/tensorflow/mpi_ops.py\", line 77, in _allreduce\n",
      "    return MPI_LIB.horovod_allreduce(tensor, name=name)\n",
      "  File \"<string>\", line 39, in horovod_allreduce\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 328, in apply_op\n",
      "    op_type_name, name, **keywords)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n",
      "    op_def=op_def)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3392, in create_op\n",
      "    op_def=op_def)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1718, in __init__\n",
      "    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n",
      "\n",
      "AbortedError (see above for traceback): Horovod has been shut down. This has been caused by an exception on one of the rank or an attempt to allreduce, allgather or broadcast a tensor after one of the ranks has finished execution.\n",
      "\t [[Node: train/DistributedMomentumOptimizer_Allreduce/HorovodAllreduce_train_gradients_AddN_11_0 = HorovodAllreduce[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](train/gradients/AddN_11)]]\n",
      "\t [[Node: train/update/_472 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_8695_train/update\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n",
      "\n",
      "Closing remaining open files:/home/ubuntu/MLMortgage/data/processed/chuncks_random_c1millx2_valid/c1mill99-01_valid_non_index_0.h5...done/home/ubuntu/MLMortgage/data/processed/chuncks_random_c1millx2_train/c1mill99-01_train_non_index_0.h5...done\n",
      "-------------------------------------------------------\n",
      "Primary job  terminated normally, but 1 process returned\n",
      "a non-zero exit code.. Per user-direction, the job has been aborted.\n",
      "-------------------------------------------------------\n",
      "--------------------------------------------------------------------------\n",
      "mpirun detected that one or more processes exited with non-zero status, thus causing\n",
      "the job to be terminated. The first process to do so was:\n",
      "\n",
      "  Process name: [[10190,1],0]\n",
      "  Exit code:    1\n",
      "--------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "!mpirun -np 2 -H ec2-107-23-254-27.compute-1.amazonaws.com,ec2-52-90-176-235.compute-1.amazonaws.com --prefix /usr/local/mpi --bind-to none --map-by slot -x NCCL_DEBUG=INFO -x NCCL_MIN_NRINGS=2 -x LD_LIBRARY_PATH -x PATH  -mca pml ob1 -mca btl ^openib python nn_real_hvd-ntb-v3.py "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow_p36]",
   "language": "python",
   "name": "conda-env-tensorflow_p36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
