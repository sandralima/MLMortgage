{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beginning Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import argparse\n",
    "import psutil\n",
    "import numpy as np\n",
    "import logging\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import glob\n",
    "from tensorflow.python.framework import ops\n",
    "import math\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "import ftplib\n",
    "\n",
    "\n",
    "nb_dir = os.path.join(Path(os.getcwd()).parents[0], 'src', 'data')\n",
    "if nb_dir not in sys.path:\n",
    "    sys.path.insert(0, nb_dir)\n",
    "print(sys.path)\n",
    "import features_selection as fs\n",
    "import make_dataset as md\n",
    "import build_data as bd\n",
    "import get_raw_data as grd\n",
    "import data_classes\n",
    "\n",
    "models_dir = os.path.join(Path(os.getcwd()).parents[0], 'src', 'models')\n",
    "if models_dir not in sys.path:\n",
    "    sys.path.insert(0, models_dir)\n",
    "import nn_real as nn\n",
    "\n",
    "try:\n",
    "    import horovod.tensorflow as hvd\n",
    "except:\n",
    "    print(\"Failed to import horovod module. \"\n",
    "          \"%s is intended for use with Uber's Horovod distributed training \"\n",
    "          \"framework. To create a Docker image with Horovod support see \"\n",
    "          \"docker-examples/Dockerfile.horovod.\" % __file__)\n",
    "    raise\n",
    "\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_DIR = os.path.join(Path(os.getcwd()).parents[0], 'data', 'raw') \n",
    "PRO_DIR = os.path.join(Path(os.getcwd()).parents[0], 'data', 'processed')\n",
    "RANDOM_SEED = 123  # Set the seed to get reproducable results.\n",
    "DT_FLOAT = tf.float32\n",
    "NP_FLOAT = np.dtype('float32')\n",
    "\n",
    "print(RAW_DIR, PRO_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hvd.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FLAGS_setting(FLAGS, net_number):\n",
    "    # To determine an optimal set of hyperparameters, see Section 11.4.2 of the\n",
    "    # deep learning book. Has (1) grid, (2) random, and (3) Bayesian\n",
    "    # model-based search methods.Swersky et al. have a paper mentioned in that\n",
    "    # section (published in 2014).\n",
    "\n",
    "    # Hyperparameters\n",
    "    # FLAGS.epoch_num = 2  # 14  # 17  # 35  # 15\n",
    "    #print(\"FLAGS.epoch_num\", FLAGS.epoch_num)\n",
    "    # FLAGS.batch_size = 141600 # 4425 # 4000  \n",
    "    FLAGS.dropout_keep = 0.9  # 0.9  # 0.95  # .75  # .6\n",
    "    # ### parameters for training optimizer.\n",
    "    #FLAGS.learning_rate = .1  # .075  # .15  # .25\n",
    "    FLAGS.momentum = .5  # used by the momentum SGD.\n",
    "\n",
    "    # ### parameters for inverse_time_decay\n",
    "    FLAGS.decay_rate = 1\n",
    "    FLAGS.decay_step = 800 * 4400 #steps_per_epoch 1 * 80000 #according to paper: 800 epochs\n",
    "    FLAGS.rate_min = .0015\n",
    "    # ### parameters for exponential_decay\n",
    "    # FLAGS.decay_base = .96  # .96\n",
    "    # FLAGS.decay_step = 15000  # 12320  # 4 * 8700\n",
    "\n",
    "    # ### parameters for regularization\n",
    "    FLAGS.reg_rate = .01 * 1e-3  # * 1e-3\n",
    "\n",
    "    FLAGS.batch_norm = True  # False  #\n",
    "    FLAGS.dropout = True\n",
    "    # A flag to show the results on the held-out test set. Keep this at False.\n",
    "    FLAGS.test_flag = True\n",
    "    FLAGS.xla = True  # False\n",
    "    FLAGS.stratified_flag = False\n",
    "    #FLAGS.batch_type = 'batch'    \n",
    "    FLAGS.weighted_sampling = False  # True  #\n",
    "    # FLAGS.logdir =  os.path.join(Path.home(), 'real_summaries')  # \n",
    "    #FLAGS.n_hidden = 3\n",
    "    #FLAGS.s_hidden = [200, 140, 140]\n",
    "    # FLAGS.allow_summaries = False\n",
    "    FLAGS.epoch_flag = 0    \n",
    "    \n",
    "    #FLAGS.max_epoch_size = 141600*70 #137 # -1\n",
    "    \n",
    "    FLAGS.valid_batch_size = 150000\n",
    "    FLAGS.test_batch_size = 1200000\n",
    "    \n",
    "    FLAGS.train_dir = 'chuncks_random_c1millx2_train'\n",
    "    FLAGS.valid_dir = 'chuncks_random_c1millx2_valid'\n",
    "    FLAGS.test_dir = 'chuncks_random_c1millx2_test'\n",
    "    FLAGS.train_period=[121,279] #[121, 143] \n",
    "    FLAGS.valid_period=[280,285] #[144, 147] \n",
    "    FLAGS.test_period=[286,304] #[148, 155]\n",
    "    FLAGS.epoch_num=15 \n",
    "    FLAGS.max_epoch_size=-1 \n",
    "    FLAGS.batch_size=4425*2 # two files per worker except at master!\n",
    "    FLAGS.lr_decay_policy       = 'time'\n",
    "    FLAGS.lr_decay_epochs       = 30\n",
    "    FLAGS.lr_decay_rate         = 0.1\n",
    "    FLAGS.lr_poly_power         = 2.\n",
    "    FLAGS.eval = False # True=Evaluation else Training\n",
    "    FLAGS.save_interval = 450\n",
    "    FLAGS.nstep_burnin = 20 # step from to count consuming time for a batch\n",
    "    FLAGS.summary_interval = 1800 # Time in seconds between saves of summary statistics\n",
    "    FLAGS.display_every = 100 # How often (in iterations) to print out running information\n",
    "    FLAGS.total_examples = 38500000 #-1 to training all dataset, otherwise the training will have a fixed length\n",
    "    \n",
    "    #Retrieveng from ftp:\n",
    "    FLAGS.ftp_dir = 'processed/c1mill'\n",
    "    \n",
    "    \n",
    "    if FLAGS.n_hidden < 0 : raise ValueError('The size of hidden layer must be at least 0')\n",
    "    if (FLAGS.n_hidden > 0) and (FLAGS.n_hidden != len(FLAGS.s_hidden)) : raise ValueError('Sizes in hidden layers should match!')\n",
    "    \n",
    "    if (net_number==0):\n",
    "        FLAGS.name ='default_settings'        \n",
    "    elif (net_number==1):\n",
    "        FLAGS.name ='Xworkers_1mill'\n",
    "        FLAGS.batch_layer_type = 'batch'        \n",
    "        \n",
    "    return FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "FLAGS, UNPARSED = nn.update_parser(argparse.ArgumentParser())\n",
    "print(\"UNPARSED\", UNPARSED)\n",
    "FLAGS.logdir = Path(str('/home/ubuntu/summ_15ep_2wrk/'))\n",
    "if not os.path.exists(os.path.join(FLAGS.logdir)): #os.path.exists\n",
    "    os.makedirs(os.path.join(FLAGS.logdir))\n",
    "else:\n",
    "    print('existent directory')\n",
    "FLAGS = FLAGS_setting(FLAGS, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"FLAGS\", FLAGS) #you can change the FLAGS by adding the setting before this line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Builder, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPUNetworkBuilder(object):\n",
    "    \"\"\"This class provides convenient methods for constructing feed-forward\n",
    "    networks with internal data layout of 'NCHW'.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 # is_training,\n",
    "                 dtype=DT_FLOAT,\n",
    "                 activation='RELU',\n",
    "                 use_batch_norm=True,\n",
    "                 batch_norm_config = {'decay':   0.9,\n",
    "                                      'epsilon': 1e-4,\n",
    "                                      'scale':   True,\n",
    "                                      'zero_debias_moving_mean': False}):\n",
    "        self.dtype             = dtype\n",
    "        self.activation_func   = activation\n",
    "        # self.is_training       = is_training\n",
    "        self.use_batch_norm    = use_batch_norm\n",
    "        self.batch_norm_config = batch_norm_config\n",
    "        #self._layer_counts     = defaultdict(lambda: 0)        \n",
    "        \n",
    "    def variable_summaries(self, name, var, allow_summaries):\n",
    "        \"\"\"Create summaries for the given Tensor (for TensorBoard visualization (TB graphs)).\n",
    "            Calculate the mean, min, max, histogram and standardeviation for 'var' variable and save the information\n",
    "            in tf.summary.\n",
    "\n",
    "        Args: \n",
    "             name (String): the of the scope for summaring. For min, max and standardeviation 'calculate_std' is used as sub-scope.\n",
    "             var (Tensor): This is the tensor variable for building summaries.\n",
    "        Returns: \n",
    "            None\n",
    "        Raises:        \n",
    "        \"\"\"\n",
    "        if allow_summaries:\n",
    "            with tf.name_scope(name):\n",
    "                mean = tf.reduce_mean(var)\n",
    "                tf.summary.scalar('mean', mean)\n",
    "                with tf.name_scope('calculate_std'):\n",
    "                    stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "                tf.summary.scalar('stddev', stddev)\n",
    "                tf.summary.scalar('min', tf.reduce_min(var))\n",
    "                tf.summary.scalar('max', tf.reduce_max(var))\n",
    "                tf.summary.histogram('histogram', var)\n",
    "                \n",
    "    def _variable_on_cpu(self, name,\n",
    "                     shape,\n",
    "                     initializer=None,\n",
    "                     regularizer=None,\n",
    "                     dtype=DT_FLOAT):\n",
    "        \"\"\"Create a Variable or get an existing one stored on CPU memory.    \n",
    "\n",
    "        Args:\n",
    "            name (String): name of the variable.\n",
    "            shape (list of ints): Shape of the variable.\n",
    "            initializer: Default None. Initializer for Variable.\n",
    "            regularizer (A (Tensor -> Tensor or None) function): Default None. Regularizer for Variable.\n",
    "            dtype (TYPE): Type of the new variable.\n",
    "        Returns:\n",
    "            Variable Tensor\n",
    "        \"\"\"\n",
    "        with tf.device('/gpu:1'): # this operation is assigned to this device, but this make a copy of data when is transferred on and off the device, which is expensive.\n",
    "            var = tf.get_variable(\n",
    "                name,\n",
    "                shape,\n",
    "                initializer=initializer,\n",
    "                regularizer=regularizer,\n",
    "                dtype=dtype)\n",
    "        return var\n",
    "\n",
    "    def _create_variable(self, name,\n",
    "                         shape, allow_summaries, \n",
    "                         initializer=None,\n",
    "                         regularizer=None,\n",
    "                         dtype=DT_FLOAT):\n",
    "        \"\"\"Call _variable_on_cpu methods and variable_summaries for the 'name' tensor variable. \n",
    "\n",
    "        Args:\n",
    "            name (String): name of the variable.\n",
    "            shape (list of ints): Shape of the variable.\n",
    "            initializer: Default None. Initializer for Variable.\n",
    "            regularizer (A (Tensor -> Tensor or None) function): Default None. Regularizer for Variable.\n",
    "            dtype (TYPE): Type of the new variable.\n",
    "        Returns:\n",
    "            Variable Tensor\n",
    "        \"\"\"\n",
    "        var = self._variable_on_cpu(name, shape, initializer, regularizer, dtype)\n",
    "        self.variable_summaries(name + '/summaries', var, allow_summaries)\n",
    "        return var\n",
    "\n",
    "    def create_weights(self, name, shape, reg_rate, allow_summaries):\n",
    "        \"\"\"Create a Variable initialized with weights which are truncated normal distribution and regularized by\n",
    "        l1_regularizer (L1 regularization encourages sparsity, Regularization can help prevent overfitting).    \n",
    "\n",
    "        Args:\n",
    "            name (String): name of the variable.\n",
    "            shape (list of ints): Shape of the variable.\n",
    "        Returns:\n",
    "            Variable Tensor\n",
    "        \"\"\"    \n",
    "        dtype = DT_FLOAT\n",
    "        # kernel_initializer = tf.uniform_unit_scaling_initializer(\n",
    "        #     factor=1.43, dtype=DT_FLOAT)\n",
    "        # kernel_initializer = tf.contrib.layers.xavier_initializer(\n",
    "        #     uniform=True, dtype=DT_FLOAT)\n",
    "        kernel_initializer = tf.truncated_normal_initializer(\n",
    "            stddev=(1.0 / np.sqrt(shape[0])), dtype=dtype)\n",
    "\n",
    "        regularizer = tf.contrib.layers.l2_regularizer(\n",
    "            np.float32(reg_rate), 'penalty')\n",
    "        return self._create_variable(name, shape, allow_summaries, kernel_initializer, regularizer,\n",
    "                                dtype)\n",
    "\n",
    "    def bias_variable(self, name, shape, layer_name, weighted_sampling): # FLAGS.weighted_sampling\n",
    "        \"\"\"Create a bias variable with appropriate initialization. In case of FLAGS.weighted_sampling==False\n",
    "        and layer_name contains 'soft' the bias variable will contain a np.array of Negative values. Otherwise\n",
    "        the bias variable will be initialized in zero.\n",
    "\n",
    "        Args:\n",
    "            name (String): name of the variable.\n",
    "            shape (list of ints): Shape of the variable.\n",
    "            layer_name (String): name of the layer.\n",
    "        Returns:\n",
    "            Variable Tensor.\n",
    "        \"\"\"\n",
    "        def initial_bias(layer_name):\n",
    "            \"\"\"Get the initial value for the bias of the layer with layer_name.\"\"\"\n",
    "            if (not weighted_sampling) and 'soft' in layer_name:\n",
    "                return np.array(\n",
    "                    [-4.66, -3.81, -4.81, -3.90, -0.08, -3.90, -7.51],\n",
    "                    dtype=NP_FLOAT) + NP_FLOAT(4.1)\n",
    "            return 0.0\n",
    "\n",
    "        initial_value = initial_bias(layer_name)\n",
    "        with tf.name_scope(name) as scope:\n",
    "            initial = tf.constant(initial_value, shape=shape)\n",
    "            bias = tf.Variable(initial, name=scope)\n",
    "            self.variable_summaries('summaries', bias)\n",
    "        return bias        \n",
    "    \n",
    "    def dropout_layer(self, name, tensor_before, FLAGS):\n",
    "        \"\"\"Compute dropout to tensor_before with name scoping and a placeholder for keep_prob. \n",
    "        With probability keep_prob, outputs the input element scaled up by 1 / keep_prob, otherwise outputs 0.\n",
    "\n",
    "        Args:\n",
    "            name (String): name of the scope.\n",
    "            tensor_before (Tensor): Variable Tensor.        \n",
    "        Returns:\n",
    "            Variable Tensor of the same shape of tensor_before.\n",
    "        \"\"\"   \n",
    "        if not FLAGS.dropout:\n",
    "            print('There is not dropout for' + name)\n",
    "            return tensor_before\n",
    "        with tf.name_scope(name) as scope:\n",
    "            keep_prob = tf.placeholder(DT_FLOAT, None, name='keep_proba')\n",
    "            tf.summary.scalar('keep_probability', keep_prob)\n",
    "            dropped = tf.nn.dropout(tensor_before, keep_prob=keep_prob, name=scope)\n",
    "            self.variable_summaries('input_dropped_out', dropped, FLAGS.allow_summaries)\n",
    "        return dropped\n",
    "\n",
    "    def batch_normalization(self, name, input_tensor, train_flag, FLAGS):\n",
    "        \"\"\"Perform batch normalization over the input tensor.\n",
    "        Batch normalization helps avoid overfitting and we're able to use more\n",
    "        aggressive (larger) learning rates, resulting in faster convergence.\n",
    "        training parameter: Either a Python boolean, or a TensorFlow boolean scalar tensor (e.g. a placeholder). \n",
    "        Whether to return the output in training mode (normalized with statistics of the current batch) or in \n",
    "        inference mode (normalized with moving statistics). NOTE: make sure to set this parameter correctly, \n",
    "        or else your training/inference will not work properly.\n",
    "\n",
    "        Args:\n",
    "            name (String): name of the scope and the name of the layer.\n",
    "            input_tensor (Tensor): Variable Tensor.        \n",
    "        Returns:\n",
    "            Variable Tensor # the same shape of input_tensor??.\n",
    "        \"\"\"\n",
    "        # if not FLAGS.batch_norm:\n",
    "        #     return input_tensor\n",
    "        # train_flag = tf.get_default_graph().get_tensor_by_name('train_flag:0')\n",
    "        with tf.name_scope(name):\n",
    "            normalized = tf.layers.batch_normalization(\n",
    "                input_tensor,\n",
    "                center=True,\n",
    "                scale=True,\n",
    "                training=train_flag,\n",
    "                name=name)  # renorm=True, renorm_momentum=0.99)\n",
    "            self.variable_summaries('normalized_batch', normalized, FLAGS.allow_summaries)\n",
    "        return normalized\n",
    "\n",
    "    def layer_normalization(self, name, input_tensor, FLAGS):\n",
    "        \"\"\"Perform layer normalization.\n",
    "\n",
    "        Layer normalization helps avoid overfitting and we're able to use more\n",
    "        aggressive (larger) learning rates, resulting in faster convergence.\n",
    "        Can be used as a normalizer function for conv2d and fully_connected.\n",
    "\n",
    "        Given a tensor inputs of rank R, moments are calculated and normalization \n",
    "        is performed over axes begin_norm_axis ... R - 1. \n",
    "        Scaling and centering, if requested, is performed over axes begin_params_axis .. R - 1.\n",
    "        \"\"\"\n",
    "        # if not FLAGS.batch_norm:\n",
    "        #     return input_tensor\n",
    "        with tf.name_scope(name):\n",
    "            normalized = tf.contrib.layers.layer_norm(\n",
    "                input_tensor, center=True, scale=True, scope=name)\n",
    "            self.variable_summaries('normalized_layer', normalized, FLAGS.allow_summaries)\n",
    "        return normalized\n",
    "\n",
    "\n",
    "    def normalize(self, name, input_tensor, train_flag, FLAGS):\n",
    "        \"\"\"Perform either type (batch/layer) of normalization.\"\"\"\n",
    "        if not FLAGS.batch_norm:\n",
    "            return input_tensor\n",
    "        if FLAGS.batch_type.lower() == 'batch':\n",
    "            return self.batch_normalization(name, input_tensor, train_flag, FLAGS)\n",
    "        if FLAGS.batch_type.lower() == 'layer':\n",
    "            return self.layer_normalization(name, input_tensor, FLAGS)\n",
    "        raise ValueError('Invalid value for batch_type: ' + FLAGS.batch_type)\n",
    "\n",
    "    def nn_layer(self, input_tensor, output_dim, layer_name, FLAGS, act, train_flag):\n",
    "        \"\"\"Create a simple neural net layer.\n",
    "\n",
    "        It performs the affine transformation and uses the activation function to\n",
    "        nonlinearize. It further sets up name scoping so that the resultant graph\n",
    "        is easy to read, and adds a number of summary ops.\n",
    "        \"\"\"\n",
    "        input_dim = input_tensor.shape[1].value    \n",
    "        with tf.variable_scope(layer_name): # A context manager for defining ops that creates variables (layers).\n",
    "            weights = self.create_weights('weights', [input_dim, output_dim], FLAGS.reg_rate, FLAGS.allow_summaries)\n",
    "            # This is outdated and no longer applies: Do not change the order of\n",
    "            # batch normalization and drop out. batch # normalization has to stay\n",
    "            # __before__ the drop out layer.\n",
    "            self.variable_summaries('input', input_tensor, FLAGS.allow_summaries)\n",
    "            input_tensor = self.dropout_layer('dropout', input_tensor, FLAGS)\n",
    "            with tf.name_scope('mix'):\n",
    "                mixed = tf.matmul(input_tensor, weights)\n",
    "                tf.summary.histogram('maybe_guassian', mixed)\n",
    "            # Batch or layer normalization has to stay __after__ the affine\n",
    "            # transformation (the bias term doens't really matter because of the\n",
    "            # beta term in the normalization equation).\n",
    "            # See pp. 5 of the batch normalization paper:\n",
    "            # ```We add the BN transform immediately before the nonlinearity, by\n",
    "            # normalizing x = W u + b```\n",
    "            # biases = bias_variable('biases', [output_dim], layer_name)\n",
    "            preactivate = self.normalize('layer_normalization', mixed, train_flag, FLAGS)  # + biases\n",
    "            # tf.summary.histogram('pre_activations', preactivate)\n",
    "            # preactivate = dropout_layer('dropout', preactivate)\n",
    "            with tf.name_scope('activation') as scope:\n",
    "                activations = self.activate(preactivate, funcname=act)\n",
    "                tf.summary.histogram('activations', activations)\n",
    "        return activations        \n",
    "    \n",
    "    def activate(self, input_layer, funcname=None):\n",
    "        \"\"\"Applies an activation function\"\"\"\n",
    "        if isinstance(funcname, tuple):\n",
    "            funcname = funcname[0]\n",
    "            params = funcname[1:]\n",
    "        if funcname is None:\n",
    "            funcname = self.activation_func\n",
    "        if funcname == 'LINEAR':\n",
    "            return input_layer\n",
    "        activation_map = {\n",
    "            'IDENT':   tf.identity,\n",
    "            'RELU':    tf.nn.relu,\n",
    "            'RELU6':   tf.nn.relu6,\n",
    "            'ELU':     tf.nn.elu,\n",
    "            'SIGMOID': tf.nn.sigmoid,\n",
    "            'TANH':    tf.nn.tanh,\n",
    "            'LRELU':   lambda x, name: tf.maximum(params[0]*x, x, name=name)\n",
    "        }\n",
    "        return activation_map[funcname](input_layer, name=funcname.lower())\n",
    "    \n",
    "    def add_hidden_layers(self, features, architecture, FLAGS, train_flag, act=None):\n",
    "        \"\"\"Add hidden layers to the model using the architecture parameters.\"\"\"\n",
    "        hidden_out = features\n",
    "        jit_scope = tf.contrib.compiler.jit.experimental_jit_scope #JIT compiler compiles and runs parts of TF graphs via XLA, fusing multiple operators (kernel fusion) nto a small number of compiled kernels.\n",
    "        with jit_scope(): #this operation will be compiled with XLA.\n",
    "            for hid_i in range(1, FLAGS.n_hidden + 1):\n",
    "                hidden_out = self.nn_layer(hidden_out,\n",
    "                                      architecture['n_hidden_{:1d}'.format(hid_i)],\n",
    "                                      '{:1d}_hidden'.format(hid_i), FLAGS, act, train_flag)\n",
    "        return hidden_out        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(object):\n",
    "    \n",
    "    def __init__(self, func, nstep_per_epoch=None, dtype='trainer'):\n",
    "        \n",
    "        if dtype == 'trainer':            \n",
    "            self.nstep_per_epoch = nstep_per_epoch\n",
    "            #self.architecture = architecture\n",
    "            #self.FLAGS = FLAGS\n",
    "            with tf.device('/cpu:0'):\n",
    "                #self.global_step = tf.contrib.framework.get_or_create_global_step()\n",
    "                # tf.train.get_global_step()\n",
    "                self.global_step = tf.get_variable(\n",
    "                    'global_step', [],\n",
    "                    initializer=tf.constant_initializer(0),\n",
    "                    dtype=tf.int64,\n",
    "                    trainable=False)\n",
    "        elif dtype != 'evaluator': #Evaluator\n",
    "            raise ValueError('Invalid dtype value: ' + dtype)\n",
    "\n",
    "        self.func = func\n",
    "        self.dtype = dtype\n",
    "\n",
    "    def get_learning_rate(self, initial_learning_rate):\n",
    "        \"\"\"Get the learning rate.\"\"\"\n",
    "        with tf.name_scope('learning_rate') as scope:\n",
    "            if FLAGS.lr_decay_policy == 'poly':\n",
    "                return tf.train.polynomial_decay(\n",
    "                                        initial_learning_rate,\n",
    "                                        self.global_step,\n",
    "                                        decay_steps=FLAGS.epoch_num*self.nstep_per_epoch,\n",
    "                                        end_learning_rate=0.,\n",
    "                                        power=FLAGS.lr_poly_power,\n",
    "                                        cycle=False)\n",
    "            elif FLAGS.lr_decay_policy == 'exp':\n",
    "                return tf.train.exponential_decay(\n",
    "                                        initial_learning_rate,\n",
    "                                        self.global_step,\n",
    "                                        decay_steps=FLAGS.lr_decay_epochs*self.nstep_per_epoch,\n",
    "                                        decay_rate=FLAGS.lr_decay_rate,\n",
    "                                        staircase=True)\n",
    "            else:            \n",
    "                # decayed_lr = tf.train.exponential_decay(\n",
    "                #     initial_learning_rate,\n",
    "                #     global_step,\n",
    "                #     FLAGS.decay_step,\n",
    "                #     FLAGS.decay_base,\n",
    "                #     staircase=False)\n",
    "                decayed_lr = tf.train.inverse_time_decay(\n",
    "                    initial_learning_rate,\n",
    "                    self.global_step,\n",
    "                    decay_steps=FLAGS.decay_step,\n",
    "                    decay_rate=FLAGS.decay_rate)\n",
    "                final_lr = tf.clip_by_value(\n",
    "                    decayed_lr, FLAGS.rate_min, 1000, name=scope)\n",
    "                tf.summary.scalar('value', final_lr)\n",
    "                return final_lr\n",
    "        # return self.learning_rate \n",
    "\n",
    "    def get_accuracy(self, labels_int, logits, name):\n",
    "        \"\"\"Get the accuracy tensor.\"\"\"\n",
    "        with tf.name_scope(name) as scope:\n",
    "            # For a classifier model, we can use the in_top_k Op.\n",
    "            # It returns a bool tensor with shape [batch_size] that is true for\n",
    "            # the examples where the label is in the top k (here k=1)\n",
    "            # of all logits for that example.\n",
    "            correct = tf.nn.in_top_k(\n",
    "                logits, labels_int, 1, name='correct_prediction') # returns a tensor of type bool.\n",
    "            return tf.reduce_mean(tf.cast(correct, DT_FLOAT), name=scope)\n",
    "\n",
    "    # auc = get_auc(labels, probs, True, 'metrics/auc')\n",
    "    def get_auc(self, labels, scores, hist_flag, name):\n",
    "        \"\"\"Calculate the AUC of the two-way classifier for the given class.\"\"\"\n",
    "\n",
    "        def get_auc_using_histogram(labels, scores, class_, name):\n",
    "            \"\"\"Calculate the AUC.\"\"\"\n",
    "            class_ind = class_dict[class_.upper()]\n",
    "            with tf.name_scope(name) as scope:\n",
    "                auc, update_op = tf.contrib.metrics.auc_using_histogram( # his Op maintains Variables containing histograms of the scores associated with True and False labels. \n",
    "                    tf.cast(labels[:, class_ind], tf.bool),\n",
    "                    scores[:, class_ind],\n",
    "                    score_range=[0.0, 1.0],\n",
    "                    nbins=200,\n",
    "                    collections=None,\n",
    "                    name=scope)\n",
    "            ops.add_to_collections(ops.GraphKeys.UPDATE_OPS, update_op)\n",
    "            # print(update_op.name)\n",
    "            # print(auc) # it doesn't work because FailedPreconditionError (see above for traceback): Attempting to use uninitialized value metrics/auc/0//hist_accumulate/hist_true_acc\n",
    "            # aucp = tf.Print(auc,[auc], message='AUC the label: ' + class_) # it doesnt work because it doesnt run in a session\n",
    "            # print(aucp)\n",
    "            return auc\n",
    "\n",
    "        def get_auc_metric(labels, scores, class_, name):\n",
    "            \"\"\"Determine the AUC using conventional methods.\"\"\"\n",
    "            class_ind = class_dict[class_.upper()]\n",
    "            with tf.name_scope(name) as scope:\n",
    "                auc, _ = tf.metrics.auc( # Computes the approximate AUC via a Riemann sum.\n",
    "                    tf.cast(labels[:, class_ind], tf.bool), # ?? Print out!!\n",
    "                    scores[:, class_ind],\n",
    "                    weights=None,\n",
    "                    num_thresholds=200,\n",
    "                    metrics_collections=None,\n",
    "                    updates_collections=ops.GraphKeys.UPDATE_OPS,\n",
    "                    curve='ROC',\n",
    "                    name=scope)\n",
    "            # print(auc.op.name)\n",
    "            return auc\n",
    "\n",
    "        classes = ['0', '3', '6', '9', 'C', 'F', 'R']\n",
    "        class_dict = {classes[ind]: ind for ind in range(len(classes))}\n",
    "        if hist_flag:\n",
    "            auc_func = get_auc_using_histogram\n",
    "        else:\n",
    "            auc_func = get_auc_metric\n",
    "        with tf.name_scope(name) as scope:\n",
    "            aucv = [\n",
    "                    auc_func(labels, scores, class_, str(ind)) for ind, class_ in enumerate(classes) # pair (index ej. 0, value ej. '0')\n",
    "                   ]      \n",
    "            auc_values = tf.stack( # Pack along first dim\n",
    "                aucv,\n",
    "                axis=0,\n",
    "                name=scope)\n",
    "            # aucv = tf.Print(auc_values,[auc_values], message='AUC for all labels: ')\n",
    "            # print(aucv) # or maybe aucv.eval() or var = tf.Variable(aucv) and then var.eval(session=sess), or ovar = sess.run(var) but Attempting to use uninitialized value metrics/auc/Variable\n",
    "            return auc_values\n",
    "\n",
    "\n",
    "    # conf_mtx = get_confusion_matrix(labels_int, predictions, len(classes), 'metrics/confusion')\n",
    "    def get_confusion_matrix(self, labels_int, predictions, num_classes, name):\n",
    "        \"\"\"Get the confusion matrix.\n",
    "        Both prediction and labels must be 1-D arrays of the same shape in order for \n",
    "        this function to work.\n",
    "        \"\"\"\n",
    "        with tf.name_scope(name) as scope:\n",
    "            conf = tf.confusion_matrix(\n",
    "                labels_int,\n",
    "                predictions=predictions,\n",
    "                num_classes=num_classes,\n",
    "                dtype=tf.int32,\n",
    "                name=scope,\n",
    "                weights=None)\n",
    "        # print(conf.op.name)\n",
    "        return conf #return a K x K Matriz K = num_classes\n",
    "\n",
    "\n",
    "    def get_m_hand(self, labels, scores, name):\n",
    "        \"\"\"Implement the M measure described in Hand.\n",
    "\n",
    "        See ```A Simple Generalisation of the Area Under the ROC Curve for Multiple\n",
    "        Class Classification Problems``` Hand, Till 2001.    \n",
    "\n",
    "        \"\"\"\n",
    "        def get_auc_using_histogram(labels, scores, first_ind, second_ind, scope):\n",
    "            \"\"\"Calculate the AUC.\n",
    "            Calculate the AUC value by maintainig histograms of boolean variables (labels and \n",
    "            scores masked by the First-Second Individuals rule).\n",
    "            \"\"\"\n",
    "            mask = (labels[:, first_ind] + labels[:, second_ind]) > 0 #one in at least one column.\n",
    "            auc, update_op = tf.contrib.metrics.auc_using_histogram( # maintains variables containing histograms of the scores associated with True, False labels. \n",
    "                tf.cast(tf.boolean_mask(labels[:, first_ind], mask), tf.bool), # tf.boolean_mask: Apply boolean mask to tensor. Numpy equivalent is tensor[mask].\n",
    "                tf.boolean_mask(scores[:, first_ind], mask),\n",
    "                score_range=[0.0, 1.0],\n",
    "                nbins=500,\n",
    "                collections=None,\n",
    "                name=scope)\n",
    "            ops.add_to_collections(ops.GraphKeys.UPDATE_OPS, update_op)\n",
    "            # print(update_op.name)\n",
    "            return auc\n",
    "\n",
    "        temp_array = []\n",
    "        with tf.name_scope(name) as main_scope:\n",
    "            for first_ind in range(7):\n",
    "                for second_ind in range(7):\n",
    "                    if first_ind != second_ind:\n",
    "                        final_name = '{:d}{:d}'.format(first_ind, second_ind)\n",
    "                        with tf.name_scope(final_name) as scope:\n",
    "                            auc = get_auc_using_histogram(\n",
    "                                labels, scores, first_ind, second_ind, scope)\n",
    "                        temp_array.append(auc)\n",
    "            return tf.stack(temp_array, axis=0, name=main_scope) # Stacks a list of rank-R tensors into one rank-(R+1) tensor.\n",
    "\n",
    "\n",
    "    def get_auc_pr_curve(self, labels, scores, name, num_thresholds):    \n",
    "        with tf.name_scope(name) as scope:                             \n",
    "            AUC_PR = []\n",
    "            AUC_data = []\n",
    "            for i in range(7):  \n",
    "                data, update_op = tf.contrib.metrics.precision_recall_at_equal_thresholds(\n",
    "                                name='pr_data',\n",
    "                                predictions=scores[:, i],\n",
    "                                labels=tf.cast(labels[:, i], tf.bool),\n",
    "                                num_thresholds=10, use_locking=True)\n",
    "                ops.add_to_collections(ops.GraphKeys.UPDATE_OPS, update_op)\n",
    "                AUC_data.append((tf.stack(data.recall), tf.stack(data.precision), tf.stack(data.thresholds)))   # we cant use sklearn with tensorflow definition!\n",
    "                auc, _ = tf.metrics.auc(labels[:, i], scores[:, i], weights=None, num_thresholds=10, \n",
    "                                        curve='PR', updates_collections=ops.GraphKeys.UPDATE_OPS, metrics_collections=None, summation_method='careful_interpolation') # \n",
    "                # ops.add_to_collections(ops.GraphKeys.UPDATE_OPS, update_op)\n",
    "                AUC_PR.append(auc)\n",
    "            # print(AUC_data)\n",
    "            return tf.stack( # Pack the array of scalar tensor along one dim tensor\n",
    "                AUC_PR,\n",
    "                axis=0,\n",
    "                name=scope), AUC_data\n",
    "\n",
    "    def log_loss(self, labels, probs, name):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            labels: Labels tensor, int32 - [batch_size, n_classes], with one-hot\n",
    "            encoded values.\n",
    "            logits: Probabilities tensor, float32 - [batch_size, n_classes].\n",
    "        \"\"\"\n",
    "        with tf.name_scope(name) as scope:\n",
    "            total_loss = 0\n",
    "            for j in range(probs.shape[1].value):\n",
    "                loss = tf.losses.log_loss(labels[:, j], probs[:, j], loss_collection=None)\n",
    "                total_loss += loss\n",
    "\n",
    "            return tf.div(total_loss, np.float32(probs.shape[1].value), name=scope)\n",
    "    \n",
    "    def calculate_metrics(self, labels, logits):\n",
    "        \"\"\"Evaluate the quality of the logits at predicting the label.\n",
    "\n",
    "        Args:\n",
    "            labels: Labels tensor, int32 - [batch_size, n_classes], with one-hot\n",
    "            encoded values.\n",
    "            logits: Logits tensor, float32 - [batch_size, n_classes].\n",
    "        Returns:\n",
    "            A scalar float32 tensor with the fraction of examples (out of\n",
    "            batch_size) that were predicted correctly.\n",
    "        \"\"\"\n",
    "        classes = ['0', '3', '6', '9', 'C', 'F', 'R']\n",
    "        with tf.name_scope('metrics'):\n",
    "            labels_int = tf.argmax(labels, 1, name='intlabels') #tf.argmax: Returns the index with the largest value across axes=1 of a tensor.\t\t\n",
    "            predictions = tf.argmax(logits, 1, name='predictions')        \n",
    "            probs = tf.nn.softmax(logits, name='probs') # Computes softmax activations. softmax = tf.exp(logits) / tf.reduce_sum(tf.exp(logits), axis)        \n",
    "\n",
    "        m_list = self.get_m_hand(labels, probs, 'metrics/m_measure')\n",
    "        accuracy = self.get_accuracy(labels_int, logits, 'metrics/accuracy')    \n",
    "        auc = self.get_auc(labels, probs, True, 'metrics/auc')    \n",
    "        conf_mtx = self.get_confusion_matrix(labels_int, predictions,\n",
    "                                        len(classes), 'metrics/confusion')\n",
    "        lloss = self.log_loss(labels, probs, 'metrics/log_loss')\n",
    "        pr_auc, pr_data = self.get_auc_pr_curve(labels, probs, 'metrics/auc_pr', 200)\n",
    "\n",
    "        # this is for the definition of the graph:\n",
    "        return accuracy, conf_mtx, auc, m_list, lloss, pr_auc, pr_data\n",
    "    \n",
    "    def training_step(self, architecture, FLAGS):        \n",
    "        features = tf.placeholder(\n",
    "            DT_FLOAT, [None, architecture['n_input']], name='features')\n",
    "        labels = tf.placeholder(\n",
    "            DT_FLOAT, [None, architecture['n_classes']], name='targets')\n",
    "        # epoch_flag = tf.placeholder(tf.int32, None, name='epoch_flag')\n",
    "        example_weights = tf.placeholder(\n",
    "            DT_FLOAT, [None], name='example_weights')\n",
    "        with tf.device('/gpu:0'):\n",
    "            # Evaluate the loss:\n",
    "            loss, logits = self.func(features, labels, example_weights, architecture, FLAGS)\n",
    "\n",
    "        with tf.device('/cpu:0'): # No in_top_k implem on GPU\n",
    "            accuracy, conf_mtx, auc_list, m_list, lloss, auc_pr, auc_data = self.calculate_metrics(labels, logits)\n",
    "            better_acc = tf.reduce_mean(tf.diag_part(conf_mtx / tf.reduce_sum(conf_mtx, axis=1, keepdims=True)))\n",
    "            auc_mean = tf.reduce_mean(auc_list)\n",
    "            m_list_mean = tf.reduce_mean(m_list)\n",
    "            auc_pr_mean = tf.reduce_mean(auc_pr)\n",
    "            \n",
    "            with tf.name_scope('0_performance'):\n",
    "                # Scalar summaries to track the loss and accuracy over time in TB.\n",
    "                tf.summary.scalar('0accuracy', accuracy)\n",
    "                tf.summary.scalar('1better_accuracy', better_acc)\n",
    "                tf.summary.scalar('2auc_aoc', auc_mean)\n",
    "                tf.summary.scalar('3m_measure', m_list_mean)\n",
    "                tf.summary.scalar('4loss', loss)\n",
    "                tf.summary.scalar('5log_loss', lloss)\n",
    "                tf.summary.scalar('6auc_pr', auc_pr_mean)\n",
    "\n",
    "        # Apply the gradients to optimize the loss function\n",
    "        with tf.device('/gpu:0'):            \n",
    "            update_ops = ops.get_collection(ops.GraphKeys.UPDATE_OPS)\n",
    "            # print(update_ops)\n",
    "            with ops.control_dependencies(update_ops):\n",
    "                with tf.name_scope('train') as scope:\n",
    "                    # print_loss = tf.Print(loss, [loss], name='print_loss') \n",
    "\n",
    "                    # Create a variable to track the global step.\n",
    "        #            global_step = tf.get_variable(\n",
    "        #                'train/global_step',\n",
    "        #                shape=[],\n",
    "        #                initializer=tf.constant_initializer(0, dtype=tf.int32),\n",
    "        #                trainable=False)            \n",
    "                    # Horovod: adjust learning rate based on number of GPUs.\n",
    "                    # optimizer = tf.train.GradientDescentOptimizer(1.0 * hvd.size())\n",
    "                    final_learning_rate = self.get_learning_rate(FLAGS.learning_rate * hvd.size())\n",
    "\n",
    "                    # optimizer = tf.train.GradientDescentOptimizer(final_learning_rate)\n",
    "                    optimizer = tf.train.MomentumOptimizer(final_learning_rate, FLAGS.momentum, use_nesterov=True)\n",
    "                    # optimizer = tf.train.AdagradOptimizer(final_learning_rate)\n",
    "\n",
    "                    # Use the optimizer to apply the gradients that minimize the loss\n",
    "                    # (and increment the global step counter) as a single training step.\n",
    "        #            return optimizer.minimize(\n",
    "        #                loss, global_step=global_step, name=scope)\n",
    "                    optimizer = hvd.DistributedOptimizer(optimizer) #HVD!!\n",
    "                    train_op = optimizer.minimize(loss, global_step=self.global_step, name=scope)\n",
    "            \n",
    "                        \n",
    "        return train_op, final_learning_rate, conf_mtx, accuracy, better_acc, auc_list, auc_mean, m_list, m_list_mean, loss, lloss, auc_pr, auc_pr_mean, auc_data\n",
    "\n",
    "    def evaluation_step(self, batch_size):\n",
    "        \n",
    "        if (self.dtype!='evaluator'):\n",
    "            raise ValueError('Invalid function for dtype: ' + self.dtype)\n",
    "            \n",
    "        features = tf.placeholder(\n",
    "            DT_FLOAT, [None, architecture['n_input']], name='features')\n",
    "        labels = tf.placeholder(\n",
    "            DT_FLOAT, [None, architecture['n_classes']], name='targets')\n",
    "        example_weights = tf.placeholder(\n",
    "            DT_FLOAT, [None], name='example_weights')\n",
    "\n",
    "        with tf.device('/gpu:0'):\n",
    "            # Evaluate the loss:\n",
    "            logits = self.func(features, labels, architecture, FLAGS)        \n",
    "            accuracy, conf_mtx = self.calculate_metrics(labels, logits)[:2]\n",
    "            better_acc = tf.reduce_mean(tf.diag_part(conf_mtx / tf.reduce_sum(conf_mtx, axis=1, keepdims=True)))\n",
    "\n",
    "        return conf_mtx, accuracy, better_acc #, auc_list, m_list, lloss, auc_pr, auc_data\n",
    "    \n",
    "    def init(self):\n",
    "        # init_op = tf.global_variables_initializer()\n",
    "        # sess.run(init_op)        \n",
    "        \"\"\"Add an Op to the graph to initialize the global and local variables.\"\"\"\n",
    "        with tf.name_scope('init') as scope:\n",
    "            with tf.name_scope('global'):\n",
    "                global_init = tf.global_variables_initializer()\n",
    "            with tf.name_scope('local'):\n",
    "                local_init = tf.local_variables_initializer()\n",
    "                # print(local_init.name)\n",
    "            #init_op = tf.group(global_init, local_init, name=scope)\n",
    "        return global_init, local_init\n",
    "        \n",
    "    def sync(self, sess):\n",
    "        sync_op = hvd.broadcast_global_variables(0)\n",
    "        sess.run(sync_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_func(features, labels, weights, architecture, FLAGS):\n",
    "    # Build the forward model\n",
    "    net = GPUNetworkBuilder(dtype=DT_FLOAT)\n",
    "    train_flag = tf.placeholder(tf.bool, None, name='train_flag')\n",
    "    with tf.name_scope('input_normalization') as scope:\n",
    "        feature_norm = features\n",
    "        net.variable_summaries('input_normalized', feature_norm, FLAGS.allow_summaries)\n",
    "    hidden_out = net.add_hidden_layers(feature_norm, architecture, FLAGS, train_flag)\n",
    "    # Linear output layer for the logits\n",
    "    logits = (net.nn_layer(hidden_out, architecture['n_classes'],'9_softmax_linear', FLAGS, 'IDENT', train_flag))\n",
    "    \n",
    "    with tf.name_scope('loss') as scope:\n",
    "        with tf.name_scope('regularization'):\n",
    "            penalty = tf.losses.get_regularization_loss(name='penalty') #Gets the total regularization loss from an optional scope name (sum for ol + 3h + 2h + 1h).\n",
    "            tf.summary.scalar('weight_norm', penalty / (1e-8 + FLAGS.reg_rate)) #for printing out\n",
    "        with tf.name_scope('cross_entropy') as xentropy_scope:\n",
    "            weighted_cross_entropy = tf.losses.softmax_cross_entropy(\n",
    "                onehot_labels=labels,\n",
    "                logits=logits,\n",
    "                weights=weights,  # 1.0,\n",
    "                scope=xentropy_scope,\n",
    "                loss_collection=ops.GraphKeys.LOSSES)\n",
    "            tf.summary.scalar('weighted_cross_entropy', weighted_cross_entropy)\n",
    "        loss= tf.add(weighted_cross_entropy, penalty, name=scope) # Returns x + y element-wise.    \n",
    "            \n",
    "    return loss, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_func(features, labels, architecture, FLAGS):\n",
    "    # Build the forward model\n",
    "    FLAGS.allow_summaries = False\n",
    "    net = GPUNetworkBuilder(dtype=DT_FLOAT)\n",
    "    train_flag = tf.placeholder(tf.bool, None, name='train_flag')    \n",
    "    hidden_out = net.add_hidden_layers(features, architecture, FLAGS, train_flag)\n",
    "    # Linear output layer for the logits\n",
    "    logits = (net.nn_layer(hidden_out, architecture['n_classes'],'9_softmax_linear', FLAGS, 'IDENT', train_flag))\n",
    "                \n",
    "    return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_start_time = time.time()\n",
    "tf.set_random_seed(1234+hvd.rank())\n",
    "np.random.seed(4321+hvd.rank())\n",
    "\n",
    "# create logger:\n",
    "log_name = FLAGS.name + '_' + str(hvd.rank())\n",
    "logger = logging.getLogger(log_name)\n",
    "logger.setLevel(logging.DEBUG)  # INFO, ERROR\n",
    "# file handler which logs debug messages\n",
    "fh = logging.FileHandler(os.path.join(FLAGS.logdir, log_name + '.log'))\n",
    "fh.setLevel(logging.DEBUG)\n",
    "# console handler\n",
    "ch = logging.StreamHandler()\n",
    "ch.setLevel(logging.INFO)\n",
    "# add formatter to the handlers\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "fh.setFormatter(formatter)\n",
    "ch.setFormatter(formatter)\n",
    "# add handlers to logger\n",
    "logger.addHandler(fh)\n",
    "logger.addHandler(ch)\n",
    "print(logger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download the data if it has not been downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_data(rank, FLAGS):\n",
    "    server = ftplib.FTP()\n",
    "    server.connect(str(os.environ.get(\"FTP_HOST\")), int(os.environ.get(\"FTP_PORT\")))\n",
    "    server.login(os.environ.get(\"FTP_USER\"), os.environ.get(\"FTP_PASS\"))\n",
    "\n",
    "    server.cwd(FLAGS.ftp_dir)               # change into ftp_dir directory\n",
    "    logger.info(\"FTP connection stablished by worker:  {}\".format(rank))\n",
    "    \n",
    "    filenames = server.nlst() # get filenames within the directory\n",
    "        \n",
    "    train_suffix = 'train_%d.h5' % rank\n",
    "    if (rank==0):\n",
    "        if FLAGS.eval:        \n",
    "            fname_suffix = 'test_%d.h5' % rank\n",
    "            filenames = [elem for elem in filenames if fname_suffix in elem]        \n",
    "        else:\n",
    "            valid_suffix = 'valid_%d.h5' % rank\n",
    "            filenames = [elem for elem in filenames if (train_suffix in elem or valid_suffix in elem)]                \n",
    "    else:\n",
    "        filenames = [elem for elem in filenames if (train_suffix in elem)]\n",
    "\n",
    "    for filename in filenames:        \n",
    "        if FLAGS.eval:\n",
    "            local_path = os.path.join(PRO_DIR, FLAGS.test_dir, filename)    \n",
    "        else:\n",
    "            if (str('train') in filename[-10:-5]):\n",
    "                local_path = os.path.join(PRO_DIR, FLAGS.train_dir, filename)    \n",
    "            elif (str('valid') in filename[-10:-5]):\n",
    "                local_path = os.path.join(PRO_DIR, FLAGS.valid_dir, filename)   \n",
    "            else: \n",
    "                continue\n",
    "                \n",
    "        if not os.path.exists(local_path):            \n",
    "            file = open(local_path, 'wb')\n",
    "            server.retrbinary('RETR '+ filename, file.write, 8*1024)            \n",
    "            file.close()\n",
    "            logger.info(\"file downloaded:  {}\".format(filename))\n",
    "\n",
    "    server.quit() # This is the polite way to close a connection\n",
    "    logger.info(\"FTP connection closed by worker:  {}\".format(rank))\n",
    "\n",
    "def download_data_by_rank(rank, FLAGS):\n",
    "    server = ftplib.FTP()\n",
    "    server.connect(str(os.environ.get(\"FTP_HOST\")), int(os.environ.get(\"FTP_PORT\")))\n",
    "    server.login(os.environ.get(\"FTP_USER\"), os.environ.get(\"FTP_PASS\"))\n",
    "\n",
    "    server.cwd(FLAGS.ftp_dir)               # change into ftp_dir directory\n",
    "    logger.info(\"FTP connection stablished by worker:  {}\".format(rank))\n",
    "    \n",
    "    filenames = server.nlst() # get filenames within the directory\n",
    "    fname_suffix = '_%d.h5' % rank\n",
    "    filenames = [elem for elem in filenames if fname_suffix in elem]        \n",
    "    \n",
    "    for filename in filenames:            \n",
    "        local_path = os.path.join(PRO_DIR, FLAGS.train_dir, filename)                    \n",
    "        if not os.path.exists(local_path):            \n",
    "            file = open(local_path, 'wb')\n",
    "            server.retrbinary('RETR '+ filename, file.write, 8*1024)            \n",
    "            file.close()\n",
    "            logger.info(\"file downloaded:  {}\".format(filename))\n",
    "\n",
    "    server.quit() \n",
    "    logger.info(\"FTP connection closed by worker:  {}\".format(rank))\n",
    "\n",
    "\n",
    "#download_data_by_rank(hvd.rank(), FLAGS)\n",
    "download_data(hvd.rank(), FLAGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_records(tf_record_pattern):\n",
    "    def count_records(file_name):\n",
    "        count = 0\n",
    "        for _ in tf.python_io.tf_record_iterator(tf_record_filename):\n",
    "            count += 1\n",
    "        return count\n",
    "    filenames = sorted(tf.gfile.Glob(tf_record_pattern))\n",
    "    nfile = len(filenames)\n",
    "    return (count_records(filenames[0])*(nfile-1) +\n",
    "            count_records(filenames[-1]))\n",
    "\n",
    "def get_files_dict(FLAGS):        \n",
    "    ext = \"*.h5\"\n",
    "\n",
    "    if (hvd.rank()==0):\n",
    "        files_dict = {'train': glob.glob(os.path.join(PRO_DIR, FLAGS.train_dir, ext)), \n",
    "                      'valid': glob.glob(os.path.join(PRO_DIR, FLAGS.valid_dir, ext)), \n",
    "                      'test': glob.glob(os.path.join(PRO_DIR, FLAGS.test_dir, ext))}\n",
    "    else:\n",
    "        files_dict = {'train': glob.glob(os.path.join(PRO_DIR, FLAGS.train_dir, ext))}\n",
    "\n",
    "    return files_dict\n",
    "\n",
    "def architecture_settings(files_dict, FLAGS):\n",
    "    architecture = {}\n",
    "    architecture['rank'] = hvd.rank()\n",
    "    ok_inputs = True\n",
    "    for key in files_dict.keys():\n",
    "        total_records = 0\n",
    "        for file in files_dict[key]:                                \n",
    "            with pd.HDFStore(file) as dataset_file:\n",
    "                if (ok_inputs): \n",
    "                    index_length = len(dataset_file.get_storer(key+'/features').attrs.data_columns)\n",
    "                    architecture['n_input'] = dataset_file.get_storer(key+ '/features').ncols - index_length\n",
    "                    architecture['n_classes'] = dataset_file.get_storer(key+'/labels').ncols - index_length\n",
    "                    ok_inputs = False                \n",
    "                total_records += dataset_file.get_storer(key + '/features').nrows\n",
    "        architecture[key + '_num_examples'] = total_records                            \n",
    "    \n",
    "    if FLAGS.eval:\n",
    "        architecture['total_num_examples'] = architecture['test_num_examples']\n",
    "    else:\n",
    "        if FLAGS.total_examples == -1:\n",
    "            architecture['total_num_examples'] = architecture['train_num_examples']\n",
    "        else:\n",
    "            architecture['total_num_examples'] = FLAGS.total_examples \n",
    "    \n",
    "    for hid_i in range(1, FLAGS.n_hidden+1):\n",
    "        architecture['n_hidden_{:1d}'.format(hid_i)] = FLAGS.s_hidden[hid_i-1]\n",
    "    # print('rank: ', hvd.rank(), 'architecture', architecture)   \n",
    "    # time.sleep(5)\n",
    "    return architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To sum up the dataset per worker (assuming the same size of files per worker approximately):\n",
    "files_dict = get_files_dict(FLAGS)\n",
    "architecture = architecture_settings(files_dict, FLAGS)\n",
    "\n",
    "nrecord = architecture['total_num_examples']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Num ranks:  {}\".format(hvd.size()))\n",
    "logger.info(\"Num of records: {}\".format(nrecord))\n",
    "logger.info(\"Total batch size: {}\".format(FLAGS.batch_size * hvd.size()))\n",
    "logger.info(\"{}, per device\".format(FLAGS.batch_size))\n",
    "logger.info(\"Data type: {}\".format(DT_FLOAT)) \n",
    "logger.info(\"architecture: {}\".format(architecture)) \n",
    "# time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if FLAGS.eval:\n",
    "    if FLAGS.test_dir is None:\n",
    "        logger.error(\"eval requires data_dir to be specified\")\n",
    "        raise ValueError(\"eval requires data_dir to be specified\")\n",
    "    if hvd.size() > 1:\n",
    "        logger.error(\"Multi-GPU evaluation is not supported\")\n",
    "        raise ValueError(\"Multi-GPU evaluation is not supported\")\n",
    "    evaluator = FeedForward(eval_func, dtype='evaluator')\n",
    "    logger.info(\"Building evaluation graph\")\n",
    "    conf_mtx_op, accuracy_op, better_acc_op = evaluator.evaluation_step(FLAGS.test_batch_size)    \n",
    "    print(evaluator)\n",
    "else:    \n",
    "    nstep_per_epoch = nrecord // FLAGS.batch_size # if it is kwnow the total size: (FLAGS.batch_size * hvd.size())\n",
    "    logger.info(\"Number of steps per epoch: %d\" % nstep_per_epoch)\n",
    "    # model_func = lambda features, labels, architecture, FLAGS: loss_func(features, labels, architecture, FLAGS) # inference_vgg(net, images, nlayer)\n",
    "    trainer = FeedForward(loss_func, nstep_per_epoch=nstep_per_epoch)\n",
    "    logger.info(\"Building training graph\")    \n",
    "    train_ops, learning_rate_op, conf_mtx_op, accuracy_op, better_acc_op, auc_list_op, auc_mean_op, m_list_op, m_list_mean_op, total_loss_op, lloss_op, auc_pr_op, auc_pr_mean_op, auc_data_op = trainer.training_step(architecture, FLAGS)\n",
    "    logger.info(\"Graph building completed....\")\n",
    "    print(trainer)\n",
    "    global_init, local_init = trainer.init()\n",
    "\n",
    "logger.info(\"Creating session\")\n",
    "config = tf.ConfigProto(allow_soft_placement = True)\n",
    "config.intra_op_parallelism_threads = 1\n",
    "config.inter_op_parallelism_threads = 10\n",
    "config.gpu_options.force_gpu_compatible = True\n",
    "config.gpu_options.visible_device_list = str(hvd.local_rank())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(FLAGS.logdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining summary (writer) and checkpoint (saver) files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session(config=config)\n",
    "\n",
    "train_writer = None\n",
    "valid_writer = None\n",
    "saver = None\n",
    "summary_ops = None\n",
    "\n",
    "\n",
    "if hvd.rank() == 0 and FLAGS.logdir is not None:\n",
    "    train_writer = tf.summary.FileWriter(os.path.join(FLAGS.logdir), sess.graph)\n",
    "    valid_writer = tf.summary.FileWriter(os.path.join(FLAGS.logdir, 'valid'), graph=None)\n",
    "    summary_ops = tf.summary.merge_all()\n",
    "    last_summary_time = time.time()\n",
    "    saver = tf.train.Saver(keep_checkpoint_every_n_hours=1)\n",
    "    last_save_time = time.time()\n",
    "\n",
    "if not FLAGS.eval:        \n",
    "    logger.info(\"Initializing variables\")    \n",
    "    sess.run([global_init, local_init])\n",
    "\n",
    "restored = False\n",
    "if hvd.rank() == 0 and saver is not None:\n",
    "    ckpt = tf.train.get_checkpoint_state(FLAGS.logdir)\n",
    "    checkpoint_file = os.path.join(FLAGS.logdir, \"checkpoint\")\n",
    "    if ckpt and ckpt.model_checkpoint_path:\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "        restored = True\n",
    "        logger.info(\"Restored session from checkpoint {}\".format(ckpt.model_checkpoint_path))\n",
    "    else:\n",
    "        if not os.path.exists(FLAGS.logdir):\n",
    "            os.mkdir(FLAGS.logdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running evaluation from a checkpoint file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_better_acc(conf_mtx):\n",
    "    cfsum = conf_mtx.sum(axis=1, keepdims=True)\n",
    "    conf_mtx1 = np.divide(conf_mtx, cfsum, out=np.zeros_like(conf_mtx, dtype=np.float32), where=(cfsum!=0), dtype=np.float32)    \n",
    "    bett_acc = conf_mtx1.diagonal().mean()\n",
    "    return bett_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if not FLAGS.eval:\n",
    "\n",
    "    if (hvd.rank()==0):\n",
    "        DATA = md.get_h5_data(PRO_DIR, architecture, FLAGS.train_dir, FLAGS.valid_dir, None, train_period=FLAGS.train_period, valid_period=FLAGS.valid_period, test_period=FLAGS.test_period)         \n",
    "    else:\n",
    "        DATA = md.get_h5_data(PRO_DIR, architecture, FLAGS.train_dir, None, None, train_period=FLAGS.train_period, valid_period=FLAGS.valid_period, test_period=FLAGS.test_period) \n",
    "        \n",
    "    logger.info('Features List: {}'.format(DATA.train.features_list))\n",
    "    logger.info('Labels List: {}'.format(DATA.train.labels_list))\n",
    "    \n",
    "else:\n",
    "    DATA = md.get_h5_data(PRO_DIR, architecture, None, None, FLAGS.test_dir, train_period=FLAGS.train_period, valid_period=FLAGS.valid_period, test_period=FLAGS.test_period) \n",
    "    logger.info('Features List: {}'.format(DATA.test.features_list))\n",
    "    logger.info('Labels List: {}'.format(DATA.test.labels_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(DATA.train._dict[0]['nrows'])\n",
    "print(nrecord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feed_dict(tag, DATA, FLAGS):\n",
    "    \"\"\"Create the feed dictionary for mapping data onto placeholders in the graph.\"\"\"\n",
    "    if tag == 'batch':\n",
    "        if hvd.rank()==0:\n",
    "            batch_size = (FLAGS.batch_size // 2)\n",
    "        else:\n",
    "            batch_size = FLAGS.batch_size\n",
    "        \n",
    "        features, targets = DATA.train.next_random_batch(batch_size)        \n",
    "        example_weights = np.array([1.0], dtype=NP_FLOAT) # [1.0] * architecture['total_num_examples']                    \n",
    "        # logger.info('features shape: {}'.format(features.shape))\n",
    "    elif tag == 'train':\n",
    "        features = DATA.train.orig.features\n",
    "        targets = DATA.train.orig.labels\n",
    "        example_weights = np.ones_like(targets.iloc[:, 1].values)\n",
    "    elif tag == 'valid':\n",
    "        features, targets, example_weights = DATA.validation.next_sequential_batch(FLAGS.valid_batch_size)\n",
    "    else:\n",
    "        features, targets, example_weights = DATA.test.next_sequential_batch(FLAGS.test_batch_size)\n",
    "\n",
    "    # features[:, :7] = targets\n",
    "    if tag == 'batch':\n",
    "        k_prob_input = 0.9  # 0.9  # .85  # .75  # 0.8  # 0.6\n",
    "        k_prob = FLAGS.dropout_keep\n",
    "        t_flag = True\n",
    "    else:\n",
    "        k_prob_input = 1.0\n",
    "        k_prob = 1.0\n",
    "        t_flag = False\n",
    "\n",
    "    # Change the python dictionary to an io-buffer for a better performance.\n",
    "    # See here:\n",
    "    # https://www.tensorflow.org/performance/performance_guide\n",
    "    feed_d = {\n",
    "        'features:0': features,\n",
    "        'targets:0': targets,\n",
    "        'example_weights:0': example_weights,        \n",
    "        'train_flag:0': t_flag,\n",
    "        #'epoch_flag:0': FLAGS.epoch_flag,\n",
    "        #'1_hidden/dropout/keep_proba:0': k_prob_input,\n",
    "        #'2_hidden/dropout/keep_proba:0': k_prob,\n",
    "        # '3_hidden/dropout/keep_proba:0': k_prob,\n",
    "        # '4_hidden/dropout/keep_proba:0': k_prob,\n",
    "        # '5_hidden/dropout/keep_proba:0': k_prob,\n",
    "        '9_softmax_linear/dropout/keep_proba:0': k_prob\n",
    "    }\n",
    "\t\n",
    "\t# for any tag:\n",
    "    if (FLAGS.n_hidden > 0) :\n",
    "        # print ('k_prob_input', k_prob_input, type(k_prob_input))\n",
    "        feed_d['1_hidden/dropout/keep_proba:0'] = k_prob_input\n",
    "        for hid_i in range(2, FLAGS.n_hidden+1):\n",
    "            feed_d['{:1d}_hidden/dropout/keep_proba:0'.format(hid_i)] = k_prob\n",
    "    # print('feed_d', feed_d)\n",
    "    # print('batch shape: ', features.shape)    \n",
    "    return feed_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_evaluation(nstep, sess, enqueue_ops):\n",
    "    logger.info(\"Evaluating Model\")\n",
    "    accuracys = []\n",
    "    better_accs = []\n",
    "    logger.info(\"  Step  Accuracy  Better-Accuracy\")\n",
    "    for step in range(nstep):\n",
    "        try:\n",
    "            feed = create_feed_dict('test', DATA, FLAGS)     \n",
    "            #logger.info('feed dictionary was created')\n",
    "            conf_mtx, accuracy, better_acc = sess.run(enqueue_ops, feed_dict=feed)\n",
    "            #logger.info('operations were ran')\n",
    "            if (math.isnan(better_acc)):                \n",
    "                better_acc = calculate_better_acc(conf_mtx)                    \n",
    "                print('beter_acc nan')\n",
    "                \n",
    "            #if step == 0 or (step+1) % FLAGS.display_every == 0:\n",
    "            logger.info(\"% 6i %5.1f%% %5.1f%%\" % (step+1, accuracy*100, better_acc*100))\n",
    "            accuracys.append(accuracy)\n",
    "            better_accs.append(better_acc)\n",
    "            #print('acc and bett_acc appended')\n",
    "            \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"Keyboard interrupt\")\n",
    "            break\n",
    "        except  Exception  as e:        \n",
    "            raise ValueError('Error running Evaluation: ' + str(e))    \n",
    "\n",
    "    nstep = len(accuracys)\n",
    "    if nstep == 0:\n",
    "        return\n",
    "    accuracys = np.asarray(accuracys) * 100.\n",
    "    better_accs = np.asarray(better_accs) * 100.\n",
    "    acc_mean = np.mean(accuracys)\n",
    "    bettacc_mean = np.mean(better_accs)\n",
    "    if nstep > 2:\n",
    "        acc_uncertainty = np.std(accuracys, ddof=1) / np.sqrt(float(nstep))\n",
    "        bettacc_uncertainty = np.std(better_accs, ddof=1) / np.sqrt(float(nstep))\n",
    "    else:\n",
    "        acc_uncertainty = float('nan')\n",
    "        bettacc_uncertainty = float('nan')\n",
    "    acc_madstd = 1.4826*np.median(np.abs(accuracys - acc_mean))\n",
    "    bettacc_madstd = 1.4826*np.median(np.abs(better_accs - bettacc_mean))\n",
    "    logger.info('-' * 64)\n",
    "    logger.info('Validation Accuracy: %.3f %% +/- %.2f (jitter = %.1f)' % (\n",
    "        acc_mean, acc_uncertainty, acc_madstd))\n",
    "    logger.info('Validation Better Accuracy: %.3f %% +/- %.2f (jitter = %.1f)' % (\n",
    "        bettacc_mean, bettacc_uncertainty, bettacc_madstd))\n",
    "    logger.info('-' * 64)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nstep = 0\n",
    "if FLAGS.eval:\n",
    "    if not restored:\n",
    "        logger.error(\"No checkpoint found for evaluation\")\n",
    "        raise ValueError(\"No checkpoint found for evaluation\")\n",
    "    else:        \n",
    "        #nstep = nrecord // FLAGS.test_batch_size \n",
    "        nstep = DATA.test.total_num_batch(FLAGS.test_batch_size) \n",
    "        logger.info(\"total steps: {}\".format(nstep))        \n",
    "        enq_ops = [conf_mtx_op, accuracy_op, better_acc_op]\n",
    "        logger.info(\"Executing Evaluation\")        \n",
    "        run_evaluation(nstep, sess, enq_ops)   \n",
    "        logger.info('Evaluation was done')\n",
    "        # sys.exit(0) #the following instructiones will not be  executed\n",
    "        quit()\n",
    "else:    \n",
    "    if FLAGS.epoch_num is not None:\n",
    "        if (nrecord <= 0):\n",
    "            logger.error(\"num_epochs requires nrecord to be specified\")\n",
    "            raise ValueError(\"num_epochs requires nrecord to be specified\")\n",
    "        nstep = math.ceil(np.float32(nrecord * FLAGS.epoch_num / FLAGS.batch_size)) # if it is kwnow the total size: (FLAGS.batch_size * hvd.size()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info('METRICS:  %s\\r\\n' % str(FLAGS))\n",
    "logger.info('Number of total steps: %d' % nstep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# broadcast_global_variables from hvd\n",
    "trainer.sync(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying to restore for training:\n",
    "if hvd.rank() == 0 and not restored:\n",
    "    if saver is not None:\n",
    "        save_path = saver.save(sess, checkpoint_file, global_step=0)\n",
    "        print(\"Checkpoint written to\", save_path)\n",
    "\n",
    "logger.info(\"Writing summaries to {}\".format(FLAGS.logdir))\n",
    "logger.info(\"Training\")\n",
    "logger.info(\"  Step; Epoch; time-per-record(sec);  batchtime/worker(sec);  Loss;   Learning Rate; Accuracy; better_acc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_and_update(sess, local_init, feed_dict):\n",
    "    \"\"\"Reset the local variables and update the necessary update ops.\"\"\"\n",
    "    # sess.run(local_init) # this is necesary in each batch??check out the local variables!\n",
    "        \n",
    "    update_names_list = [\n",
    "        'metrics/auc/{:d}/hist_accumulate/update_op'.format(i)\n",
    "        for i in range(7)\n",
    "    ]\n",
    "\n",
    "    update_names_list.extend([\n",
    "        'metrics/m_measure/' + str(i) + str(j) + '/hist_accumulate/update_op'\n",
    "        for i in range(7) for j in range(7) if i != j\n",
    "    ])\n",
    "\n",
    "    sess.run(update_names_list, feed_dict=feed_dict)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_m_mtx(mtx):\n",
    "    \"\"\"Reshape the python list into a np array.\"\"\"\n",
    "    new_mtx = [0]\n",
    "    for i in range(6):\n",
    "        new_mtx.extend(mtx[i * 7:(i + 1) * 7])\n",
    "        new_mtx.append(0)\n",
    "    temp = np.array(new_mtx).reshape(7, 7)\n",
    "\n",
    "    return temp\n",
    "\n",
    "def print_stats(name, conf_mtx, accuracy, better_acc, auc_list, auc_mean, m_list, m_list_mean, lloss, auc_pr, auc_pr_mean, loss):\n",
    "    \"\"\"Print to logger the given stats.\"\"\"        \n",
    "                \n",
    "    m_mtx = np.nan_to_num(reshape_m_mtx(m_list)) \n",
    "    auc_list = np.nan_to_num(auc_list)\n",
    "    conf_mtx = np.array(conf_mtx, dtype=int)\n",
    "            \n",
    "    stdout = 'Loss in ' + name +': {:.5f}\\n'.format(loss)        \n",
    "    stdout = stdout + ' Avg Log_Loss in ' + name +': {:.5f}\\n'.format(lloss)\n",
    "    stdout = stdout +  '{:s}:'.format(name) + ' (Silly) Global-ACC={:.5f}, Better ACC={:.5f},'.format(accuracy, better_acc) + \\\n",
    "        ' Avg M-Measure={:.4f},'.format(m_list_mean) + \\\n",
    "        ' Avg AUC_AOC={:.4f}'.format(auc_mean) + ' Avg AUC_PR={:.4f}\\n'.format(auc_pr_mean)\n",
    "    stdout = stdout + (';').join(['Total Confusion Matrix', 'Total M-Measure Matrix', 'Total AUC_AOC', 'Total AUC_PR\\n'])\n",
    "    for conf_row, row, auc, auc_pr in zip(conf_mtx, m_mtx, auc_list, auc_pr):\n",
    "        for conf_value in conf_row:\n",
    "            stdout = stdout + '{}'.format(conf_value) + ';'\n",
    "        stdout = stdout + ';'\n",
    "        for value in row:\n",
    "            stdout = stdout + '{:.4f}'.format(value) + ';'\n",
    "        stdout = stdout + ';{:.4f}'.format(auc) + ' ;{:.4f}'.format(auc_pr) + '\\n'\n",
    "    stdout = stdout + '---------------------------------------------------------------------'\n",
    "              \n",
    "    logger.info('METRICS each %s (secs):  %s\\r\\n' % (FLAGS.summary_interval, stdout))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for validation set:\n",
    "def batching_dataset(sess, epoch, writer, tag, DATA, FLAGS):\n",
    "    if tag =='valid':\n",
    "        batch_num = DATA.validation.total_num_batch(FLAGS.valid_batch_size) \n",
    "        \n",
    "    # metrics = acc_metrics_init(DATA)\n",
    "    sess.run(local_init)\n",
    "    start_time = datetime.now()\n",
    "    acc_conf_mtx=np.zeros((DATA.train.num_classes, DATA.train.num_classes))        \n",
    "    metrics = [0.0] * 4\n",
    "    for batch_i in range(batch_num):\n",
    "        step = epoch * batch_num + batch_i    # total steps for all epochs        \n",
    "        feed = create_feed_dict(tag, DATA, FLAGS)     \n",
    "        reset_and_update(sess, local_init, feed)\n",
    "        summary, conf_mtx, accuracy, better_acc,  lloss, loss = sess.run([summary_ops, conf_mtx_op, accuracy_op, better_acc_op, lloss_op, total_loss_op], feed_dict=feed)\n",
    "        if (math.isnan(better_acc)):\n",
    "            better_acc = calculate_better_acc(conf_mtx)\n",
    "        acc_conf_mtx = np.add(acc_conf_mtx, conf_mtx)\n",
    "        metrics = np.add(metrics, np.array([accuracy, better_acc,  lloss, loss]))\n",
    "        writer.add_summary(summary, step)\n",
    "        writer.flush()\n",
    "        del feed\n",
    "    \n",
    "    metrics[:] = [x / batch_num for x in metrics]\n",
    "    valid_time = datetime.now() - start_time\n",
    "    logger.info('%s - Number of batches: %d; batch_size: %d; Total Time: %s' %(tag, batch_num,  FLAGS.valid_batch_size, valid_time))\n",
    "    return acc_conf_mtx, valid_time, metrics\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if hvd.rank() == 0:\n",
    "    if not FLAGS.eval:\n",
    "        dtype = ['step','epoch','batch_time','Loss','LogLoss','Accuracy','Better-Accuracy','M-Measure Mean','AUC_AOC Mean','AUC_PR Mean']\n",
    "        train_file = os.path.join(FLAGS.logdir, FLAGS.name + \"_train.csv\")\n",
    "        valid_file = os.path.join(FLAGS.logdir, FLAGS.name + \"_valid.csv\")        \n",
    "        \n",
    "        if not Path(train_file).exists():\n",
    "            df_train = pd.DataFrame(columns=dtype)                \n",
    "            df_train.to_csv(train_file, sep=';', index=False)\n",
    "        else:\n",
    "            df_train = pd.read_csv(train_file, sep=';')\n",
    "\n",
    "        if not Path(valid_file).exists():\n",
    "            df_valid = pd.DataFrame(columns=dtype[:7])\n",
    "            df_valid.to_csv(valid_file, sep=';', index=False)            \n",
    "        else:\n",
    "            df_valid = pd.read_csv(valid_file, sep=';')\n",
    "        \n",
    "        print('df_train: \\n', df_train)\n",
    "        print('df_valid: \\n', df_valid)\n",
    "        \n",
    "    else:  # validation set:\n",
    "        dtype = ['NN_name', 'NN_Number','Total Epochs', 'Execute Epochs', 'Total Training Time', 'Loss','LogLoss','Accuracy','Better-Accuracy','M-Measure Mean','AUC_AOC Mean','AUC_PR Mean']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ops_to_run = [learning_rate_op, train_ops]\n",
    "ops_stats = [conf_mtx_op, accuracy_op, better_acc_op, auc_list_op, auc_mean_op, m_list_op, m_list_mean_op, \n",
    "             lloss_op, auc_pr_op, auc_pr_mean_op, total_loss_op]\n",
    "                    \n",
    "oom = False\n",
    "step0 = int(sess.run(trainer.global_step))\n",
    "for step in range(step0, nstep):    \n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        epoch = step*FLAGS.batch_size // nrecord #*hvd.size()\n",
    "        batch_dict= create_feed_dict('batch', DATA, FLAGS)        \n",
    "        \n",
    "        if (hvd.rank() == 0 and summary_ops is not None and\n",
    "            (step == 0 or step+1 == nstep or\n",
    "             time.time() - last_summary_time > FLAGS.summary_interval)):\n",
    "            \n",
    "            if step != 0:\n",
    "                last_summary_time += FLAGS.summary_interval                        \n",
    "                \n",
    "            reset_and_update(sess, local_init, batch_dict)\n",
    "            summary, conf_mtx, accuracy, better_acc, auc_list, auc_mean, m_list, m_list_mean, lloss, auc_pr, auc_pr_mean, loss, lr, _ = sess.run([summary_ops] + ops_stats + ops_to_run, feed_dict=batch_dict)                        \n",
    "            train_writer.add_summary(summary, step)            \n",
    "            train_writer.flush()\n",
    "            if (math.isnan(better_acc)):\n",
    "                better_acc = calculate_better_acc(conf_mtx)        \n",
    "            elapsed = time.time() - start_time            \n",
    "            #this not necessarily matches with the display at console not even with validation set, due the summary_interval!\n",
    "            print_stats('---Training in Summary---', conf_mtx, accuracy, better_acc, auc_list, auc_mean, m_list, m_list_mean, lloss, auc_pr, auc_pr_mean, loss)                         \n",
    "            df_train.loc[len(df_train)] = [step+1, epoch+1, elapsed, loss, lloss, accuracy, better_acc, m_list_mean, auc_mean, auc_pr_mean]                                                    \n",
    "        else:\n",
    "            accuracy, conf_mtx, better_acc, loss, lr, _ = sess.run([accuracy_op, conf_mtx_op, better_acc_op, total_loss_op] + ops_to_run, feed_dict=batch_dict)\n",
    "            if (math.isnan(better_acc)):\n",
    "                better_acc = calculate_better_acc(conf_mtx)        \n",
    "            elapsed = time.time() - start_time\n",
    "        \n",
    "        if step == 0 or (step+1) % FLAGS.display_every == 0:                    \n",
    "            feature_per_sec = FLAGS.batch_size / elapsed                        \n",
    "            logger.info(\"%6i; %5i; %7.1f; %7.3f; %7.5f; %7.5f; %7.5f; %7.5f\" % (\n",
    "                step+1, epoch+1, feature_per_sec*hvd.size(), elapsed, loss, lr, accuracy, better_acc))        \n",
    "\n",
    "        if (hvd.rank() == 0 and  (step == 0 or (step+1) % nstep_per_epoch == 0 or step+1 == nstep)):\n",
    "            #Running validation set:\n",
    "            valid_conf_mtx, valid_time, metrics = batching_dataset(sess, epoch, valid_writer, 'valid', DATA, FLAGS)\n",
    "            #valid_conf_mtx = np.array2string(valid_conf_mtx, formatter={'int_type':lambda x: \"int(%)\" % x})\n",
    "            valid_conf_mtx = np.array(valid_conf_mtx, dtype=int)\n",
    "            logger.info(\"---Validation--- Training Step: %d; Training Epoch: %d; \\n Confusion Matrix:\\n %s\" % (step+1, epoch+1, str(valid_conf_mtx)))\n",
    "            df_valid.loc[len(df_valid)] = [step+1, epoch+1, valid_time, metrics[3], metrics[2], metrics[0], metrics[1]]            \n",
    "            logger.info(\"(Training Step, Training Epoch, loss, accuracy, better accuracy) in Validation: %6i; %5i; %7.5f; %7.5f; %7.5f\" % (\n",
    "                step+1, epoch+1, metrics[3], metrics[0], metrics[1]))    \n",
    "            sess.run(local_init)\n",
    "        \n",
    "        del batch_dict\n",
    "                    \n",
    "    except KeyboardInterrupt:\n",
    "        if hvd.rank() == 0:\n",
    "            df_train.to_csv(train_file, index=False, mode='a', sep =';', header=False)\n",
    "            df_valid.to_csv(valid_file, index=False, mode='a', sep =';', header=False)\n",
    "        logger.info(\"Keyboard interrupt\")\n",
    "        break\n",
    "    except tf.errors.ResourceExhaustedError:\n",
    "        elapsed = -1.\n",
    "        loss    = 0.\n",
    "        lr      = -1\n",
    "        if hvd.rank() == 0:\n",
    "            df_train.to_csv(train_file, index=False, mode='a', sep =';', header=False)\n",
    "            df_valid.to_csv(valid_file, index=False, mode='a', sep =';', header=False)\n",
    "        oom = True\n",
    "    \n",
    "    if (hvd.rank() == 0 and saver is not None and\n",
    "        (time.time() - last_save_time > FLAGS.save_interval or step+1 == nstep)):\n",
    "        last_save_time += FLAGS.save_interval\n",
    "        save_path = saver.save(sess, checkpoint_file, global_step=trainer.global_step)\n",
    "        print(\"Checkpoint written to\", save_path)\n",
    "    \n",
    "    if oom:\n",
    "        break\n",
    "        \n",
    "\n",
    "if hvd.rank() == 0:                               \n",
    "    df_train.to_csv(train_file, index=False, mode='a', sep =';', header=False)\n",
    "    df_valid.to_csv(valid_file, index=False, mode='a', sep =';', header=False)\n",
    "                               \n",
    "if train_writer is not None:\n",
    "    train_writer.close()\n",
    "\n",
    "if valid_writer is not None:\n",
    "    valid_writer.close()    \n",
    "    \n",
    "global_end_time = time.time()\n",
    "#logger.info(\"start time is {}, end time is {}\".format(global_start_time, global_end_time))\n",
    "logger.info('Time used in total: %.1f seconds' % (global_end_time - global_start_time))\n",
    "\n",
    "if oom:\n",
    "    print(\"Out of memory error detected, exiting\")\n",
    "    sys.exit(-2)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook nn_real_hvd-ntb-v5-batch_size.ipynb to python\n",
      "[NbConvertApp] Writing 66756 bytes to nn_real_hvd-ntb-v5-batch_size.py\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to=python  nn_real_hvd-ntb-v5-batch_size.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "nn_real_hvd-ntb-v5-batch_size.py                0%    0     0.0KB/s   --:-- ETA\r",
      "nn_real_hvd-ntb-v5-batch_size.py              100%   64KB  64.5KB/s   00:00    \r\n"
     ]
    }
   ],
   "source": [
    "!scp nn_real_hvd-ntb-v5-batch_size.py ubuntu@ec2-52-70-194-2.compute-1.amazonaws.com:/home/ubuntu/MLMortgage/notebooks/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/ubuntu/MLMortgage/src/data', '/home/ubuntu/MLMortgage/notebooks', '/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python36.zip', '/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6', '/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/lib-dynload', '/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages']\n",
      "['/home/ubuntu/MLMortgage/src/data', '/home/ubuntu/MLMortgage/notebooks', '/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python36.zip', '/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6', '/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/lib-dynload', '/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages']\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel package will be renamed from release 2.8; in order to keep installing from binary please use \"pip install psycopg2-binary\" instead. For details see: <http://initd.org/psycopg/docs/install.html#binary-install-from-pypi>.\n",
      "  \"\"\")\n",
      "2018-11-01 18:37:05,782 - matplotlib - DEBUG - CACHEDIR=/home/ubuntu/.cache/matplotlib\n",
      "2018-11-01 18:37:05,784 - matplotlib.font_manager - DEBUG - Using fontManager instance from /home/ubuntu/.cache/matplotlib/fontList.json\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel package will be renamed from release 2.8; in order to keep installing from binary please use \"pip install psycopg2-binary\" instead. For details see: <http://initd.org/psycopg/docs/install.html#binary-install-from-pypi>.\n",
      "  \"\"\")\n",
      "2018-11-01 18:37:05,814 - matplotlib - DEBUG - CACHEDIR=/home/ubuntu/.cache/matplotlib\n",
      "2018-11-01 18:37:05,816 - matplotlib.font_manager - DEBUG - Using fontManager instance from /home/ubuntu/.cache/matplotlib/fontList.json\n",
      "['/home/ubuntu/MLMortgage/src/data', '/home/ubuntu/MLMortgage/notebooks', '/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python36.zip', '/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6', '/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/lib-dynload', '/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages']\n",
      "['/home/ubuntu/MLMortgage/src/data', '/home/ubuntu/MLMortgage/notebooks', '/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python36.zip', '/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6', '/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/lib-dynload', '/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages']\n",
      "2018-11-01 18:37:05,858 - matplotlib.backends - DEBUG - backend agg version v2.2\n",
      "2018-11-01 18:37:05,892 - matplotlib.backends - DEBUG - backend agg version v2.2\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel package will be renamed from release 2.8; in order to keep installing from binary please use \"pip install psycopg2-binary\" instead. For details see: <http://initd.org/psycopg/docs/install.html#binary-install-from-pypi>.\n",
      "  \"\"\")\n",
      "2018-11-01 18:37:05,929 - matplotlib - DEBUG - CACHEDIR=/home/ubuntu/.cache/matplotlib\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel package will be renamed from release 2.8; in order to keep installing from binary please use \"pip install psycopg2-binary\" instead. For details see: <http://initd.org/psycopg/docs/install.html#binary-install-from-pypi>.\n",
      "  \"\"\")\n",
      "2018-11-01 18:37:05,961 - matplotlib - DEBUG - CACHEDIR=/home/ubuntu/.cache/matplotlib\n",
      "2018-11-01 18:37:05,931 - matplotlib.font_manager - DEBUG - Using fontManager instance from /home/ubuntu/.cache/matplotlib/fontList.json\n",
      "2018-11-01 18:37:05,963 - matplotlib.font_manager - DEBUG - Using fontManager instance from /home/ubuntu/.cache/matplotlib/fontList.json\n",
      "2018-11-01 18:37:06,012 - matplotlib.backends - DEBUG - backend agg version v2.2\n",
      "2018-11-01 18:37:06,045 - matplotlib.backends - DEBUG - backend agg version v2.2\n",
      "/home/ubuntu/MLMortgage/data/raw /home/ubuntu/MLMortgage/data/processed\n",
      "/home/ubuntu/MLMortgage/data/raw /home/ubuntu/MLMortgage/data/processed\n",
      "/home/ubuntu/MLMortgage/data/raw /home/ubuntu/MLMortgage/data/processed\n",
      "/home/ubuntu/MLMortgage/data/raw /home/ubuntu/MLMortgage/data/processed\n",
      "['/home/ubuntu/MLMortgage/src/data', '/home/ubuntu/MLMortgage/notebooks', '/home/ubuntu/src/cntk/bindings/python', '/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python36.zip', '/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6', '/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/lib-dynload', '/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages']\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel package will be renamed from release 2.8; in order to keep installing from binary please use \"pip install psycopg2-binary\" instead. For details see: <http://initd.org/psycopg/docs/install.html#binary-install-from-pypi>.\n",
      "  \"\"\")\n",
      "2018-11-01 18:37:07,979 - matplotlib - DEBUG - CACHEDIR=/home/ubuntu/.cache/matplotlib\n",
      "2018-11-01 18:37:07,983 - matplotlib.font_manager - DEBUG - Using fontManager instance from /home/ubuntu/.cache/matplotlib/fontList.json\n",
      "2018-11-01 18:37:08,342 - matplotlib.backends - DEBUG - backend module://ipykernel.pylab.backend_inline version unknown\n",
      "/home/ubuntu/MLMortgage/data/raw /home/ubuntu/MLMortgage/data/processed\n",
      "UNPARSED []\n",
      "existent directory\n",
      "FLAGS Namespace(allow_summaries=True, batch_layer_type='batch', batch_norm=True, batch_size=8850, batch_type='layer', data_dir='/input_data', decay_rate=1, decay_step=3520000, display_every=100, dropout=True, dropout_keep=0.9, epoch_flag=0, epoch_num=15, eval=False, ftp_dir='processed/c1mill', learning_rate=0.1, log_file=None, logdir=PosixPath('/home/ubuntu/summ_15ep_2wrk'), loss_tolerance=0.0001, lr_decay_epochs=30, lr_decay_policy='time', lr_decay_rate=0.1, lr_poly_power=2.0, max_epoch_size=-1, momentum=0.5, n_hidden=3, name='Xworkers_1mill', nstep_burnin=20, rate_min=0.0015, reg_rate=1e-05, s_hidden=[200, 140, 140], save_interval=450, stratified_flag=False, summary_interval=1800, test_batch_size=1200000, test_dir='chuncks_random_c1millx2_test', test_flag=True, test_period=[286, 304], total_examples=38500000, train_dir='chuncks_random_c1millx2_train', train_period=[121, 279], valid_batch_size=150000, valid_dir='chuncks_random_c1millx2_valid', valid_period=[280, 285], weighted_sampling=False, xla=True)\n",
      "UNPARSED []\n",
      "existent directory\n",
      "UNPARSED []\n",
      "FLAGS Namespace(allow_summaries=True, batch_layer_type='batch', batch_norm=True, batch_size=8850, batch_type='layer', data_dir='/input_data', decay_rate=1, decay_step=3520000, display_every=100, dropout=True, dropout_keep=0.9, epoch_flag=0, epoch_num=15, eval=False, ftp_dir='processed/c1mill', learning_rate=0.1, log_file=None, logdir=PosixPath('/home/ubuntu/summ_15ep_2wrk'), loss_tolerance=0.0001, lr_decay_epochs=30, lr_decay_policy='time', lr_decay_rate=0.1, lr_poly_power=2.0, max_epoch_size=-1, momentum=0.5, n_hidden=3, name='Xworkers_1mill', nstep_burnin=20, rate_min=0.0015, reg_rate=1e-05, s_hidden=[200, 140, 140], save_interval=450, stratified_flag=False, summary_interval=1800, test_batch_size=1200000, test_dir='chuncks_random_c1millx2_test', test_flag=True, test_period=[286, 304], total_examples=38500000, train_dir='chuncks_random_c1millx2_train', train_period=[121, 279], valid_batch_size=150000, valid_dir='chuncks_random_c1millx2_valid', valid_period=[280, 285], weighted_sampling=False, xla=True)\n",
      "existent directory\n",
      "UNPARSED []\n",
      "existent directory\n",
      "FLAGS Namespace(allow_summaries=True, batch_layer_type='batch', batch_norm=True, batch_size=8850, batch_type='layer', data_dir='/input_data', decay_rate=1, decay_step=3520000, display_every=100, dropout=True, dropout_keep=0.9, epoch_flag=0, epoch_num=15, eval=False, ftp_dir='processed/c1mill', learning_rate=0.1, log_file=None, logdir=PosixPath('/home/ubuntu/summ_15ep_2wrk'), loss_tolerance=0.0001, lr_decay_epochs=30, lr_decay_policy='time', lr_decay_rate=0.1, lr_poly_power=2.0, max_epoch_size=-1, momentum=0.5, n_hidden=3, name='Xworkers_1mill', nstep_burnin=20, rate_min=0.0015, reg_rate=1e-05, s_hidden=[200, 140, 140], save_interval=450, stratified_flag=False, summary_interval=1800, test_batch_size=1200000, test_dir='chuncks_random_c1millx2_test', test_flag=True, test_period=[286, 304], total_examples=38500000, train_dir='chuncks_random_c1millx2_train', train_period=[121, 279], valid_batch_size=150000, valid_dir='chuncks_random_c1millx2_valid', valid_period=[280, 285], weighted_sampling=False, xla=True)\n",
      "FLAGS Namespace(allow_summaries=True, batch_layer_type='batch', batch_norm=True, batch_size=8850, batch_type='layer', data_dir='/input_data', decay_rate=1, decay_step=3520000, display_every=100, dropout=True, dropout_keep=0.9, epoch_flag=0, epoch_num=15, eval=False, ftp_dir='processed/c1mill', learning_rate=0.1, log_file=None, logdir=PosixPath('/home/ubuntu/summ_15ep_2wrk'), loss_tolerance=0.0001, lr_decay_epochs=30, lr_decay_policy='time', lr_decay_rate=0.1, lr_poly_power=2.0, max_epoch_size=-1, momentum=0.5, n_hidden=3, name='Xworkers_1mill', nstep_burnin=20, rate_min=0.0015, reg_rate=1e-05, s_hidden=[200, 140, 140], save_interval=450, stratified_flag=False, summary_interval=1800, test_batch_size=1200000, test_dir='chuncks_random_c1millx2_test', test_flag=True, test_period=[286, 304], total_examples=38500000, train_dir='chuncks_random_c1millx2_train', train_period=[121, 279], valid_batch_size=150000, valid_dir='chuncks_random_c1millx2_valid', valid_period=[280, 285], weighted_sampling=False, xla=True)\n",
      "UNPARSED []\n",
      "existent directory\n",
      "FLAGS Namespace(allow_summaries=True, batch_layer_type='batch', batch_norm=True, batch_size=8850, batch_type='layer', data_dir='/input_data', decay_rate=1, decay_step=3520000, display_every=100, dropout=True, dropout_keep=0.9, epoch_flag=0, epoch_num=15, eval=False, ftp_dir='processed/c1mill', learning_rate=0.1, log_file=None, logdir=PosixPath('/home/ubuntu/summ_15ep_2wrk'), loss_tolerance=0.0001, lr_decay_epochs=30, lr_decay_policy='time', lr_decay_rate=0.1, lr_poly_power=2.0, max_epoch_size=-1, momentum=0.5, n_hidden=3, name='Xworkers_1mill', nstep_burnin=20, rate_min=0.0015, reg_rate=1e-05, s_hidden=[200, 140, 140], save_interval=450, stratified_flag=False, summary_interval=1800, test_batch_size=1200000, test_dir='chuncks_random_c1millx2_test', test_flag=True, test_period=[286, 304], total_examples=38500000, train_dir='chuncks_random_c1millx2_train', train_period=[121, 279], valid_batch_size=150000, valid_dir='chuncks_random_c1millx2_valid', valid_period=[280, 285], weighted_sampling=False, xla=True)\n",
      "<Logger Xworkers_1mill_3 (DEBUG)>\n",
      "<Logger Xworkers_1mill_1 (DEBUG)>\n",
      "<Logger Xworkers_1mill_4 (DEBUG)>\n",
      "<Logger Xworkers_1mill_2 (DEBUG)>\n",
      "<Logger Xworkers_1mill_0 (DEBUG)>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-01 18:37:09,401 - Xworkers_1mill_2 - INFO - FTP connection stablished by worker:  2\n",
      "2018-11-01 18:37:09,401 - Xworkers_1mill_2 - INFO - FTP connection stablished by worker:  2\n",
      "2018-11-01 18:37:09,440 - Xworkers_1mill_4 - INFO - FTP connection stablished by worker:  4\n",
      "2018-11-01 18:37:09,440 - Xworkers_1mill_4 - INFO - FTP connection stablished by worker:  4\n",
      "2018-11-01 18:37:09,415 - Xworkers_1mill_1 - INFO - FTP connection stablished by worker:  1\n",
      "2018-11-01 18:37:09,415 - Xworkers_1mill_1 - INFO - FTP connection stablished by worker:  1\n",
      "2018-11-01 18:37:09,415 - Xworkers_1mill_3 - INFO - FTP connection stablished by worker:  3\n",
      "2018-11-01 18:37:09,415 - Xworkers_1mill_3 - INFO - FTP connection stablished by worker:  3\n",
      "2018-11-01 18:37:09,366 - Xworkers_1mill_0 - INFO - FTP connection stablished by worker:  0\n",
      "2018-11-01 18:37:09,366 - Xworkers_1mill_0 - INFO - FTP connection stablished by worker:  0\n",
      "2018-11-01 18:37:09,652 - Xworkers_1mill_4 - INFO - FTP connection closed by worker:  4\n",
      "2018-11-01 18:37:09,652 - Xworkers_1mill_4 - INFO - FTP connection closed by worker:  4\n",
      "2018-11-01 18:37:09,616 - Xworkers_1mill_2 - INFO - FTP connection closed by worker:  2\n",
      "2018-11-01 18:37:09,616 - Xworkers_1mill_2 - INFO - FTP connection closed by worker:  2\n",
      "2018-11-01 18:37:09,633 - Xworkers_1mill_1 - INFO - FTP connection closed by worker:  1\n",
      "2018-11-01 18:37:09,633 - Xworkers_1mill_1 - INFO - FTP connection closed by worker:  1\n",
      "2018-11-01 18:37:09,634 - Xworkers_1mill_3 - INFO - FTP connection closed by worker:  3\n",
      "2018-11-01 18:37:09,634 - Xworkers_1mill_3 - INFO - FTP connection closed by worker:  3\n",
      "2018-11-01 18:37:09,584 - Xworkers_1mill_0 - INFO - FTP connection closed by worker:  0\n",
      "2018-11-01 18:37:09,584 - Xworkers_1mill_0 - INFO - FTP connection closed by worker:  0\n",
      "2018-11-01 18:37:09,687 - Xworkers_1mill_1 - INFO - Num ranks:  5\n",
      "2018-11-01 18:37:09,687 - Xworkers_1mill_1 - INFO - Num ranks:  5\n",
      "2018-11-01 18:37:09,687 - Xworkers_1mill_1 - INFO - Num of records: 38500000\n",
      "2018-11-01 18:37:09,687 - Xworkers_1mill_1 - INFO - Num of records: 38500000\n",
      "2018-11-01 18:37:09,687 - Xworkers_1mill_1 - INFO - Total batch size: 44250\n",
      "2018-11-01 18:37:09,687 - Xworkers_1mill_1 - INFO - Total batch size: 44250\n",
      "2018-11-01 18:37:09,687 - Xworkers_1mill_1 - INFO - 8850, per device\n",
      "2018-11-01 18:37:09,687 - Xworkers_1mill_1 - INFO - 8850, per device\n",
      "2018-11-01 18:37:09,687 - Xworkers_1mill_1 - INFO - Data type: <dtype: 'float32'>\n",
      "2018-11-01 18:37:09,687 - Xworkers_1mill_1 - INFO - Data type: <dtype: 'float32'>\n",
      "2018-11-01 18:37:09,687 - Xworkers_1mill_1 - INFO - architecture: {'rank': 1, 'n_input': 258, 'n_classes': 7, 'train_num_examples': 38931603, 'total_num_examples': 38500000, 'n_hidden_1': 200, 'n_hidden_2': 140, 'n_hidden_3': 140}\n",
      "2018-11-01 18:37:09,687 - Xworkers_1mill_1 - INFO - architecture: {'rank': 1, 'n_input': 258, 'n_classes': 7, 'train_num_examples': 38931603, 'total_num_examples': 38500000, 'n_hidden_1': 200, 'n_hidden_2': 140, 'n_hidden_3': 140}\n",
      "2018-11-01 18:37:09,687 - Xworkers_1mill_1 - INFO - Number of steps per epoch: 4350\n",
      "2018-11-01 18:37:09,687 - Xworkers_1mill_1 - INFO - Number of steps per epoch: 4350\n",
      "2018-11-01 18:37:09,680 - Xworkers_1mill_2 - INFO - Num ranks:  5\n",
      "2018-11-01 18:37:09,680 - Xworkers_1mill_2 - INFO - Num ranks:  5\n",
      "2018-11-01 18:37:09,680 - Xworkers_1mill_2 - INFO - Num of records: 38500000\n",
      "2018-11-01 18:37:09,680 - Xworkers_1mill_2 - INFO - Num of records: 38500000\n",
      "2018-11-01 18:37:09,680 - Xworkers_1mill_2 - INFO - Total batch size: 44250\n",
      "2018-11-01 18:37:09,680 - Xworkers_1mill_2 - INFO - Total batch size: 44250\n",
      "2018-11-01 18:37:09,680 - Xworkers_1mill_2 - INFO - 8850, per device\n",
      "2018-11-01 18:37:09,680 - Xworkers_1mill_2 - INFO - 8850, per device\n",
      "2018-11-01 18:37:09,681 - Xworkers_1mill_2 - INFO - Data type: <dtype: 'float32'>\n",
      "2018-11-01 18:37:09,681 - Xworkers_1mill_2 - INFO - Data type: <dtype: 'float32'>\n",
      "2018-11-01 18:37:09,681 - Xworkers_1mill_2 - INFO - architecture: {'rank': 2, 'n_input': 258, 'n_classes': 7, 'train_num_examples': 76862478, 'total_num_examples': 38500000, 'n_hidden_1': 200, 'n_hidden_2': 140, 'n_hidden_3': 140}\n",
      "2018-11-01 18:37:09,681 - Xworkers_1mill_2 - INFO - architecture: {'rank': 2, 'n_input': 258, 'n_classes': 7, 'train_num_examples': 76862478, 'total_num_examples': 38500000, 'n_hidden_1': 200, 'n_hidden_2': 140, 'n_hidden_3': 140}\n",
      "2018-11-01 18:37:09,681 - Xworkers_1mill_2 - INFO - Number of steps per epoch: 4350\n",
      "2018-11-01 18:37:09,681 - Xworkers_1mill_2 - INFO - Number of steps per epoch: 4350\n",
      "2018-11-01 18:37:09,721 - Xworkers_1mill_4 - INFO - Num ranks:  5\n",
      "2018-11-01 18:37:09,721 - Xworkers_1mill_4 - INFO - Num ranks:  5\n",
      "2018-11-01 18:37:09,721 - Xworkers_1mill_4 - INFO - Num of records: 38500000\n",
      "2018-11-01 18:37:09,721 - Xworkers_1mill_4 - INFO - Num of records: 38500000\n",
      "2018-11-01 18:37:09,721 - Xworkers_1mill_4 - INFO - Total batch size: 44250\n",
      "2018-11-01 18:37:09,721 - Xworkers_1mill_4 - INFO - Total batch size: 44250\n",
      "2018-11-01 18:37:09,722 - Xworkers_1mill_4 - INFO - 8850, per device\n",
      "2018-11-01 18:37:09,722 - Xworkers_1mill_4 - INFO - 8850, per device\n",
      "2018-11-01 18:37:09,722 - Xworkers_1mill_4 - INFO - Data type: <dtype: 'float32'>\n",
      "2018-11-01 18:37:09,722 - Xworkers_1mill_4 - INFO - Data type: <dtype: 'float32'>\n",
      "2018-11-01 18:37:09,722 - Xworkers_1mill_4 - INFO - architecture: {'rank': 4, 'n_input': 258, 'n_classes': 7, 'train_num_examples': 76818783, 'total_num_examples': 38500000, 'n_hidden_1': 200, 'n_hidden_2': 140, 'n_hidden_3': 140}\n",
      "2018-11-01 18:37:09,722 - Xworkers_1mill_4 - INFO - architecture: {'rank': 4, 'n_input': 258, 'n_classes': 7, 'train_num_examples': 76818783, 'total_num_examples': 38500000, 'n_hidden_1': 200, 'n_hidden_2': 140, 'n_hidden_3': 140}\n",
      "2018-11-01 18:37:09,722 - Xworkers_1mill_4 - INFO - Number of steps per epoch: 4350\n",
      "2018-11-01 18:37:09,722 - Xworkers_1mill_4 - INFO - Number of steps per epoch: 4350\n",
      "2018-11-01 18:37:09,692 - Xworkers_1mill_1 - INFO - Building training graph\n",
      "2018-11-01 18:37:09,692 - Xworkers_1mill_1 - INFO - Building training graph\n",
      "2018-11-01 18:37:09,685 - Xworkers_1mill_2 - INFO - Building training graph\n",
      "2018-11-01 18:37:09,685 - Xworkers_1mill_2 - INFO - Building training graph\n",
      "2018-11-01 18:37:09,694 - Xworkers_1mill_3 - INFO - Num ranks:  5\n",
      "2018-11-01 18:37:09,694 - Xworkers_1mill_3 - INFO - Num ranks:  5\n",
      "2018-11-01 18:37:09,694 - Xworkers_1mill_3 - INFO - Num of records: 38500000\n",
      "2018-11-01 18:37:09,694 - Xworkers_1mill_3 - INFO - Num of records: 38500000\n",
      "2018-11-01 18:37:09,694 - Xworkers_1mill_3 - INFO - Total batch size: 44250\n",
      "2018-11-01 18:37:09,694 - Xworkers_1mill_3 - INFO - Total batch size: 44250\n",
      "2018-11-01 18:37:09,694 - Xworkers_1mill_3 - INFO - 8850, per device\n",
      "2018-11-01 18:37:09,694 - Xworkers_1mill_3 - INFO - 8850, per device\n",
      "2018-11-01 18:37:09,694 - Xworkers_1mill_3 - INFO - Data type: <dtype: 'float32'>\n",
      "2018-11-01 18:37:09,694 - Xworkers_1mill_3 - INFO - Data type: <dtype: 'float32'>\n",
      "2018-11-01 18:37:09,695 - Xworkers_1mill_3 - INFO - architecture: {'rank': 3, 'n_input': 258, 'n_classes': 7, 'train_num_examples': 57439636, 'total_num_examples': 38500000, 'n_hidden_1': 200, 'n_hidden_2': 140, 'n_hidden_3': 140}\n",
      "2018-11-01 18:37:09,695 - Xworkers_1mill_3 - INFO - architecture: {'rank': 3, 'n_input': 258, 'n_classes': 7, 'train_num_examples': 57439636, 'total_num_examples': 38500000, 'n_hidden_1': 200, 'n_hidden_2': 140, 'n_hidden_3': 140}\n",
      "2018-11-01 18:37:09,695 - Xworkers_1mill_3 - INFO - Number of steps per epoch: 4350\n",
      "2018-11-01 18:37:09,695 - Xworkers_1mill_3 - INFO - Number of steps per epoch: 4350\n",
      "2018-11-01 18:37:09,727 - Xworkers_1mill_4 - INFO - Building training graph\n",
      "2018-11-01 18:37:09,727 - Xworkers_1mill_4 - INFO - Building training graph\n",
      "2018-11-01 18:37:09,700 - Xworkers_1mill_3 - INFO - Building training graph\n",
      "2018-11-01 18:37:09,700 - Xworkers_1mill_3 - INFO - Building training graph\n",
      "2018-11-01 18:37:09,857 - Xworkers_1mill_0 - INFO - Num ranks:  5\n",
      "2018-11-01 18:37:09,857 - Xworkers_1mill_0 - INFO - Num ranks:  5\n",
      "2018-11-01 18:37:09,857 - Xworkers_1mill_0 - INFO - Num of records: 38500000\n",
      "2018-11-01 18:37:09,857 - Xworkers_1mill_0 - INFO - Num of records: 38500000\n",
      "2018-11-01 18:37:09,857 - Xworkers_1mill_0 - INFO - Total batch size: 44250\n",
      "2018-11-01 18:37:09,857 - Xworkers_1mill_0 - INFO - Total batch size: 44250\n",
      "2018-11-01 18:37:09,858 - Xworkers_1mill_0 - INFO - 8850, per device\n",
      "2018-11-01 18:37:09,858 - Xworkers_1mill_0 - INFO - 8850, per device\n",
      "2018-11-01 18:37:09,858 - Xworkers_1mill_0 - INFO - Data type: <dtype: 'float32'>\n",
      "2018-11-01 18:37:09,858 - Xworkers_1mill_0 - INFO - Data type: <dtype: 'float32'>\n",
      "2018-11-01 18:37:09,858 - Xworkers_1mill_0 - INFO - architecture: {'rank': 0, 'n_input': 258, 'n_classes': 7, 'train_num_examples': 19435138, 'valid_num_examples': 2082911, 'test_num_examples': 6472386, 'total_num_examples': 38500000, 'n_hidden_1': 200, 'n_hidden_2': 140, 'n_hidden_3': 140}\n",
      "2018-11-01 18:37:09,858 - Xworkers_1mill_0 - INFO - architecture: {'rank': 0, 'n_input': 258, 'n_classes': 7, 'train_num_examples': 19435138, 'valid_num_examples': 2082911, 'test_num_examples': 6472386, 'total_num_examples': 38500000, 'n_hidden_1': 200, 'n_hidden_2': 140, 'n_hidden_3': 140}\n",
      "2018-11-01 18:37:09,858 - Xworkers_1mill_0 - INFO - Number of steps per epoch: 4350\n",
      "2018-11-01 18:37:09,858 - Xworkers_1mill_0 - INFO - Number of steps per epoch: 4350\n",
      "2018-11-01 18:37:09,881 - Xworkers_1mill_0 - INFO - Building training graph\n",
      "2018-11-01 18:37:09,881 - Xworkers_1mill_0 - INFO - Building training graph\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/contrib/metrics/python/ops/histogram_ops.py:83: remove_squeezable_dimensions (from tensorflow.contrib.framework.python.framework.tensor_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please switch to remove_squeezable_dimensions from tf.confusion_matrix. Note that the order of the inputs and outputs of labels and predictions have also been switched.\n",
      "2018-11-01 18:37:13,159 - tensorflow - WARNING - From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/contrib/metrics/python/ops/histogram_ops.py:83: remove_squeezable_dimensions (from tensorflow.contrib.framework.python.framework.tensor_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please switch to remove_squeezable_dimensions from tf.confusion_matrix. Note that the order of the inputs and outputs of labels and predictions have also been switched.\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/contrib/metrics/python/ops/histogram_ops.py:83: remove_squeezable_dimensions (from tensorflow.contrib.framework.python.framework.tensor_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please switch to remove_squeezable_dimensions from tf.confusion_matrix. Note that the order of the inputs and outputs of labels and predictions have also been switched.\n",
      "2018-11-01 18:37:13,208 - tensorflow - WARNING - From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/contrib/metrics/python/ops/histogram_ops.py:83: remove_squeezable_dimensions (from tensorflow.contrib.framework.python.framework.tensor_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please switch to remove_squeezable_dimensions from tf.confusion_matrix. Note that the order of the inputs and outputs of labels and predictions have also been switched.\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/contrib/metrics/python/ops/histogram_ops.py:83: remove_squeezable_dimensions (from tensorflow.contrib.framework.python.framework.tensor_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please switch to remove_squeezable_dimensions from tf.confusion_matrix. Note that the order of the inputs and outputs of labels and predictions have also been switched.\n",
      "2018-11-01 18:37:13,343 - tensorflow - WARNING - From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/contrib/metrics/python/ops/histogram_ops.py:83: remove_squeezable_dimensions (from tensorflow.contrib.framework.python.framework.tensor_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please switch to remove_squeezable_dimensions from tf.confusion_matrix. Note that the order of the inputs and outputs of labels and predictions have also been switched.\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/contrib/metrics/python/ops/histogram_ops.py:83: remove_squeezable_dimensions (from tensorflow.contrib.framework.python.framework.tensor_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please switch to remove_squeezable_dimensions from tf.confusion_matrix. Note that the order of the inputs and outputs of labels and predictions have also been switched.\n",
      "2018-11-01 18:37:13,353 - tensorflow - WARNING - From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/contrib/metrics/python/ops/histogram_ops.py:83: remove_squeezable_dimensions (from tensorflow.contrib.framework.python.framework.tensor_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please switch to remove_squeezable_dimensions from tf.confusion_matrix. Note that the order of the inputs and outputs of labels and predictions have also been switched.\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/contrib/metrics/python/ops/histogram_ops.py:83: remove_squeezable_dimensions (from tensorflow.contrib.framework.python.framework.tensor_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please switch to remove_squeezable_dimensions from tf.confusion_matrix. Note that the order of the inputs and outputs of labels and predictions have also been switched.\n",
      "2018-11-01 18:37:16,003 - tensorflow - WARNING - From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/contrib/metrics/python/ops/histogram_ops.py:83: remove_squeezable_dimensions (from tensorflow.contrib.framework.python.framework.tensor_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please switch to remove_squeezable_dimensions from tf.confusion_matrix. Note that the order of the inputs and outputs of labels and predictions have also been switched.\n",
      "2018-11-01 18:37:25,187 - Xworkers_1mill_2 - INFO - Graph building completed....\n",
      "2018-11-01 18:37:25,187 - Xworkers_1mill_2 - INFO - Graph building completed....\n",
      "<__main__.FeedForward object at 0x7fc188183d30>\n",
      "2018-11-01 18:37:25,191 - Xworkers_1mill_2 - INFO - Creating session\n",
      "2018-11-01 18:37:25,191 - Xworkers_1mill_2 - INFO - Creating session\n",
      "/home/ubuntu/summ_15ep_2wrk\n",
      "2018-11-01 18:37:25.305459: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2018-11-01 18:37:25.305910: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1356] Found device 0 with properties: \n",
      "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
      "pciBusID: 0000:00:1e.0\n",
      "totalMemory: 11.17GiB freeMemory: 11.10GiB\n",
      "2018-11-01 18:37:25.305938: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1435] Adding visible gpu devices: 0\n",
      "2018-11-01 18:37:25,326 - Xworkers_1mill_1 - INFO - Graph building completed....\n",
      "2018-11-01 18:37:25,326 - Xworkers_1mill_1 - INFO - Graph building completed....\n",
      "<__main__.FeedForward object at 0x7f986811c9b0>\n",
      "2018-11-01 18:37:25,330 - Xworkers_1mill_1 - INFO - Creating session\n",
      "2018-11-01 18:37:25,330 - Xworkers_1mill_1 - INFO - Creating session\n",
      "/home/ubuntu/summ_15ep_2wrk\n",
      "2018-11-01 18:37:25.444258: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2018-11-01 18:37:25.444708: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1356] Found device 0 with properties: \n",
      "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
      "pciBusID: 0000:00:1e.0\n",
      "totalMemory: 11.17GiB freeMemory: 11.10GiB\n",
      "2018-11-01 18:37:25.444738: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1435] Adding visible gpu devices: 0\n",
      "2018-11-01 18:37:25,552 - Xworkers_1mill_4 - INFO - Graph building completed....\n",
      "2018-11-01 18:37:25,552 - Xworkers_1mill_4 - INFO - Graph building completed....\n",
      "<__main__.FeedForward object at 0x7f3f4f753668>\n",
      "2018-11-01 18:37:25,556 - Xworkers_1mill_4 - INFO - Creating session\n",
      "2018-11-01 18:37:25,556 - Xworkers_1mill_4 - INFO - Creating session\n",
      "/home/ubuntu/summ_15ep_2wrk\n",
      "2018-11-01 18:37:25.598953: I tensorflow/core/common_runtime/gpu/gpu_device.cc:923] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2018-11-01 18:37:25.599000: I tensorflow/core/common_runtime/gpu/gpu_device.cc:929]      0 \n",
      "2018-11-01 18:37:25.599009: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 0:   N \n",
      "2018-11-01 18:37:25.599284: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10760 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:1e.0, compute capability: 3.7)\n",
      "2018-11-01 18:37:25.665132: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2018-11-01 18:37:25.665560: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1356] Found device 0 with properties: \n",
      "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
      "pciBusID: 0000:00:1e.0\n",
      "totalMemory: 11.17GiB freeMemory: 11.10GiB\n",
      "2018-11-01 18:37:25.665592: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1435] Adding visible gpu devices: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-01 18:37:25,708 - Xworkers_1mill_2 - INFO - Initializing variables\n",
      "2018-11-01 18:37:25,708 - Xworkers_1mill_2 - INFO - Initializing variables\n",
      "2018-11-01 18:37:25.741681: I tensorflow/core/common_runtime/gpu/gpu_device.cc:923] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2018-11-01 18:37:25.741729: I tensorflow/core/common_runtime/gpu/gpu_device.cc:929]      0 \n",
      "2018-11-01 18:37:25.741738: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 0:   N \n",
      "2018-11-01 18:37:25.742046: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10760 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:1e.0, compute capability: 3.7)\n",
      "2018-11-01 18:37:25,852 - Xworkers_1mill_1 - INFO - Initializing variables\n",
      "2018-11-01 18:37:25,852 - Xworkers_1mill_1 - INFO - Initializing variables\n",
      "2018-11-01 18:37:25.966015: I tensorflow/core/common_runtime/gpu/gpu_device.cc:923] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2018-11-01 18:37:25.966065: I tensorflow/core/common_runtime/gpu/gpu_device.cc:929]      0 \n",
      "2018-11-01 18:37:25.966078: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 0:   N \n",
      "2018-11-01 18:37:25.966399: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10760 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:1e.0, compute capability: 3.7)\n",
      "2018-11-01 18:37:26,075 - Xworkers_1mill_4 - INFO - Initializing variables\n",
      "2018-11-01 18:37:26,075 - Xworkers_1mill_4 - INFO - Initializing variables\n",
      "/home/ubuntu/MLMortgage/data/processed/chuncks_random_c1millx2_train/2mill-3mill_cs1200_train_non_index_3.h5 ...to load\n",
      "2018-11-01 18:37:26,376 - Xworkers_1mill_3 - INFO - Graph building completed....\n",
      "2018-11-01 18:37:26,376 - Xworkers_1mill_3 - INFO - Graph building completed....\n",
      "<__main__.FeedForward object at 0x7f9814c86c88>\n",
      "2018-11-01 18:37:26,380 - Xworkers_1mill_3 - INFO - Creating session\n",
      "2018-11-01 18:37:26,380 - Xworkers_1mill_3 - INFO - Creating session\n",
      "/home/ubuntu/summ_15ep_2wrk\n",
      "/home/ubuntu/MLMortgage/data/processed/chuncks_random_c1millx2_train/temporalloandynmodifmrstaticitu-random-1mill-2mill_non_index-train_1.h5 ...to load\n",
      "2018-11-01 18:37:26.496412: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2018-11-01 18:37:26.496836: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1356] Found device 0 with properties: \n",
      "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
      "pciBusID: 0000:00:1e.0\n",
      "totalMemory: 11.17GiB freeMemory: 11.10GiB\n",
      "2018-11-01 18:37:26.496870: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1435] Adding visible gpu devices: 0\n",
      "/home/ubuntu/MLMortgage/data/processed/chuncks_random_c1millx2_train/temporalloandynmodifmrstaticitur4mill-5mil_non_index-train_2.h5 ...to load\n",
      "2018-11-01 18:37:26.811077: I tensorflow/core/common_runtime/gpu/gpu_device.cc:923] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2018-11-01 18:37:26.811142: I tensorflow/core/common_runtime/gpu/gpu_device.cc:929]      0 \n",
      "2018-11-01 18:37:26.811151: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 0:   N \n",
      "2018-11-01 18:37:26.811460: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10760 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:1e.0, compute capability: 3.7)\n",
      "2018-11-01 18:37:26,924 - Xworkers_1mill_3 - INFO - Initializing variables\n",
      "2018-11-01 18:37:26,924 - Xworkers_1mill_3 - INFO - Initializing variables\n",
      "/home/ubuntu/MLMortgage/data/processed/chuncks_random_c1millx2_train/temporalloandynmodifmrstaticitur6mill-7mill_non_index-train_3.h5 ...to load\n",
      "2018-11-01 18:37:28,524 - Xworkers_1mill_0 - INFO - Graph building completed....\n",
      "2018-11-01 18:37:28,524 - Xworkers_1mill_0 - INFO - Graph building completed....\n",
      "<__main__.FeedForward object at 0x7feae017bba8>\n",
      "2018-11-01 18:37:28,527 - Xworkers_1mill_0 - INFO - Creating session\n",
      "2018-11-01 18:37:28,527 - Xworkers_1mill_0 - INFO - Creating session\n",
      "/home/ubuntu/summ_15ep_2wrk\n",
      "2018-11-01 18:37:28.711292: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2018-11-01 18:37:28.711746: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1356] Found device 0 with properties: \n",
      "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
      "pciBusID: 0000:00:1e.0\n",
      "totalMemory: 11.17GiB freeMemory: 11.10GiB\n",
      "2018-11-01 18:37:28.711774: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1435] Adding visible gpu devices: 0\n",
      "2018-11-01 18:37:30.541209: I tensorflow/core/common_runtime/gpu/gpu_device.cc:923] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2018-11-01 18:37:30.541257: I tensorflow/core/common_runtime/gpu/gpu_device.cc:929]      0 \n",
      "2018-11-01 18:37:30.541266: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 0:   N \n",
      "2018-11-01 18:37:30.543464: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10760 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:1e.0, compute capability: 3.7)\n",
      "2018-11-01 18:37:33,246 - Xworkers_1mill_0 - INFO - Initializing variables\n",
      "2018-11-01 18:37:33,246 - Xworkers_1mill_0 - INFO - Initializing variables\n",
      "INFO:tensorflow:Restoring parameters from /home/ubuntu/summ_15ep_2wrk/checkpoint-46615\n",
      "2018-11-01 18:37:33,916 - tensorflow - INFO - Restoring parameters from /home/ubuntu/summ_15ep_2wrk/checkpoint-46615\n",
      "2018-11-01 18:37:34,036 - Xworkers_1mill_0 - INFO - Restored session from checkpoint /home/ubuntu/summ_15ep_2wrk/checkpoint-46615\n",
      "2018-11-01 18:37:34,036 - Xworkers_1mill_0 - INFO - Restored session from checkpoint /home/ubuntu/summ_15ep_2wrk/checkpoint-46615\n",
      "/home/ubuntu/MLMortgage/data/processed/chuncks_random_c1millx2_train/1-1mill_cs1200_non_index_train_0.h5 ...to load\n",
      "/home/ubuntu/MLMortgage/data/processed/chuncks_random_c1millx2_train/2mill-3mill_cs1200_train_non_index_3.h5  loaded in RAM\n",
      "/home/ubuntu/MLMortgage/data/processed/chuncks_random_c1millx2_train/temporalloandynmodifmrstaticitur6mill-7mill_non_index-train_3.h5 ...to load\n",
      "/home/ubuntu/MLMortgage/data/processed/chuncks_random_c1millx2_train/temporalloandynmodifmrstaticitur4mill-5mil_non_index-train_2.h5  loaded in RAM\n",
      "/home/ubuntu/MLMortgage/data/processed/chuncks_random_c1millx2_train/temporalloandynmodifmrstaticitur-5mill-6mill_non_index-train_2.h5 ...to load\n",
      "/home/ubuntu/MLMortgage/data/processed/chuncks_random_c1millx2_train/temporalloandynmodifmrstaticitu-random-1mill-2mill_non_index-train_1.h5  loaded in RAM\n",
      "/home/ubuntu/MLMortgage/data/processed/chuncks_random_c1millx2_train/temporalloandynmodifmrstaticitur3mill-4mill_non_index-train_1.h5 ...to load\n",
      "/home/ubuntu/MLMortgage/data/processed/chuncks_random_c1millx2_train/temporalloandynmodifmrstaticitur6mill-7mill_non_index-train_3.h5  loaded in RAM\n",
      "/home/ubuntu/MLMortgage/data/processed/chuncks_random_c1millx2_train/temporalloandynmodifmrstaticitur4mill-5mil_non_index-train_2.h5 ...to load\n",
      "/home/ubuntu/MLMortgage/data/processed/chuncks_random_c1millx2_train/1-1mill_cs1200_non_index_train_0.h5  loaded in RAM\n",
      "/home/ubuntu/MLMortgage/data/processed/chuncks_random_c1millx2_valid/2mill-3mill_cs1200_non_index_1_valid_0.h5 ...to load\n",
      "/home/ubuntu/MLMortgage/data/processed/chuncks_random_c1millx2_valid/2mill-3mill_cs1200_non_index_1_valid_0.h5  loaded in RAM\n",
      "/home/ubuntu/MLMortgage/data/processed/chuncks_random_c1millx2_valid/1-11mill_cs1200_non_index_valid_0.h5 ...to load\n",
      "/home/ubuntu/MLMortgage/data/processed/chuncks_random_c1millx2_valid/1-11mill_cs1200_non_index_valid_0.h5  loaded in RAM\n",
      "2018-11-01 18:38:34,253 - Xworkers_1mill_0 - INFO - Features List: ['MBA_DAYS_DELINQUENT', 'MBA_DAYS_DELINQUENT_NAN', 'CURRENT_INTEREST_RATE', 'CURRENT_INTEREST_RATE_NAN', 'LOANAGE', 'LOANAGE_NAN', 'CURRENT_BALANCE', 'CURRENT_BALANCE_NAN', 'SCHEDULED_PRINCIPAL', 'SCHEDULED_PRINCIPAL_NAN', 'SCHEDULED_MONTHLY_PANDI', 'SCHEDULED_MONTHLY_PANDI_NAN', 'LLMA2_CURRENT_INTEREST_SPREAD', 'LLMA2_CURRENT_INTEREST_SPREAD_NAN', 'LLMA2_C_IN_LAST_12_MONTHS', 'LLMA2_30_IN_LAST_12_MONTHS', 'LLMA2_60_IN_LAST_12_MONTHS', 'LLMA2_90_IN_LAST_12_MONTHS', 'LLMA2_FC_IN_LAST_12_MONTHS', 'LLMA2_REO_IN_LAST_12_MONTHS', 'LLMA2_0_IN_LAST_12_MONTHS', 'LLMA2_HIST_LAST_12_MONTHS_MIS', 'NUM_MODIF', 'NUM_MODIF_NAN', 'P_RATE_TO_MOD', 'P_RATE_TO_MOD_NAN', 'MOD_RATE', 'MOD_RATE_NAN', 'DIF_RATE', 'DIF_RATE_NAN', 'P_MONTHLY_PAY', 'P_MONTHLY_PAY_NAN', 'MOD_MONTHLY_PAY', 'MOD_MONTHLY_PAY_NAN', 'DIF_MONTHLY_PAY', 'DIF_MONTHLY_PAY_NAN', 'CAPITALIZATION_AMT', 'CAPITALIZATION_AMT_NAN', 'MORTGAGE_RATE', 'MORTGAGE_RATE_NAN', 'FICO_SCORE_ORIGINATION', 'INITIAL_INTEREST_RATE', 'ORIGINAL_LTV', 'ORIGINAL_BALANCE', 'BACKEND_RATIO', 'BACKEND_RATIO_NAN', 'ORIGINAL_TERM', 'ORIGINAL_TERM_NAN', 'SALE_PRICE', 'SALE_PRICE_NAN', 'PREPAY_PENALTY_TERM', 'PREPAY_PENALTY_TERM_NAN', 'NUMBER_OF_UNITS', 'NUMBER_OF_UNITS_NAN', 'MARGIN', 'MARGIN_NAN', 'PERIODIC_RATE_CAP', 'PERIODIC_RATE_CAP_NAN', 'PERIODIC_RATE_FLOOR', 'PERIODIC_RATE_FLOOR_NAN', 'LIFETIME_RATE_CAP', 'LIFETIME_RATE_CAP_NAN', 'LIFETIME_RATE_FLOOR', 'LIFETIME_RATE_FLOOR_NAN', 'RATE_RESET_FREQUENCY', 'RATE_RESET_FREQUENCY_NAN', 'PAY_RESET_FREQUENCY', 'PAY_RESET_FREQUENCY_NAN', 'FIRST_RATE_RESET_PERIOD', 'FIRST_RATE_RESET_PERIOD_NAN', 'LLMA2_PRIME', 'LLMA2_SUBPRIME', 'LLMA2_APPVAL_LT_SALEPRICE', 'LLMA2_ORIG_RATE_SPREAD', 'LLMA2_ORIG_RATE_SPREAD_NAN', 'AGI', 'AGI_NAN', 'UR', 'UR_NAN', 'LLMA2_ORIG_RATE_ORIG_MR_SPREAD', 'LLMA2_ORIG_RATE_ORIG_MR_SPREAD_NAN', 'COUNT_INT_RATE_LESS', 'NUM_PRIME_ZIP', 'NUM_PRIME_ZIP_NAN', 'MBA_DELINQUENCY_STATUS_0', 'MBA_DELINQUENCY_STATUS_3', 'MBA_DELINQUENCY_STATUS_6', 'MBA_DELINQUENCY_STATUS_9', 'MBA_DELINQUENCY_STATUS_C', 'MBA_DELINQUENCY_STATUS_F', 'MBA_DELINQUENCY_STATUS_R', 'BUYDOWN_FLAG_N', 'BUYDOWN_FLAG_U', 'BUYDOWN_FLAG_Y', 'NEGATIVE_AMORTIZATION_FLAG_N', 'NEGATIVE_AMORTIZATION_FLAG_U', 'NEGATIVE_AMORTIZATION_FLAG_Y', 'PREPAY_PENALTY_FLAG_N', 'PREPAY_PENALTY_FLAG_U', 'PREPAY_PENALTY_FLAG_Y', 'OCCUPANCY_TYPE_1', 'OCCUPANCY_TYPE_2', 'OCCUPANCY_TYPE_3', 'OCCUPANCY_TYPE_U', 'PRODUCT_TYPE_10', 'PRODUCT_TYPE_20', 'PRODUCT_TYPE_30', 'PRODUCT_TYPE_40', 'PRODUCT_TYPE_50', 'PRODUCT_TYPE_51', 'PRODUCT_TYPE_52', 'PRODUCT_TYPE_53', 'PRODUCT_TYPE_54', 'PRODUCT_TYPE_5A', 'PRODUCT_TYPE_5Z', 'PRODUCT_TYPE_60', 'PRODUCT_TYPE_61', 'PRODUCT_TYPE_62', 'PRODUCT_TYPE_63', 'PRODUCT_TYPE_6Z', 'PRODUCT_TYPE_70', 'PRODUCT_TYPE_80', 'PRODUCT_TYPE_81', 'PRODUCT_TYPE_82', 'PRODUCT_TYPE_83', 'PRODUCT_TYPE_84', 'PRODUCT_TYPE_8Z', 'PRODUCT_TYPE_U', 'PROPERTY_TYPE_1', 'PROPERTY_TYPE_2', 'PROPERTY_TYPE_3', 'PROPERTY_TYPE_4', 'PROPERTY_TYPE_5', 'PROPERTY_TYPE_6', 'PROPERTY_TYPE_7', 'PROPERTY_TYPE_8', 'PROPERTY_TYPE_9', 'PROPERTY_TYPE_M', 'PROPERTY_TYPE_U', 'PROPERTY_TYPE_Z', 'LOAN_PURPOSE_CATEGORY_P', 'LOAN_PURPOSE_CATEGORY_R', 'LOAN_PURPOSE_CATEGORY_U', 'DOCUMENTATION_TYPE_1', 'DOCUMENTATION_TYPE_2', 'DOCUMENTATION_TYPE_3', 'DOCUMENTATION_TYPE_U', 'CHANNEL_1', 'CHANNEL_2', 'CHANNEL_3', 'CHANNEL_4', 'CHANNEL_5', 'CHANNEL_6', 'CHANNEL_7', 'CHANNEL_8', 'CHANNEL_9', 'CHANNEL_A', 'CHANNEL_B', 'CHANNEL_C', 'CHANNEL_D', 'CHANNEL_U', 'LOAN_TYPE_1', 'LOAN_TYPE_2', 'LOAN_TYPE_3', 'LOAN_TYPE_4', 'LOAN_TYPE_5', 'LOAN_TYPE_6', 'LOAN_TYPE_U', 'IO_FLAG_N', 'IO_FLAG_U', 'IO_FLAG_Y', 'CONVERTIBLE_FLAG_N', 'CONVERTIBLE_FLAG_U', 'CONVERTIBLE_FLAG_Y', 'POOL_INSURANCE_FLAG_N', 'POOL_INSURANCE_FLAG_U', 'POOL_INSURANCE_FLAG_Y', 'STATE_AK', 'STATE_AL', 'STATE_AR', 'STATE_AZ', 'STATE_CA', 'STATE_CO', 'STATE_CT', 'STATE_DC', 'STATE_DE', 'STATE_FL', 'STATE_GA', 'STATE_HI', 'STATE_IA', 'STATE_ID', 'STATE_IL', 'STATE_IN', 'STATE_KS', 'STATE_KY', 'STATE_LA', 'STATE_MA', 'STATE_MD', 'STATE_ME', 'STATE_MI', 'STATE_MN', 'STATE_MO', 'STATE_MS', 'STATE_MT', 'STATE_NC', 'STATE_ND', 'STATE_NE', 'STATE_NH', 'STATE_NJ', 'STATE_NM', 'STATE_NV', 'STATE_NY', 'STATE_OH', 'STATE_OK', 'STATE_OR', 'STATE_PA', 'STATE_PR', 'STATE_RI', 'STATE_SC', 'STATE_SD', 'STATE_TN', 'STATE_TX', 'STATE_UT', 'STATE_VA', 'STATE_VT', 'STATE_WA', 'STATE_WI', 'STATE_WV', 'STATE_WY', 'CURRENT_INVESTOR_CODE_240', 'CURRENT_INVESTOR_CODE_250', 'CURRENT_INVESTOR_CODE_253', 'CURRENT_INVESTOR_CODE_U', 'ORIGINATION_YEAR_B1995', 'ORIGINATION_YEAR_1995', 'ORIGINATION_YEAR_1996', 'ORIGINATION_YEAR_1997', 'ORIGINATION_YEAR_1998', 'ORIGINATION_YEAR_1999', 'ORIGINATION_YEAR_2000', 'ORIGINATION_YEAR_2001', 'ORIGINATION_YEAR_2002', 'ORIGINATION_YEAR_2003', 'ORIGINATION_YEAR_2004', 'ORIGINATION_YEAR_2005', 'ORIGINATION_YEAR_2006', 'ORIGINATION_YEAR_2007', 'ORIGINATION_YEAR_2008', 'ORIGINATION_YEAR_2009', 'ORIGINATION_YEAR_2010', 'ORIGINATION_YEAR_2011', 'ORIGINATION_YEAR_2012', 'ORIGINATION_YEAR_2013', 'ORIGINATION_YEAR_2014', 'ORIGINATION_YEAR_2015', 'ORIGINATION_YEAR_2016', 'ORIGINATION_YEAR_2017', 'ORIGINATION_YEAR_2018']\n",
      "2018-11-01 18:38:34,253 - Xworkers_1mill_0 - INFO - Features List: ['MBA_DAYS_DELINQUENT', 'MBA_DAYS_DELINQUENT_NAN', 'CURRENT_INTEREST_RATE', 'CURRENT_INTEREST_RATE_NAN', 'LOANAGE', 'LOANAGE_NAN', 'CURRENT_BALANCE', 'CURRENT_BALANCE_NAN', 'SCHEDULED_PRINCIPAL', 'SCHEDULED_PRINCIPAL_NAN', 'SCHEDULED_MONTHLY_PANDI', 'SCHEDULED_MONTHLY_PANDI_NAN', 'LLMA2_CURRENT_INTEREST_SPREAD', 'LLMA2_CURRENT_INTEREST_SPREAD_NAN', 'LLMA2_C_IN_LAST_12_MONTHS', 'LLMA2_30_IN_LAST_12_MONTHS', 'LLMA2_60_IN_LAST_12_MONTHS', 'LLMA2_90_IN_LAST_12_MONTHS', 'LLMA2_FC_IN_LAST_12_MONTHS', 'LLMA2_REO_IN_LAST_12_MONTHS', 'LLMA2_0_IN_LAST_12_MONTHS', 'LLMA2_HIST_LAST_12_MONTHS_MIS', 'NUM_MODIF', 'NUM_MODIF_NAN', 'P_RATE_TO_MOD', 'P_RATE_TO_MOD_NAN', 'MOD_RATE', 'MOD_RATE_NAN', 'DIF_RATE', 'DIF_RATE_NAN', 'P_MONTHLY_PAY', 'P_MONTHLY_PAY_NAN', 'MOD_MONTHLY_PAY', 'MOD_MONTHLY_PAY_NAN', 'DIF_MONTHLY_PAY', 'DIF_MONTHLY_PAY_NAN', 'CAPITALIZATION_AMT', 'CAPITALIZATION_AMT_NAN', 'MORTGAGE_RATE', 'MORTGAGE_RATE_NAN', 'FICO_SCORE_ORIGINATION', 'INITIAL_INTEREST_RATE', 'ORIGINAL_LTV', 'ORIGINAL_BALANCE', 'BACKEND_RATIO', 'BACKEND_RATIO_NAN', 'ORIGINAL_TERM', 'ORIGINAL_TERM_NAN', 'SALE_PRICE', 'SALE_PRICE_NAN', 'PREPAY_PENALTY_TERM', 'PREPAY_PENALTY_TERM_NAN', 'NUMBER_OF_UNITS', 'NUMBER_OF_UNITS_NAN', 'MARGIN', 'MARGIN_NAN', 'PERIODIC_RATE_CAP', 'PERIODIC_RATE_CAP_NAN', 'PERIODIC_RATE_FLOOR', 'PERIODIC_RATE_FLOOR_NAN', 'LIFETIME_RATE_CAP', 'LIFETIME_RATE_CAP_NAN', 'LIFETIME_RATE_FLOOR', 'LIFETIME_RATE_FLOOR_NAN', 'RATE_RESET_FREQUENCY', 'RATE_RESET_FREQUENCY_NAN', 'PAY_RESET_FREQUENCY', 'PAY_RESET_FREQUENCY_NAN', 'FIRST_RATE_RESET_PERIOD', 'FIRST_RATE_RESET_PERIOD_NAN', 'LLMA2_PRIME', 'LLMA2_SUBPRIME', 'LLMA2_APPVAL_LT_SALEPRICE', 'LLMA2_ORIG_RATE_SPREAD', 'LLMA2_ORIG_RATE_SPREAD_NAN', 'AGI', 'AGI_NAN', 'UR', 'UR_NAN', 'LLMA2_ORIG_RATE_ORIG_MR_SPREAD', 'LLMA2_ORIG_RATE_ORIG_MR_SPREAD_NAN', 'COUNT_INT_RATE_LESS', 'NUM_PRIME_ZIP', 'NUM_PRIME_ZIP_NAN', 'MBA_DELINQUENCY_STATUS_0', 'MBA_DELINQUENCY_STATUS_3', 'MBA_DELINQUENCY_STATUS_6', 'MBA_DELINQUENCY_STATUS_9', 'MBA_DELINQUENCY_STATUS_C', 'MBA_DELINQUENCY_STATUS_F', 'MBA_DELINQUENCY_STATUS_R', 'BUYDOWN_FLAG_N', 'BUYDOWN_FLAG_U', 'BUYDOWN_FLAG_Y', 'NEGATIVE_AMORTIZATION_FLAG_N', 'NEGATIVE_AMORTIZATION_FLAG_U', 'NEGATIVE_AMORTIZATION_FLAG_Y', 'PREPAY_PENALTY_FLAG_N', 'PREPAY_PENALTY_FLAG_U', 'PREPAY_PENALTY_FLAG_Y', 'OCCUPANCY_TYPE_1', 'OCCUPANCY_TYPE_2', 'OCCUPANCY_TYPE_3', 'OCCUPANCY_TYPE_U', 'PRODUCT_TYPE_10', 'PRODUCT_TYPE_20', 'PRODUCT_TYPE_30', 'PRODUCT_TYPE_40', 'PRODUCT_TYPE_50', 'PRODUCT_TYPE_51', 'PRODUCT_TYPE_52', 'PRODUCT_TYPE_53', 'PRODUCT_TYPE_54', 'PRODUCT_TYPE_5A', 'PRODUCT_TYPE_5Z', 'PRODUCT_TYPE_60', 'PRODUCT_TYPE_61', 'PRODUCT_TYPE_62', 'PRODUCT_TYPE_63', 'PRODUCT_TYPE_6Z', 'PRODUCT_TYPE_70', 'PRODUCT_TYPE_80', 'PRODUCT_TYPE_81', 'PRODUCT_TYPE_82', 'PRODUCT_TYPE_83', 'PRODUCT_TYPE_84', 'PRODUCT_TYPE_8Z', 'PRODUCT_TYPE_U', 'PROPERTY_TYPE_1', 'PROPERTY_TYPE_2', 'PROPERTY_TYPE_3', 'PROPERTY_TYPE_4', 'PROPERTY_TYPE_5', 'PROPERTY_TYPE_6', 'PROPERTY_TYPE_7', 'PROPERTY_TYPE_8', 'PROPERTY_TYPE_9', 'PROPERTY_TYPE_M', 'PROPERTY_TYPE_U', 'PROPERTY_TYPE_Z', 'LOAN_PURPOSE_CATEGORY_P', 'LOAN_PURPOSE_CATEGORY_R', 'LOAN_PURPOSE_CATEGORY_U', 'DOCUMENTATION_TYPE_1', 'DOCUMENTATION_TYPE_2', 'DOCUMENTATION_TYPE_3', 'DOCUMENTATION_TYPE_U', 'CHANNEL_1', 'CHANNEL_2', 'CHANNEL_3', 'CHANNEL_4', 'CHANNEL_5', 'CHANNEL_6', 'CHANNEL_7', 'CHANNEL_8', 'CHANNEL_9', 'CHANNEL_A', 'CHANNEL_B', 'CHANNEL_C', 'CHANNEL_D', 'CHANNEL_U', 'LOAN_TYPE_1', 'LOAN_TYPE_2', 'LOAN_TYPE_3', 'LOAN_TYPE_4', 'LOAN_TYPE_5', 'LOAN_TYPE_6', 'LOAN_TYPE_U', 'IO_FLAG_N', 'IO_FLAG_U', 'IO_FLAG_Y', 'CONVERTIBLE_FLAG_N', 'CONVERTIBLE_FLAG_U', 'CONVERTIBLE_FLAG_Y', 'POOL_INSURANCE_FLAG_N', 'POOL_INSURANCE_FLAG_U', 'POOL_INSURANCE_FLAG_Y', 'STATE_AK', 'STATE_AL', 'STATE_AR', 'STATE_AZ', 'STATE_CA', 'STATE_CO', 'STATE_CT', 'STATE_DC', 'STATE_DE', 'STATE_FL', 'STATE_GA', 'STATE_HI', 'STATE_IA', 'STATE_ID', 'STATE_IL', 'STATE_IN', 'STATE_KS', 'STATE_KY', 'STATE_LA', 'STATE_MA', 'STATE_MD', 'STATE_ME', 'STATE_MI', 'STATE_MN', 'STATE_MO', 'STATE_MS', 'STATE_MT', 'STATE_NC', 'STATE_ND', 'STATE_NE', 'STATE_NH', 'STATE_NJ', 'STATE_NM', 'STATE_NV', 'STATE_NY', 'STATE_OH', 'STATE_OK', 'STATE_OR', 'STATE_PA', 'STATE_PR', 'STATE_RI', 'STATE_SC', 'STATE_SD', 'STATE_TN', 'STATE_TX', 'STATE_UT', 'STATE_VA', 'STATE_VT', 'STATE_WA', 'STATE_WI', 'STATE_WV', 'STATE_WY', 'CURRENT_INVESTOR_CODE_240', 'CURRENT_INVESTOR_CODE_250', 'CURRENT_INVESTOR_CODE_253', 'CURRENT_INVESTOR_CODE_U', 'ORIGINATION_YEAR_B1995', 'ORIGINATION_YEAR_1995', 'ORIGINATION_YEAR_1996', 'ORIGINATION_YEAR_1997', 'ORIGINATION_YEAR_1998', 'ORIGINATION_YEAR_1999', 'ORIGINATION_YEAR_2000', 'ORIGINATION_YEAR_2001', 'ORIGINATION_YEAR_2002', 'ORIGINATION_YEAR_2003', 'ORIGINATION_YEAR_2004', 'ORIGINATION_YEAR_2005', 'ORIGINATION_YEAR_2006', 'ORIGINATION_YEAR_2007', 'ORIGINATION_YEAR_2008', 'ORIGINATION_YEAR_2009', 'ORIGINATION_YEAR_2010', 'ORIGINATION_YEAR_2011', 'ORIGINATION_YEAR_2012', 'ORIGINATION_YEAR_2013', 'ORIGINATION_YEAR_2014', 'ORIGINATION_YEAR_2015', 'ORIGINATION_YEAR_2016', 'ORIGINATION_YEAR_2017', 'ORIGINATION_YEAR_2018']\n",
      "2018-11-01 18:38:34,253 - Xworkers_1mill_0 - INFO - Labels List: ['DELINQUENCY_STATUS_NEXT_0', 'DELINQUENCY_STATUS_NEXT_3', 'DELINQUENCY_STATUS_NEXT_6', 'DELINQUENCY_STATUS_NEXT_9', 'DELINQUENCY_STATUS_NEXT_C', 'DELINQUENCY_STATUS_NEXT_F', 'DELINQUENCY_STATUS_NEXT_R']\n",
      "2018-11-01 18:38:34,253 - Xworkers_1mill_0 - INFO - Labels List: ['DELINQUENCY_STATUS_NEXT_0', 'DELINQUENCY_STATUS_NEXT_3', 'DELINQUENCY_STATUS_NEXT_6', 'DELINQUENCY_STATUS_NEXT_9', 'DELINQUENCY_STATUS_NEXT_C', 'DELINQUENCY_STATUS_NEXT_F', 'DELINQUENCY_STATUS_NEXT_R']\n",
      "19435138\n",
      "38500000\n",
      "2018-11-01 18:38:34,253 - Xworkers_1mill_0 - INFO - METRICS:  Namespace(allow_summaries=True, batch_layer_type='batch', batch_norm=True, batch_size=8850, batch_type='layer', data_dir='/input_data', decay_rate=1, decay_step=3520000, display_every=100, dropout=True, dropout_keep=0.9, epoch_flag=0, epoch_num=15, eval=False, ftp_dir='processed/c1mill', learning_rate=0.1, log_file=None, logdir=PosixPath('/home/ubuntu/summ_15ep_2wrk'), loss_tolerance=0.0001, lr_decay_epochs=30, lr_decay_policy='time', lr_decay_rate=0.1, lr_poly_power=2.0, max_epoch_size=-1, momentum=0.5, n_hidden=3, name='Xworkers_1mill', nstep_burnin=20, rate_min=0.0015, reg_rate=1e-05, s_hidden=[200, 140, 140], save_interval=450, stratified_flag=False, summary_interval=1800, test_batch_size=1200000, test_dir='chuncks_random_c1millx2_test', test_flag=True, test_period=[286, 304], total_examples=38500000, train_dir='chuncks_random_c1millx2_train', train_period=[121, 279], valid_batch_size=150000, valid_dir='chuncks_random_c1millx2_valid', valid_period=[280, 285], weighted_sampling=False, xla=True)\n",
      "\n",
      "2018-11-01 18:38:34,253 - Xworkers_1mill_0 - INFO - METRICS:  Namespace(allow_summaries=True, batch_layer_type='batch', batch_norm=True, batch_size=8850, batch_type='layer', data_dir='/input_data', decay_rate=1, decay_step=3520000, display_every=100, dropout=True, dropout_keep=0.9, epoch_flag=0, epoch_num=15, eval=False, ftp_dir='processed/c1mill', learning_rate=0.1, log_file=None, logdir=PosixPath('/home/ubuntu/summ_15ep_2wrk'), loss_tolerance=0.0001, lr_decay_epochs=30, lr_decay_policy='time', lr_decay_rate=0.1, lr_poly_power=2.0, max_epoch_size=-1, momentum=0.5, n_hidden=3, name='Xworkers_1mill', nstep_burnin=20, rate_min=0.0015, reg_rate=1e-05, s_hidden=[200, 140, 140], save_interval=450, stratified_flag=False, summary_interval=1800, test_batch_size=1200000, test_dir='chuncks_random_c1millx2_test', test_flag=True, test_period=[286, 304], total_examples=38500000, train_dir='chuncks_random_c1millx2_train', train_period=[121, 279], valid_batch_size=150000, valid_dir='chuncks_random_c1millx2_valid', valid_period=[280, 285], weighted_sampling=False, xla=True)\n",
      "\n",
      "2018-11-01 18:38:34,254 - Xworkers_1mill_0 - INFO - Number of total steps: 65255\n",
      "2018-11-01 18:38:34,254 - Xworkers_1mill_0 - INFO - Number of total steps: 65255\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/MLMortgage/data/processed/chuncks_random_c1millx2_train/temporalloandynmodifmrstaticitur6mill-7mill_non_index-train_3.h5  loaded in RAM\n",
      "2018-11-01 18:39:02,177 - Xworkers_1mill_2 - INFO - Features List: ['MBA_DAYS_DELINQUENT', 'MBA_DAYS_DELINQUENT_NAN', 'CURRENT_INTEREST_RATE', 'CURRENT_INTEREST_RATE_NAN', 'LOANAGE', 'LOANAGE_NAN', 'CURRENT_BALANCE', 'CURRENT_BALANCE_NAN', 'SCHEDULED_PRINCIPAL', 'SCHEDULED_PRINCIPAL_NAN', 'SCHEDULED_MONTHLY_PANDI', 'SCHEDULED_MONTHLY_PANDI_NAN', 'LLMA2_CURRENT_INTEREST_SPREAD', 'LLMA2_CURRENT_INTEREST_SPREAD_NAN', 'LLMA2_C_IN_LAST_12_MONTHS', 'LLMA2_30_IN_LAST_12_MONTHS', 'LLMA2_60_IN_LAST_12_MONTHS', 'LLMA2_90_IN_LAST_12_MONTHS', 'LLMA2_FC_IN_LAST_12_MONTHS', 'LLMA2_REO_IN_LAST_12_MONTHS', 'LLMA2_0_IN_LAST_12_MONTHS', 'LLMA2_HIST_LAST_12_MONTHS_MIS', 'NUM_MODIF', 'NUM_MODIF_NAN', 'P_RATE_TO_MOD', 'P_RATE_TO_MOD_NAN', 'MOD_RATE', 'MOD_RATE_NAN', 'DIF_RATE', 'DIF_RATE_NAN', 'P_MONTHLY_PAY', 'P_MONTHLY_PAY_NAN', 'MOD_MONTHLY_PAY', 'MOD_MONTHLY_PAY_NAN', 'DIF_MONTHLY_PAY', 'DIF_MONTHLY_PAY_NAN', 'CAPITALIZATION_AMT', 'CAPITALIZATION_AMT_NAN', 'MORTGAGE_RATE', 'MORTGAGE_RATE_NAN', 'FICO_SCORE_ORIGINATION', 'INITIAL_INTEREST_RATE', 'ORIGINAL_LTV', 'ORIGINAL_BALANCE', 'BACKEND_RATIO', 'BACKEND_RATIO_NAN', 'ORIGINAL_TERM', 'ORIGINAL_TERM_NAN', 'SALE_PRICE', 'SALE_PRICE_NAN', 'PREPAY_PENALTY_TERM', 'PREPAY_PENALTY_TERM_NAN', 'NUMBER_OF_UNITS', 'NUMBER_OF_UNITS_NAN', 'MARGIN', 'MARGIN_NAN', 'PERIODIC_RATE_CAP', 'PERIODIC_RATE_CAP_NAN', 'PERIODIC_RATE_FLOOR', 'PERIODIC_RATE_FLOOR_NAN', 'LIFETIME_RATE_CAP', 'LIFETIME_RATE_CAP_NAN', 'LIFETIME_RATE_FLOOR', 'LIFETIME_RATE_FLOOR_NAN', 'RATE_RESET_FREQUENCY', 'RATE_RESET_FREQUENCY_NAN', 'PAY_RESET_FREQUENCY', 'PAY_RESET_FREQUENCY_NAN', 'FIRST_RATE_RESET_PERIOD', 'FIRST_RATE_RESET_PERIOD_NAN', 'LLMA2_PRIME', 'LLMA2_SUBPRIME', 'LLMA2_APPVAL_LT_SALEPRICE', 'LLMA2_ORIG_RATE_SPREAD', 'LLMA2_ORIG_RATE_SPREAD_NAN', 'AGI', 'AGI_NAN', 'UR', 'UR_NAN', 'LLMA2_ORIG_RATE_ORIG_MR_SPREAD', 'LLMA2_ORIG_RATE_ORIG_MR_SPREAD_NAN', 'COUNT_INT_RATE_LESS', 'NUM_PRIME_ZIP', 'NUM_PRIME_ZIP_NAN', 'MBA_DELINQUENCY_STATUS_0', 'MBA_DELINQUENCY_STATUS_3', 'MBA_DELINQUENCY_STATUS_6', 'MBA_DELINQUENCY_STATUS_9', 'MBA_DELINQUENCY_STATUS_C', 'MBA_DELINQUENCY_STATUS_F', 'MBA_DELINQUENCY_STATUS_R', 'BUYDOWN_FLAG_N', 'BUYDOWN_FLAG_U', 'BUYDOWN_FLAG_Y', 'NEGATIVE_AMORTIZATION_FLAG_N', 'NEGATIVE_AMORTIZATION_FLAG_U', 'NEGATIVE_AMORTIZATION_FLAG_Y', 'PREPAY_PENALTY_FLAG_N', 'PREPAY_PENALTY_FLAG_U', 'PREPAY_PENALTY_FLAG_Y', 'OCCUPANCY_TYPE_1', 'OCCUPANCY_TYPE_2', 'OCCUPANCY_TYPE_3', 'OCCUPANCY_TYPE_U', 'PRODUCT_TYPE_10', 'PRODUCT_TYPE_20', 'PRODUCT_TYPE_30', 'PRODUCT_TYPE_40', 'PRODUCT_TYPE_50', 'PRODUCT_TYPE_51', 'PRODUCT_TYPE_52', 'PRODUCT_TYPE_53', 'PRODUCT_TYPE_54', 'PRODUCT_TYPE_5A', 'PRODUCT_TYPE_5Z', 'PRODUCT_TYPE_60', 'PRODUCT_TYPE_61', 'PRODUCT_TYPE_62', 'PRODUCT_TYPE_63', 'PRODUCT_TYPE_6Z', 'PRODUCT_TYPE_70', 'PRODUCT_TYPE_80', 'PRODUCT_TYPE_81', 'PRODUCT_TYPE_82', 'PRODUCT_TYPE_83', 'PRODUCT_TYPE_84', 'PRODUCT_TYPE_8Z', 'PRODUCT_TYPE_U', 'PROPERTY_TYPE_1', 'PROPERTY_TYPE_2', 'PROPERTY_TYPE_3', 'PROPERTY_TYPE_4', 'PROPERTY_TYPE_5', 'PROPERTY_TYPE_6', 'PROPERTY_TYPE_7', 'PROPERTY_TYPE_8', 'PROPERTY_TYPE_9', 'PROPERTY_TYPE_M', 'PROPERTY_TYPE_U', 'PROPERTY_TYPE_Z', 'LOAN_PURPOSE_CATEGORY_P', 'LOAN_PURPOSE_CATEGORY_R', 'LOAN_PURPOSE_CATEGORY_U', 'DOCUMENTATION_TYPE_1', 'DOCUMENTATION_TYPE_2', 'DOCUMENTATION_TYPE_3', 'DOCUMENTATION_TYPE_U', 'CHANNEL_1', 'CHANNEL_2', 'CHANNEL_3', 'CHANNEL_4', 'CHANNEL_5', 'CHANNEL_6', 'CHANNEL_7', 'CHANNEL_8', 'CHANNEL_9', 'CHANNEL_A', 'CHANNEL_B', 'CHANNEL_C', 'CHANNEL_D', 'CHANNEL_U', 'LOAN_TYPE_1', 'LOAN_TYPE_2', 'LOAN_TYPE_3', 'LOAN_TYPE_4', 'LOAN_TYPE_5', 'LOAN_TYPE_6', 'LOAN_TYPE_U', 'IO_FLAG_N', 'IO_FLAG_U', 'IO_FLAG_Y', 'CONVERTIBLE_FLAG_N', 'CONVERTIBLE_FLAG_U', 'CONVERTIBLE_FLAG_Y', 'POOL_INSURANCE_FLAG_N', 'POOL_INSURANCE_FLAG_U', 'POOL_INSURANCE_FLAG_Y', 'STATE_AK', 'STATE_AL', 'STATE_AR', 'STATE_AZ', 'STATE_CA', 'STATE_CO', 'STATE_CT', 'STATE_DC', 'STATE_DE', 'STATE_FL', 'STATE_GA', 'STATE_HI', 'STATE_IA', 'STATE_ID', 'STATE_IL', 'STATE_IN', 'STATE_KS', 'STATE_KY', 'STATE_LA', 'STATE_MA', 'STATE_MD', 'STATE_ME', 'STATE_MI', 'STATE_MN', 'STATE_MO', 'STATE_MS', 'STATE_MT', 'STATE_NC', 'STATE_ND', 'STATE_NE', 'STATE_NH', 'STATE_NJ', 'STATE_NM', 'STATE_NV', 'STATE_NY', 'STATE_OH', 'STATE_OK', 'STATE_OR', 'STATE_PA', 'STATE_PR', 'STATE_RI', 'STATE_SC', 'STATE_SD', 'STATE_TN', 'STATE_TX', 'STATE_UT', 'STATE_VA', 'STATE_VT', 'STATE_WA', 'STATE_WI', 'STATE_WV', 'STATE_WY', 'CURRENT_INVESTOR_CODE_240', 'CURRENT_INVESTOR_CODE_250', 'CURRENT_INVESTOR_CODE_253', 'CURRENT_INVESTOR_CODE_U', 'ORIGINATION_YEAR_B1995', 'ORIGINATION_YEAR_1995', 'ORIGINATION_YEAR_1996', 'ORIGINATION_YEAR_1997', 'ORIGINATION_YEAR_1998', 'ORIGINATION_YEAR_1999', 'ORIGINATION_YEAR_2000', 'ORIGINATION_YEAR_2001', 'ORIGINATION_YEAR_2002', 'ORIGINATION_YEAR_2003', 'ORIGINATION_YEAR_2004', 'ORIGINATION_YEAR_2005', 'ORIGINATION_YEAR_2006', 'ORIGINATION_YEAR_2007', 'ORIGINATION_YEAR_2008', 'ORIGINATION_YEAR_2009', 'ORIGINATION_YEAR_2010', 'ORIGINATION_YEAR_2011', 'ORIGINATION_YEAR_2012', 'ORIGINATION_YEAR_2013', 'ORIGINATION_YEAR_2014', 'ORIGINATION_YEAR_2015', 'ORIGINATION_YEAR_2016', 'ORIGINATION_YEAR_2017', 'ORIGINATION_YEAR_2018']\n",
      "2018-11-01 18:39:02,177 - Xworkers_1mill_2 - INFO - Features List: ['MBA_DAYS_DELINQUENT', 'MBA_DAYS_DELINQUENT_NAN', 'CURRENT_INTEREST_RATE', 'CURRENT_INTEREST_RATE_NAN', 'LOANAGE', 'LOANAGE_NAN', 'CURRENT_BALANCE', 'CURRENT_BALANCE_NAN', 'SCHEDULED_PRINCIPAL', 'SCHEDULED_PRINCIPAL_NAN', 'SCHEDULED_MONTHLY_PANDI', 'SCHEDULED_MONTHLY_PANDI_NAN', 'LLMA2_CURRENT_INTEREST_SPREAD', 'LLMA2_CURRENT_INTEREST_SPREAD_NAN', 'LLMA2_C_IN_LAST_12_MONTHS', 'LLMA2_30_IN_LAST_12_MONTHS', 'LLMA2_60_IN_LAST_12_MONTHS', 'LLMA2_90_IN_LAST_12_MONTHS', 'LLMA2_FC_IN_LAST_12_MONTHS', 'LLMA2_REO_IN_LAST_12_MONTHS', 'LLMA2_0_IN_LAST_12_MONTHS', 'LLMA2_HIST_LAST_12_MONTHS_MIS', 'NUM_MODIF', 'NUM_MODIF_NAN', 'P_RATE_TO_MOD', 'P_RATE_TO_MOD_NAN', 'MOD_RATE', 'MOD_RATE_NAN', 'DIF_RATE', 'DIF_RATE_NAN', 'P_MONTHLY_PAY', 'P_MONTHLY_PAY_NAN', 'MOD_MONTHLY_PAY', 'MOD_MONTHLY_PAY_NAN', 'DIF_MONTHLY_PAY', 'DIF_MONTHLY_PAY_NAN', 'CAPITALIZATION_AMT', 'CAPITALIZATION_AMT_NAN', 'MORTGAGE_RATE', 'MORTGAGE_RATE_NAN', 'FICO_SCORE_ORIGINATION', 'INITIAL_INTEREST_RATE', 'ORIGINAL_LTV', 'ORIGINAL_BALANCE', 'BACKEND_RATIO', 'BACKEND_RATIO_NAN', 'ORIGINAL_TERM', 'ORIGINAL_TERM_NAN', 'SALE_PRICE', 'SALE_PRICE_NAN', 'PREPAY_PENALTY_TERM', 'PREPAY_PENALTY_TERM_NAN', 'NUMBER_OF_UNITS', 'NUMBER_OF_UNITS_NAN', 'MARGIN', 'MARGIN_NAN', 'PERIODIC_RATE_CAP', 'PERIODIC_RATE_CAP_NAN', 'PERIODIC_RATE_FLOOR', 'PERIODIC_RATE_FLOOR_NAN', 'LIFETIME_RATE_CAP', 'LIFETIME_RATE_CAP_NAN', 'LIFETIME_RATE_FLOOR', 'LIFETIME_RATE_FLOOR_NAN', 'RATE_RESET_FREQUENCY', 'RATE_RESET_FREQUENCY_NAN', 'PAY_RESET_FREQUENCY', 'PAY_RESET_FREQUENCY_NAN', 'FIRST_RATE_RESET_PERIOD', 'FIRST_RATE_RESET_PERIOD_NAN', 'LLMA2_PRIME', 'LLMA2_SUBPRIME', 'LLMA2_APPVAL_LT_SALEPRICE', 'LLMA2_ORIG_RATE_SPREAD', 'LLMA2_ORIG_RATE_SPREAD_NAN', 'AGI', 'AGI_NAN', 'UR', 'UR_NAN', 'LLMA2_ORIG_RATE_ORIG_MR_SPREAD', 'LLMA2_ORIG_RATE_ORIG_MR_SPREAD_NAN', 'COUNT_INT_RATE_LESS', 'NUM_PRIME_ZIP', 'NUM_PRIME_ZIP_NAN', 'MBA_DELINQUENCY_STATUS_0', 'MBA_DELINQUENCY_STATUS_3', 'MBA_DELINQUENCY_STATUS_6', 'MBA_DELINQUENCY_STATUS_9', 'MBA_DELINQUENCY_STATUS_C', 'MBA_DELINQUENCY_STATUS_F', 'MBA_DELINQUENCY_STATUS_R', 'BUYDOWN_FLAG_N', 'BUYDOWN_FLAG_U', 'BUYDOWN_FLAG_Y', 'NEGATIVE_AMORTIZATION_FLAG_N', 'NEGATIVE_AMORTIZATION_FLAG_U', 'NEGATIVE_AMORTIZATION_FLAG_Y', 'PREPAY_PENALTY_FLAG_N', 'PREPAY_PENALTY_FLAG_U', 'PREPAY_PENALTY_FLAG_Y', 'OCCUPANCY_TYPE_1', 'OCCUPANCY_TYPE_2', 'OCCUPANCY_TYPE_3', 'OCCUPANCY_TYPE_U', 'PRODUCT_TYPE_10', 'PRODUCT_TYPE_20', 'PRODUCT_TYPE_30', 'PRODUCT_TYPE_40', 'PRODUCT_TYPE_50', 'PRODUCT_TYPE_51', 'PRODUCT_TYPE_52', 'PRODUCT_TYPE_53', 'PRODUCT_TYPE_54', 'PRODUCT_TYPE_5A', 'PRODUCT_TYPE_5Z', 'PRODUCT_TYPE_60', 'PRODUCT_TYPE_61', 'PRODUCT_TYPE_62', 'PRODUCT_TYPE_63', 'PRODUCT_TYPE_6Z', 'PRODUCT_TYPE_70', 'PRODUCT_TYPE_80', 'PRODUCT_TYPE_81', 'PRODUCT_TYPE_82', 'PRODUCT_TYPE_83', 'PRODUCT_TYPE_84', 'PRODUCT_TYPE_8Z', 'PRODUCT_TYPE_U', 'PROPERTY_TYPE_1', 'PROPERTY_TYPE_2', 'PROPERTY_TYPE_3', 'PROPERTY_TYPE_4', 'PROPERTY_TYPE_5', 'PROPERTY_TYPE_6', 'PROPERTY_TYPE_7', 'PROPERTY_TYPE_8', 'PROPERTY_TYPE_9', 'PROPERTY_TYPE_M', 'PROPERTY_TYPE_U', 'PROPERTY_TYPE_Z', 'LOAN_PURPOSE_CATEGORY_P', 'LOAN_PURPOSE_CATEGORY_R', 'LOAN_PURPOSE_CATEGORY_U', 'DOCUMENTATION_TYPE_1', 'DOCUMENTATION_TYPE_2', 'DOCUMENTATION_TYPE_3', 'DOCUMENTATION_TYPE_U', 'CHANNEL_1', 'CHANNEL_2', 'CHANNEL_3', 'CHANNEL_4', 'CHANNEL_5', 'CHANNEL_6', 'CHANNEL_7', 'CHANNEL_8', 'CHANNEL_9', 'CHANNEL_A', 'CHANNEL_B', 'CHANNEL_C', 'CHANNEL_D', 'CHANNEL_U', 'LOAN_TYPE_1', 'LOAN_TYPE_2', 'LOAN_TYPE_3', 'LOAN_TYPE_4', 'LOAN_TYPE_5', 'LOAN_TYPE_6', 'LOAN_TYPE_U', 'IO_FLAG_N', 'IO_FLAG_U', 'IO_FLAG_Y', 'CONVERTIBLE_FLAG_N', 'CONVERTIBLE_FLAG_U', 'CONVERTIBLE_FLAG_Y', 'POOL_INSURANCE_FLAG_N', 'POOL_INSURANCE_FLAG_U', 'POOL_INSURANCE_FLAG_Y', 'STATE_AK', 'STATE_AL', 'STATE_AR', 'STATE_AZ', 'STATE_CA', 'STATE_CO', 'STATE_CT', 'STATE_DC', 'STATE_DE', 'STATE_FL', 'STATE_GA', 'STATE_HI', 'STATE_IA', 'STATE_ID', 'STATE_IL', 'STATE_IN', 'STATE_KS', 'STATE_KY', 'STATE_LA', 'STATE_MA', 'STATE_MD', 'STATE_ME', 'STATE_MI', 'STATE_MN', 'STATE_MO', 'STATE_MS', 'STATE_MT', 'STATE_NC', 'STATE_ND', 'STATE_NE', 'STATE_NH', 'STATE_NJ', 'STATE_NM', 'STATE_NV', 'STATE_NY', 'STATE_OH', 'STATE_OK', 'STATE_OR', 'STATE_PA', 'STATE_PR', 'STATE_RI', 'STATE_SC', 'STATE_SD', 'STATE_TN', 'STATE_TX', 'STATE_UT', 'STATE_VA', 'STATE_VT', 'STATE_WA', 'STATE_WI', 'STATE_WV', 'STATE_WY', 'CURRENT_INVESTOR_CODE_240', 'CURRENT_INVESTOR_CODE_250', 'CURRENT_INVESTOR_CODE_253', 'CURRENT_INVESTOR_CODE_U', 'ORIGINATION_YEAR_B1995', 'ORIGINATION_YEAR_1995', 'ORIGINATION_YEAR_1996', 'ORIGINATION_YEAR_1997', 'ORIGINATION_YEAR_1998', 'ORIGINATION_YEAR_1999', 'ORIGINATION_YEAR_2000', 'ORIGINATION_YEAR_2001', 'ORIGINATION_YEAR_2002', 'ORIGINATION_YEAR_2003', 'ORIGINATION_YEAR_2004', 'ORIGINATION_YEAR_2005', 'ORIGINATION_YEAR_2006', 'ORIGINATION_YEAR_2007', 'ORIGINATION_YEAR_2008', 'ORIGINATION_YEAR_2009', 'ORIGINATION_YEAR_2010', 'ORIGINATION_YEAR_2011', 'ORIGINATION_YEAR_2012', 'ORIGINATION_YEAR_2013', 'ORIGINATION_YEAR_2014', 'ORIGINATION_YEAR_2015', 'ORIGINATION_YEAR_2016', 'ORIGINATION_YEAR_2017', 'ORIGINATION_YEAR_2018']\n",
      "2018-11-01 18:39:02,177 - Xworkers_1mill_2 - INFO - Labels List: ['DELINQUENCY_STATUS_NEXT_0', 'DELINQUENCY_STATUS_NEXT_3', 'DELINQUENCY_STATUS_NEXT_6', 'DELINQUENCY_STATUS_NEXT_9', 'DELINQUENCY_STATUS_NEXT_C', 'DELINQUENCY_STATUS_NEXT_F', 'DELINQUENCY_STATUS_NEXT_R']\n",
      "38500000\n",
      "38500000\n",
      "2018-11-01 18:39:02,177 - Xworkers_1mill_2 - INFO - Labels List: ['DELINQUENCY_STATUS_NEXT_0', 'DELINQUENCY_STATUS_NEXT_3', 'DELINQUENCY_STATUS_NEXT_6', 'DELINQUENCY_STATUS_NEXT_9', 'DELINQUENCY_STATUS_NEXT_C', 'DELINQUENCY_STATUS_NEXT_F', 'DELINQUENCY_STATUS_NEXT_R']\n",
      "2018-11-01 18:39:02,177 - Xworkers_1mill_2 - INFO - METRICS:  Namespace(allow_summaries=True, batch_layer_type='batch', batch_norm=True, batch_size=8850, batch_type='layer', data_dir='/input_data', decay_rate=1, decay_step=3520000, display_every=100, dropout=True, dropout_keep=0.9, epoch_flag=0, epoch_num=15, eval=False, ftp_dir='processed/c1mill', learning_rate=0.1, log_file=None, logdir=PosixPath('/home/ubuntu/summ_15ep_2wrk'), loss_tolerance=0.0001, lr_decay_epochs=30, lr_decay_policy='time', lr_decay_rate=0.1, lr_poly_power=2.0, max_epoch_size=-1, momentum=0.5, n_hidden=3, name='Xworkers_1mill', nstep_burnin=20, rate_min=0.0015, reg_rate=1e-05, s_hidden=[200, 140, 140], save_interval=450, stratified_flag=False, summary_interval=1800, test_batch_size=1200000, test_dir='chuncks_random_c1millx2_test', test_flag=True, test_period=[286, 304], total_examples=38500000, train_dir='chuncks_random_c1millx2_train', train_period=[121, 279], valid_batch_size=150000, valid_dir='chuncks_random_c1millx2_valid', valid_period=[280, 285], weighted_sampling=False, xla=True)\n",
      "\n",
      "2018-11-01 18:39:02,177 - Xworkers_1mill_2 - INFO - METRICS:  Namespace(allow_summaries=True, batch_layer_type='batch', batch_norm=True, batch_size=8850, batch_type='layer', data_dir='/input_data', decay_rate=1, decay_step=3520000, display_every=100, dropout=True, dropout_keep=0.9, epoch_flag=0, epoch_num=15, eval=False, ftp_dir='processed/c1mill', learning_rate=0.1, log_file=None, logdir=PosixPath('/home/ubuntu/summ_15ep_2wrk'), loss_tolerance=0.0001, lr_decay_epochs=30, lr_decay_policy='time', lr_decay_rate=0.1, lr_poly_power=2.0, max_epoch_size=-1, momentum=0.5, n_hidden=3, name='Xworkers_1mill', nstep_burnin=20, rate_min=0.0015, reg_rate=1e-05, s_hidden=[200, 140, 140], save_interval=450, stratified_flag=False, summary_interval=1800, test_batch_size=1200000, test_dir='chuncks_random_c1millx2_test', test_flag=True, test_period=[286, 304], total_examples=38500000, train_dir='chuncks_random_c1millx2_train', train_period=[121, 279], valid_batch_size=150000, valid_dir='chuncks_random_c1millx2_valid', valid_period=[280, 285], weighted_sampling=False, xla=True)\n",
      "\n",
      "2018-11-01 18:39:02,178 - Xworkers_1mill_2 - INFO - Number of total steps: 65255\n",
      "2018-11-01 18:39:02,178 - Xworkers_1mill_2 - INFO - Number of total steps: 65255\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/MLMortgage/data/processed/chuncks_random_c1millx2_train/temporalloandynmodifmrstaticitur3mill-4mill_non_index-train_1.h5  loaded in RAM\r\n",
      "2018-11-01 18:39:04,894 - Xworkers_1mill_1 - INFO - Features List: ['MBA_DAYS_DELINQUENT', 'MBA_DAYS_DELINQUENT_NAN', 'CURRENT_INTEREST_RATE', 'CURRENT_INTEREST_RATE_NAN', 'LOANAGE', 'LOANAGE_NAN', 'CURRENT_BALANCE', 'CURRENT_BALANCE_NAN', 'SCHEDULED_PRINCIPAL', 'SCHEDULED_PRINCIPAL_NAN', 'SCHEDULED_MONTHLY_PANDI', 'SCHEDULED_MONTHLY_PANDI_NAN', 'LLMA2_CURRENT_INTEREST_SPREAD', 'LLMA2_CURRENT_INTEREST_SPREAD_NAN', 'LLMA2_C_IN_LAST_12_MONTHS', 'LLMA2_30_IN_LAST_12_MONTHS', 'LLMA2_60_IN_LAST_12_MONTHS', 'LLMA2_90_IN_LAST_12_MONTHS', 'LLMA2_FC_IN_LAST_12_MONTHS', 'LLMA2_REO_IN_LAST_12_MONTHS', 'LLMA2_0_IN_LAST_12_MONTHS', 'LLMA2_HIST_LAST_12_MONTHS_MIS', 'NUM_MODIF', 'NUM_MODIF_NAN', 'P_RATE_TO_MOD', 'P_RATE_TO_MOD_NAN', 'MOD_RATE', 'MOD_RATE_NAN', 'DIF_RATE', 'DIF_RATE_NAN', 'P_MONTHLY_PAY', 'P_MONTHLY_PAY_NAN', 'MOD_MONTHLY_PAY', 'MOD_MONTHLY_PAY_NAN', 'DIF_MONTHLY_PAY', 'DIF_MONTHLY_PAY_NAN', 'CAPITALIZATION_AMT', 'CAPITALIZATION_AMT_NAN', 'MORTGAGE_RATE', 'MORTGAGE_RATE_NAN', 'FICO_SCORE_ORIGINATION', 'INITIAL_INTEREST_RATE', 'ORIGINAL_LTV', 'ORIGINAL_BALANCE', 'BACKEND_RATIO', 'BACKEND_RATIO_NAN', 'ORIGINAL_TERM', 'ORIGINAL_TERM_NAN', 'SALE_PRICE', 'SALE_PRICE_NAN', 'PREPAY_PENALTY_TERM', 'PREPAY_PENALTY_TERM_NAN', 'NUMBER_OF_UNITS', 'NUMBER_OF_UNITS_NAN', 'MARGIN', 'MARGIN_NAN', 'PERIODIC_RATE_CAP', 'PERIODIC_RATE_CAP_NAN', 'PERIODIC_RATE_FLOOR', 'PERIODIC_RATE_FLOOR_NAN', 'LIFETIME_RATE_CAP', 'LIFETIME_RATE_CAP_NAN', 'LIFETIME_RATE_FLOOR', 'LIFETIME_RATE_FLOOR_NAN', 'RATE_RESET_FREQUENCY', 'RATE_RESET_FREQUENCY_NAN', 'PAY_RESET_FREQUENCY', 'PAY_RESET_FREQUENCY_NAN', 'FIRST_RATE_RESET_PERIOD', 'FIRST_RATE_RESET_PERIOD_NAN', 'LLMA2_PRIME', 'LLMA2_SUBPRIME', 'LLMA2_APPVAL_LT_SALEPRICE', 'LLMA2_ORIG_RATE_SPREAD', 'LLMA2_ORIG_RATE_SPREAD_NAN', 'AGI', 'AGI_NAN', 'UR', 'UR_NAN', 'LLMA2_ORIG_RATE_ORIG_MR_SPREAD', 'LLMA2_ORIG_RATE_ORIG_MR_SPREAD_NAN', 'COUNT_INT_RATE_LESS', 'NUM_PRIME_ZIP', 'NUM_PRIME_ZIP_NAN', 'MBA_DELINQUENCY_STATUS_0', 'MBA_DELINQUENCY_STATUS_3', 'MBA_DELINQUENCY_STATUS_6', 'MBA_DELINQUENCY_STATUS_9', 'MBA_DELINQUENCY_STATUS_C', 'MBA_DELINQUENCY_STATUS_F', 'MBA_DELINQUENCY_STATUS_R', 'BUYDOWN_FLAG_N', 'BUYDOWN_FLAG_U', 'BUYDOWN_FLAG_Y', 'NEGATIVE_AMORTIZATION_FLAG_N', 'NEGATIVE_AMORTIZATION_FLAG_U', 'NEGATIVE_AMORTIZATION_FLAG_Y', 'PREPAY_PENALTY_FLAG_N', 'PREPAY_PENALTY_FLAG_U', 'PREPAY_PENALTY_FLAG_Y', 'OCCUPANCY_TYPE_1', 'OCCUPANCY_TYPE_2', 'OCCUPANCY_TYPE_3', 'OCCUPANCY_TYPE_U', 'PRODUCT_TYPE_10', 'PRODUCT_TYPE_20', 'PRODUCT_TYPE_30', 'PRODUCT_TYPE_40', 'PRODUCT_TYPE_50', 'PRODUCT_TYPE_51', 'PRODUCT_TYPE_52', 'PRODUCT_TYPE_53', 'PRODUCT_TYPE_54', 'PRODUCT_TYPE_5A', 'PRODUCT_TYPE_5Z', 'PRODUCT_TYPE_60', 'PRODUCT_TYPE_61', 'PRODUCT_TYPE_62', 'PRODUCT_TYPE_63', 'PRODUCT_TYPE_6Z', 'PRODUCT_TYPE_70', 'PRODUCT_TYPE_80', 'PRODUCT_TYPE_81', 'PRODUCT_TYPE_82', 'PRODUCT_TYPE_83', 'PRODUCT_TYPE_84', 'PRODUCT_TYPE_8Z', 'PRODUCT_TYPE_U', 'PROPERTY_TYPE_1', 'PROPERTY_TYPE_2', 'PROPERTY_TYPE_3', 'PROPERTY_TYPE_4', 'PROPERTY_TYPE_5', 'PROPERTY_TYPE_6', 'PROPERTY_TYPE_7', 'PROPERTY_TYPE_8', 'PROPERTY_TYPE_9', 'PROPERTY_TYPE_M', 'PROPERTY_TYPE_U', 'PROPERTY_TYPE_Z', 'LOAN_PURPOSE_CATEGORY_P', 'LOAN_PURPOSE_CATEGORY_R', 'LOAN_PURPOSE_CATEGORY_U', 'DOCUMENTATION_TYPE_1', 'DOCUMENTATION_TYPE_2', 'DOCUMENTATION_TYPE_3', 'DOCUMENTATION_TYPE_U', 'CHANNEL_1', 'CHANNEL_2', 'CHANNEL_3', 'CHANNEL_4', 'CHANNEL_5', 'CHANNEL_6', 'CHANNEL_7', 'CHANNEL_8', 'CHANNEL_9', 'CHANNEL_A', 'CHANNEL_B', 'CHANNEL_C', 'CHANNEL_D', 'CHANNEL_U', 'LOAN_TYPE_1', 'LOAN_TYPE_2', 'LOAN_TYPE_3', 'LOAN_TYPE_4', 'LOAN_TYPE_5', 'LOAN_TYPE_6', 'LOAN_TYPE_U', 'IO_FLAG_N', 'IO_FLAG_U', 'IO_FLAG_Y', 'CONVERTIBLE_FLAG_N', 'CONVERTIBLE_FLAG_U', 'CONVERTIBLE_FLAG_Y', 'POOL_INSURANCE_FLAG_N', 'POOL_INSURANCE_FLAG_U', 'POOL_INSURANCE_FLAG_Y', 'STATE_AK', 'STATE_AL', 'STATE_AR', 'STATE_AZ', 'STATE_CA', 'STATE_CO', 'STATE_CT', 'STATE_DC', 'STATE_DE', 'STATE_FL', 'STATE_GA', 'STATE_HI', 'STATE_IA', 'STATE_ID', 'STATE_IL', 'STATE_IN', 'STATE_KS', 'STATE_KY', 'STATE_LA', 'STATE_MA', 'STATE_MD', 'STATE_ME', 'STATE_MI', 'STATE_MN', 'STATE_MO', 'STATE_MS', 'STATE_MT', 'STATE_NC', 'STATE_ND', 'STATE_NE', 'STATE_NH', 'STATE_NJ', 'STATE_NM', 'STATE_NV', 'STATE_NY', 'STATE_OH', 'STATE_OK', 'STATE_OR', 'STATE_PA', 'STATE_PR', 'STATE_RI', 'STATE_SC', 'STATE_SD', 'STATE_TN', 'STATE_TX', 'STATE_UT', 'STATE_VA', 'STATE_VT', 'STATE_WA', 'STATE_WI', 'STATE_WV', 'STATE_WY', 'CURRENT_INVESTOR_CODE_240', 'CURRENT_INVESTOR_CODE_250', 'CURRENT_INVESTOR_CODE_253', 'CURRENT_INVESTOR_CODE_U', 'ORIGINATION_YEAR_B1995', 'ORIGINATION_YEAR_1995', 'ORIGINATION_YEAR_1996', 'ORIGINATION_YEAR_1997', 'ORIGINATION_YEAR_1998', 'ORIGINATION_YEAR_1999', 'ORIGINATION_YEAR_2000', 'ORIGINATION_YEAR_2001', 'ORIGINATION_YEAR_2002', 'ORIGINATION_YEAR_2003', 'ORIGINATION_YEAR_2004', 'ORIGINATION_YEAR_2005', 'ORIGINATION_YEAR_2006', 'ORIGINATION_YEAR_2007', 'ORIGINATION_YEAR_2008', 'ORIGINATION_YEAR_2009', 'ORIGINATION_YEAR_2010', 'ORIGINATION_YEAR_2011', 'ORIGINATION_YEAR_2012', 'ORIGINATION_YEAR_2013', 'ORIGINATION_YEAR_2014', 'ORIGINATION_YEAR_2015', 'ORIGINATION_YEAR_2016', 'ORIGINATION_YEAR_2017', 'ORIGINATION_YEAR_2018']\r\n",
      "2018-11-01 18:39:04,894 - Xworkers_1mill_1 - INFO - Features List: ['MBA_DAYS_DELINQUENT', 'MBA_DAYS_DELINQUENT_NAN', 'CURRENT_INTEREST_RATE', 'CURRENT_INTEREST_RATE_NAN', 'LOANAGE', 'LOANAGE_NAN', 'CURRENT_BALANCE', 'CURRENT_BALANCE_NAN', 'SCHEDULED_PRINCIPAL', 'SCHEDULED_PRINCIPAL_NAN', 'SCHEDULED_MONTHLY_PANDI', 'SCHEDULED_MONTHLY_PANDI_NAN', 'LLMA2_CURRENT_INTEREST_SPREAD', 'LLMA2_CURRENT_INTEREST_SPREAD_NAN', 'LLMA2_C_IN_LAST_12_MONTHS', 'LLMA2_30_IN_LAST_12_MONTHS', 'LLMA2_60_IN_LAST_12_MONTHS', 'LLMA2_90_IN_LAST_12_MONTHS', 'LLMA2_FC_IN_LAST_12_MONTHS', 'LLMA2_REO_IN_LAST_12_MONTHS', 'LLMA2_0_IN_LAST_12_MONTHS', 'LLMA2_HIST_LAST_12_MONTHS_MIS', 'NUM_MODIF', 'NUM_MODIF_NAN', 'P_RATE_TO_MOD', 'P_RATE_TO_MOD_NAN', 'MOD_RATE', 'MOD_RATE_NAN', 'DIF_RATE', 'DIF_RATE_NAN', 'P_MONTHLY_PAY', 'P_MONTHLY_PAY_NAN', 'MOD_MONTHLY_PAY', 'MOD_MONTHLY_PAY_NAN', 'DIF_MONTHLY_PAY', 'DIF_MONTHLY_PAY_NAN', 'CAPITALIZATION_AMT', 'CAPITALIZATION_AMT_NAN', 'MORTGAGE_RATE', 'MORTGAGE_RATE_NAN', 'FICO_SCORE_ORIGINATION', 'INITIAL_INTEREST_RATE', 'ORIGINAL_LTV', 'ORIGINAL_BALANCE', 'BACKEND_RATIO', 'BACKEND_RATIO_NAN', 'ORIGINAL_TERM', 'ORIGINAL_TERM_NAN', 'SALE_PRICE', 'SALE_PRICE_NAN', 'PREPAY_PENALTY_TERM', 'PREPAY_PENALTY_TERM_NAN', 'NUMBER_OF_UNITS', 'NUMBER_OF_UNITS_NAN', 'MARGIN', 'MARGIN_NAN', 'PERIODIC_RATE_CAP', 'PERIODIC_RATE_CAP_NAN', 'PERIODIC_RATE_FLOOR', 'PERIODIC_RATE_FLOOR_NAN', 'LIFETIME_RATE_CAP', 'LIFETIME_RATE_CAP_NAN', 'LIFETIME_RATE_FLOOR', 'LIFETIME_RATE_FLOOR_NAN', 'RATE_RESET_FREQUENCY', 'RATE_RESET_FREQUENCY_NAN', 'PAY_RESET_FREQUENCY', 'PAY_RESET_FREQUENCY_NAN', 'FIRST_RATE_RESET_PERIOD', 'FIRST_RATE_RESET_PERIOD_NAN', 'LLMA2_PRIME', 'LLMA2_SUBPRIME', 'LLMA2_APPVAL_LT_SALEPRICE', 'LLMA2_ORIG_RATE_SPREAD', 'LLMA2_ORIG_RATE_SPREAD_NAN', 'AGI', 'AGI_NAN', 'UR', 'UR_NAN', 'LLMA2_ORIG_RATE_ORIG_MR_SPREAD', 'LLMA2_ORIG_RATE_ORIG_MR_SPREAD_NAN', 'COUNT_INT_RATE_LESS', 'NUM_PRIME_ZIP', 'NUM_PRIME_ZIP_NAN', 'MBA_DELINQUENCY_STATUS_0', 'MBA_DELINQUENCY_STATUS_3', 'MBA_DELINQUENCY_STATUS_6', 'MBA_DELINQUENCY_STATUS_9', 'MBA_DELINQUENCY_STATUS_C', 'MBA_DELINQUENCY_STATUS_F', 'MBA_DELINQUENCY_STATUS_R', 'BUYDOWN_FLAG_N', 'BUYDOWN_FLAG_U', 'BUYDOWN_FLAG_Y', 'NEGATIVE_AMORTIZATION_FLAG_N', 'NEGATIVE_AMORTIZATION_FLAG_U', 'NEGATIVE_AMORTIZATION_FLAG_Y', 'PREPAY_PENALTY_FLAG_N', 'PREPAY_PENALTY_FLAG_U', 'PREPAY_PENALTY_FLAG_Y', 'OCCUPANCY_TYPE_1', 'OCCUPANCY_TYPE_2', 'OCCUPANCY_TYPE_3', 'OCCUPANCY_TYPE_U', 'PRODUCT_TYPE_10', 'PRODUCT_TYPE_20', 'PRODUCT_TYPE_30', 'PRODUCT_TYPE_40', 'PRODUCT_TYPE_50', 'PRODUCT_TYPE_51', 'PRODUCT_TYPE_52', 'PRODUCT_TYPE_53', 'PRODUCT_TYPE_54', 'PRODUCT_TYPE_5A', 'PRODUCT_TYPE_5Z', 'PRODUCT_TYPE_60', 'PRODUCT_TYPE_61', 'PRODUCT_TYPE_62', 'PRODUCT_TYPE_63', 'PRODUCT_TYPE_6Z', 'PRODUCT_TYPE_70', 'PRODUCT_TYPE_80', 'PRODUCT_TYPE_81', 'PRODUCT_TYPE_82', 'PRODUCT_TYPE_83', 'PRODUCT_TYPE_84', 'PRODUCT_TYPE_8Z', 'PRODUCT_TYPE_U', 'PROPERTY_TYPE_1', 'PROPERTY_TYPE_2', 'PROPERTY_TYPE_3', 'PROPERTY_TYPE_4', 'PROPERTY_TYPE_5', 'PROPERTY_TYPE_6', 'PROPERTY_TYPE_7', 'PROPERTY_TYPE_8', 'PROPERTY_TYPE_9', 'PROPERTY_TYPE_M', 'PROPERTY_TYPE_U', 'PROPERTY_TYPE_Z', 'LOAN_PURPOSE_CATEGORY_P', 'LOAN_PURPOSE_CATEGORY_R', 'LOAN_PURPOSE_CATEGORY_U', 'DOCUMENTATION_TYPE_1', 'DOCUMENTATION_TYPE_2', 'DOCUMENTATION_TYPE_3', 'DOCUMENTATION_TYPE_U', 'CHANNEL_1', 'CHANNEL_2', 'CHANNEL_3', 'CHANNEL_4', 'CHANNEL_5', 'CHANNEL_6', 'CHANNEL_7', 'CHANNEL_8', 'CHANNEL_9', 'CHANNEL_A', 'CHANNEL_B', 'CHANNEL_C', 'CHANNEL_D', 'CHANNEL_U', 'LOAN_TYPE_1', 'LOAN_TYPE_2', 'LOAN_TYPE_3', 'LOAN_TYPE_4', 'LOAN_TYPE_5', 'LOAN_TYPE_6', 'LOAN_TYPE_U', 'IO_FLAG_N', 'IO_FLAG_U', 'IO_FLAG_Y', 'CONVERTIBLE_FLAG_N', 'CONVERTIBLE_FLAG_U', 'CONVERTIBLE_FLAG_Y', 'POOL_INSURANCE_FLAG_N', 'POOL_INSURANCE_FLAG_U', 'POOL_INSURANCE_FLAG_Y', 'STATE_AK', 'STATE_AL', 'STATE_AR', 'STATE_AZ', 'STATE_CA', 'STATE_CO', 'STATE_CT', 'STATE_DC', 'STATE_DE', 'STATE_FL', 'STATE_GA', 'STATE_HI', 'STATE_IA', 'STATE_ID', 'STATE_IL', 'STATE_IN', 'STATE_KS', 'STATE_KY', 'STATE_LA', 'STATE_MA', 'STATE_MD', 'STATE_ME', 'STATE_MI', 'STATE_MN', 'STATE_MO', 'STATE_MS', 'STATE_MT', 'STATE_NC', 'STATE_ND', 'STATE_NE', 'STATE_NH', 'STATE_NJ', 'STATE_NM', 'STATE_NV', 'STATE_NY', 'STATE_OH', 'STATE_OK', 'STATE_OR', 'STATE_PA', 'STATE_PR', 'STATE_RI', 'STATE_SC', 'STATE_SD', 'STATE_TN', 'STATE_TX', 'STATE_UT', 'STATE_VA', 'STATE_VT', 'STATE_WA', 'STATE_WI', 'STATE_WV', 'STATE_WY', 'CURRENT_INVESTOR_CODE_240', 'CURRENT_INVESTOR_CODE_250', 'CURRENT_INVESTOR_CODE_253', 'CURRENT_INVESTOR_CODE_U', 'ORIGINATION_YEAR_B1995', 'ORIGINATION_YEAR_1995', 'ORIGINATION_YEAR_1996', 'ORIGINATION_YEAR_1997', 'ORIGINATION_YEAR_1998', 'ORIGINATION_YEAR_1999', 'ORIGINATION_YEAR_2000', 'ORIGINATION_YEAR_2001', 'ORIGINATION_YEAR_2002', 'ORIGINATION_YEAR_2003', 'ORIGINATION_YEAR_2004', 'ORIGINATION_YEAR_2005', 'ORIGINATION_YEAR_2006', 'ORIGINATION_YEAR_2007', 'ORIGINATION_YEAR_2008', 'ORIGINATION_YEAR_2009', 'ORIGINATION_YEAR_2010', 'ORIGINATION_YEAR_2011', 'ORIGINATION_YEAR_2012', 'ORIGINATION_YEAR_2013', 'ORIGINATION_YEAR_2014', 'ORIGINATION_YEAR_2015', 'ORIGINATION_YEAR_2016', 'ORIGINATION_YEAR_2017', 'ORIGINATION_YEAR_2018']\r\n",
      "2018-11-01 18:39:04,894 - Xworkers_1mill_1 - INFO - Labels List: ['DELINQUENCY_STATUS_NEXT_0', 'DELINQUENCY_STATUS_NEXT_3', 'DELINQUENCY_STATUS_NEXT_6', 'DELINQUENCY_STATUS_NEXT_9', 'DELINQUENCY_STATUS_NEXT_C', 'DELINQUENCY_STATUS_NEXT_F', 'DELINQUENCY_STATUS_NEXT_R']\r\n",
      "2018-11-01 18:39:04,894 - Xworkers_1mill_1 - INFO - Labels List: ['DELINQUENCY_STATUS_NEXT_0', 'DELINQUENCY_STATUS_NEXT_3', 'DELINQUENCY_STATUS_NEXT_6', 'DELINQUENCY_STATUS_NEXT_9', 'DELINQUENCY_STATUS_NEXT_C', 'DELINQUENCY_STATUS_NEXT_F', 'DELINQUENCY_STATUS_NEXT_R']\r\n",
      "38500000\r\n",
      "38500000\r\n",
      "2018-11-01 18:39:04,894 - Xworkers_1mill_1 - INFO - METRICS:  Namespace(allow_summaries=True, batch_layer_type='batch', batch_norm=True, batch_size=8850, batch_type='layer', data_dir='/input_data', decay_rate=1, decay_step=3520000, display_every=100, dropout=True, dropout_keep=0.9, epoch_flag=0, epoch_num=15, eval=False, ftp_dir='processed/c1mill', learning_rate=0.1, log_file=None, logdir=PosixPath('/home/ubuntu/summ_15ep_2wrk'), loss_tolerance=0.0001, lr_decay_epochs=30, lr_decay_policy='time', lr_decay_rate=0.1, lr_poly_power=2.0, max_epoch_size=-1, momentum=0.5, n_hidden=3, name='Xworkers_1mill', nstep_burnin=20, rate_min=0.0015, reg_rate=1e-05, s_hidden=[200, 140, 140], save_interval=450, stratified_flag=False, summary_interval=1800, test_batch_size=1200000, test_dir='chuncks_random_c1millx2_test', test_flag=True, test_period=[286, 304], total_examples=38500000, train_dir='chuncks_random_c1millx2_train', train_period=[121, 279], valid_batch_size=150000, valid_dir='chuncks_random_c1millx2_valid', valid_period=[280, 285], weighted_sampling=False, xla=True)\r",
      "\r\n",
      "\r\n",
      "2018-11-01 18:39:04,894 - Xworkers_1mill_1 - INFO - METRICS:  Namespace(allow_summaries=True, batch_layer_type='batch', batch_norm=True, batch_size=8850, batch_type='layer', data_dir='/input_data', decay_rate=1, decay_step=3520000, display_every=100, dropout=True, dropout_keep=0.9, epoch_flag=0, epoch_num=15, eval=False, ftp_dir='processed/c1mill', learning_rate=0.1, log_file=None, logdir=PosixPath('/home/ubuntu/summ_15ep_2wrk'), loss_tolerance=0.0001, lr_decay_epochs=30, lr_decay_policy='time', lr_decay_rate=0.1, lr_poly_power=2.0, max_epoch_size=-1, momentum=0.5, n_hidden=3, name='Xworkers_1mill', nstep_burnin=20, rate_min=0.0015, reg_rate=1e-05, s_hidden=[200, 140, 140], save_interval=450, stratified_flag=False, summary_interval=1800, test_batch_size=1200000, test_dir='chuncks_random_c1millx2_test', test_flag=True, test_period=[286, 304], total_examples=38500000, train_dir='chuncks_random_c1millx2_train', train_period=[121, 279], valid_batch_size=150000, valid_dir='chuncks_random_c1millx2_valid', valid_period=[280, 285], weighted_sampling=False, xla=True)\r",
      "\r\n",
      "\r\n",
      "2018-11-01 18:39:04,895 - Xworkers_1mill_1 - INFO - Number of total steps: 65255\r\n",
      "2018-11-01 18:39:04,895 - Xworkers_1mill_1 - INFO - Number of total steps: 65255\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/MLMortgage/data/processed/chuncks_random_c1millx2_train/temporalloandynmodifmrstaticitur-5mill-6mill_non_index-train_2.h5  loaded in RAM\n",
      "/home/ubuntu/MLMortgage/data/processed/chuncks_random_c1millx2_train/temporalloandynmodifmrstaticitur8mill-9mill_non_index-train_4.h5 ...to load\n",
      "/home/ubuntu/MLMortgage/data/processed/chuncks_random_c1millx2_train/temporalloandynmodifmrstaticitur8mill-9mill_non_index-train_4.h5  loaded in RAM\n",
      "2018-11-01 18:39:07,320 - Xworkers_1mill_4 - INFO - Features List: ['MBA_DAYS_DELINQUENT', 'MBA_DAYS_DELINQUENT_NAN', 'CURRENT_INTEREST_RATE', 'CURRENT_INTEREST_RATE_NAN', 'LOANAGE', 'LOANAGE_NAN', 'CURRENT_BALANCE', 'CURRENT_BALANCE_NAN', 'SCHEDULED_PRINCIPAL', 'SCHEDULED_PRINCIPAL_NAN', 'SCHEDULED_MONTHLY_PANDI', 'SCHEDULED_MONTHLY_PANDI_NAN', 'LLMA2_CURRENT_INTEREST_SPREAD', 'LLMA2_CURRENT_INTEREST_SPREAD_NAN', 'LLMA2_C_IN_LAST_12_MONTHS', 'LLMA2_30_IN_LAST_12_MONTHS', 'LLMA2_60_IN_LAST_12_MONTHS', 'LLMA2_90_IN_LAST_12_MONTHS', 'LLMA2_FC_IN_LAST_12_MONTHS', 'LLMA2_REO_IN_LAST_12_MONTHS', 'LLMA2_0_IN_LAST_12_MONTHS', 'LLMA2_HIST_LAST_12_MONTHS_MIS', 'NUM_MODIF', 'NUM_MODIF_NAN', 'P_RATE_TO_MOD', 'P_RATE_TO_MOD_NAN', 'MOD_RATE', 'MOD_RATE_NAN', 'DIF_RATE', 'DIF_RATE_NAN', 'P_MONTHLY_PAY', 'P_MONTHLY_PAY_NAN', 'MOD_MONTHLY_PAY', 'MOD_MONTHLY_PAY_NAN', 'DIF_MONTHLY_PAY', 'DIF_MONTHLY_PAY_NAN', 'CAPITALIZATION_AMT', 'CAPITALIZATION_AMT_NAN', 'MORTGAGE_RATE', 'MORTGAGE_RATE_NAN', 'FICO_SCORE_ORIGINATION', 'INITIAL_INTEREST_RATE', 'ORIGINAL_LTV', 'ORIGINAL_BALANCE', 'BACKEND_RATIO', 'BACKEND_RATIO_NAN', 'ORIGINAL_TERM', 'ORIGINAL_TERM_NAN', 'SALE_PRICE', 'SALE_PRICE_NAN', 'PREPAY_PENALTY_TERM', 'PREPAY_PENALTY_TERM_NAN', 'NUMBER_OF_UNITS', 'NUMBER_OF_UNITS_NAN', 'MARGIN', 'MARGIN_NAN', 'PERIODIC_RATE_CAP', 'PERIODIC_RATE_CAP_NAN', 'PERIODIC_RATE_FLOOR', 'PERIODIC_RATE_FLOOR_NAN', 'LIFETIME_RATE_CAP', 'LIFETIME_RATE_CAP_NAN', 'LIFETIME_RATE_FLOOR', 'LIFETIME_RATE_FLOOR_NAN', 'RATE_RESET_FREQUENCY', 'RATE_RESET_FREQUENCY_NAN', 'PAY_RESET_FREQUENCY', 'PAY_RESET_FREQUENCY_NAN', 'FIRST_RATE_RESET_PERIOD', 'FIRST_RATE_RESET_PERIOD_NAN', 'LLMA2_PRIME', 'LLMA2_SUBPRIME', 'LLMA2_APPVAL_LT_SALEPRICE', 'LLMA2_ORIG_RATE_SPREAD', 'LLMA2_ORIG_RATE_SPREAD_NAN', 'AGI', 'AGI_NAN', 'UR', 'UR_NAN', 'LLMA2_ORIG_RATE_ORIG_MR_SPREAD', 'LLMA2_ORIG_RATE_ORIG_MR_SPREAD_NAN', 'COUNT_INT_RATE_LESS', 'NUM_PRIME_ZIP', 'NUM_PRIME_ZIP_NAN', 'MBA_DELINQUENCY_STATUS_0', 'MBA_DELINQUENCY_STATUS_3', 'MBA_DELINQUENCY_STATUS_6', 'MBA_DELINQUENCY_STATUS_9', 'MBA_DELINQUENCY_STATUS_C', 'MBA_DELINQUENCY_STATUS_F', 'MBA_DELINQUENCY_STATUS_R', 'BUYDOWN_FLAG_N', 'BUYDOWN_FLAG_U', 'BUYDOWN_FLAG_Y', 'NEGATIVE_AMORTIZATION_FLAG_N', 'NEGATIVE_AMORTIZATION_FLAG_U', 'NEGATIVE_AMORTIZATION_FLAG_Y', 'PREPAY_PENALTY_FLAG_N', 'PREPAY_PENALTY_FLAG_U', 'PREPAY_PENALTY_FLAG_Y', 'OCCUPANCY_TYPE_1', 'OCCUPANCY_TYPE_2', 'OCCUPANCY_TYPE_3', 'OCCUPANCY_TYPE_U', 'PRODUCT_TYPE_10', 'PRODUCT_TYPE_20', 'PRODUCT_TYPE_30', 'PRODUCT_TYPE_40', 'PRODUCT_TYPE_50', 'PRODUCT_TYPE_51', 'PRODUCT_TYPE_52', 'PRODUCT_TYPE_53', 'PRODUCT_TYPE_54', 'PRODUCT_TYPE_5A', 'PRODUCT_TYPE_5Z', 'PRODUCT_TYPE_60', 'PRODUCT_TYPE_61', 'PRODUCT_TYPE_62', 'PRODUCT_TYPE_63', 'PRODUCT_TYPE_6Z', 'PRODUCT_TYPE_70', 'PRODUCT_TYPE_80', 'PRODUCT_TYPE_81', 'PRODUCT_TYPE_82', 'PRODUCT_TYPE_83', 'PRODUCT_TYPE_84', 'PRODUCT_TYPE_8Z', 'PRODUCT_TYPE_U', 'PROPERTY_TYPE_1', 'PROPERTY_TYPE_2', 'PROPERTY_TYPE_3', 'PROPERTY_TYPE_4', 'PROPERTY_TYPE_5', 'PROPERTY_TYPE_6', 'PROPERTY_TYPE_7', 'PROPERTY_TYPE_8', 'PROPERTY_TYPE_9', 'PROPERTY_TYPE_M', 'PROPERTY_TYPE_U', 'PROPERTY_TYPE_Z', 'LOAN_PURPOSE_CATEGORY_P', 'LOAN_PURPOSE_CATEGORY_R', 'LOAN_PURPOSE_CATEGORY_U', 'DOCUMENTATION_TYPE_1', 'DOCUMENTATION_TYPE_2', 'DOCUMENTATION_TYPE_3', 'DOCUMENTATION_TYPE_U', 'CHANNEL_1', 'CHANNEL_2', 'CHANNEL_3', 'CHANNEL_4', 'CHANNEL_5', 'CHANNEL_6', 'CHANNEL_7', 'CHANNEL_8', 'CHANNEL_9', 'CHANNEL_A', 'CHANNEL_B', 'CHANNEL_C', 'CHANNEL_D', 'CHANNEL_U', 'LOAN_TYPE_1', 'LOAN_TYPE_2', 'LOAN_TYPE_3', 'LOAN_TYPE_4', 'LOAN_TYPE_5', 'LOAN_TYPE_6', 'LOAN_TYPE_U', 'IO_FLAG_N', 'IO_FLAG_U', 'IO_FLAG_Y', 'CONVERTIBLE_FLAG_N', 'CONVERTIBLE_FLAG_U', 'CONVERTIBLE_FLAG_Y', 'POOL_INSURANCE_FLAG_N', 'POOL_INSURANCE_FLAG_U', 'POOL_INSURANCE_FLAG_Y', 'STATE_AK', 'STATE_AL', 'STATE_AR', 'STATE_AZ', 'STATE_CA', 'STATE_CO', 'STATE_CT', 'STATE_DC', 'STATE_DE', 'STATE_FL', 'STATE_GA', 'STATE_HI', 'STATE_IA', 'STATE_ID', 'STATE_IL', 'STATE_IN', 'STATE_KS', 'STATE_KY', 'STATE_LA', 'STATE_MA', 'STATE_MD', 'STATE_ME', 'STATE_MI', 'STATE_MN', 'STATE_MO', 'STATE_MS', 'STATE_MT', 'STATE_NC', 'STATE_ND', 'STATE_NE', 'STATE_NH', 'STATE_NJ', 'STATE_NM', 'STATE_NV', 'STATE_NY', 'STATE_OH', 'STATE_OK', 'STATE_OR', 'STATE_PA', 'STATE_PR', 'STATE_RI', 'STATE_SC', 'STATE_SD', 'STATE_TN', 'STATE_TX', 'STATE_UT', 'STATE_VA', 'STATE_VT', 'STATE_WA', 'STATE_WI', 'STATE_WV', 'STATE_WY', 'CURRENT_INVESTOR_CODE_240', 'CURRENT_INVESTOR_CODE_250', 'CURRENT_INVESTOR_CODE_253', 'CURRENT_INVESTOR_CODE_U', 'ORIGINATION_YEAR_B1995', 'ORIGINATION_YEAR_1995', 'ORIGINATION_YEAR_1996', 'ORIGINATION_YEAR_1997', 'ORIGINATION_YEAR_1998', 'ORIGINATION_YEAR_1999', 'ORIGINATION_YEAR_2000', 'ORIGINATION_YEAR_2001', 'ORIGINATION_YEAR_2002', 'ORIGINATION_YEAR_2003', 'ORIGINATION_YEAR_2004', 'ORIGINATION_YEAR_2005', 'ORIGINATION_YEAR_2006', 'ORIGINATION_YEAR_2007', 'ORIGINATION_YEAR_2008', 'ORIGINATION_YEAR_2009', 'ORIGINATION_YEAR_2010', 'ORIGINATION_YEAR_2011', 'ORIGINATION_YEAR_2012', 'ORIGINATION_YEAR_2013', 'ORIGINATION_YEAR_2014', 'ORIGINATION_YEAR_2015', 'ORIGINATION_YEAR_2016', 'ORIGINATION_YEAR_2017', 'ORIGINATION_YEAR_2018']\n",
      "2018-11-01 18:39:07,320 - Xworkers_1mill_4 - INFO - Features List: ['MBA_DAYS_DELINQUENT', 'MBA_DAYS_DELINQUENT_NAN', 'CURRENT_INTEREST_RATE', 'CURRENT_INTEREST_RATE_NAN', 'LOANAGE', 'LOANAGE_NAN', 'CURRENT_BALANCE', 'CURRENT_BALANCE_NAN', 'SCHEDULED_PRINCIPAL', 'SCHEDULED_PRINCIPAL_NAN', 'SCHEDULED_MONTHLY_PANDI', 'SCHEDULED_MONTHLY_PANDI_NAN', 'LLMA2_CURRENT_INTEREST_SPREAD', 'LLMA2_CURRENT_INTEREST_SPREAD_NAN', 'LLMA2_C_IN_LAST_12_MONTHS', 'LLMA2_30_IN_LAST_12_MONTHS', 'LLMA2_60_IN_LAST_12_MONTHS', 'LLMA2_90_IN_LAST_12_MONTHS', 'LLMA2_FC_IN_LAST_12_MONTHS', 'LLMA2_REO_IN_LAST_12_MONTHS', 'LLMA2_0_IN_LAST_12_MONTHS', 'LLMA2_HIST_LAST_12_MONTHS_MIS', 'NUM_MODIF', 'NUM_MODIF_NAN', 'P_RATE_TO_MOD', 'P_RATE_TO_MOD_NAN', 'MOD_RATE', 'MOD_RATE_NAN', 'DIF_RATE', 'DIF_RATE_NAN', 'P_MONTHLY_PAY', 'P_MONTHLY_PAY_NAN', 'MOD_MONTHLY_PAY', 'MOD_MONTHLY_PAY_NAN', 'DIF_MONTHLY_PAY', 'DIF_MONTHLY_PAY_NAN', 'CAPITALIZATION_AMT', 'CAPITALIZATION_AMT_NAN', 'MORTGAGE_RATE', 'MORTGAGE_RATE_NAN', 'FICO_SCORE_ORIGINATION', 'INITIAL_INTEREST_RATE', 'ORIGINAL_LTV', 'ORIGINAL_BALANCE', 'BACKEND_RATIO', 'BACKEND_RATIO_NAN', 'ORIGINAL_TERM', 'ORIGINAL_TERM_NAN', 'SALE_PRICE', 'SALE_PRICE_NAN', 'PREPAY_PENALTY_TERM', 'PREPAY_PENALTY_TERM_NAN', 'NUMBER_OF_UNITS', 'NUMBER_OF_UNITS_NAN', 'MARGIN', 'MARGIN_NAN', 'PERIODIC_RATE_CAP', 'PERIODIC_RATE_CAP_NAN', 'PERIODIC_RATE_FLOOR', 'PERIODIC_RATE_FLOOR_NAN', 'LIFETIME_RATE_CAP', 'LIFETIME_RATE_CAP_NAN', 'LIFETIME_RATE_FLOOR', 'LIFETIME_RATE_FLOOR_NAN', 'RATE_RESET_FREQUENCY', 'RATE_RESET_FREQUENCY_NAN', 'PAY_RESET_FREQUENCY', 'PAY_RESET_FREQUENCY_NAN', 'FIRST_RATE_RESET_PERIOD', 'FIRST_RATE_RESET_PERIOD_NAN', 'LLMA2_PRIME', 'LLMA2_SUBPRIME', 'LLMA2_APPVAL_LT_SALEPRICE', 'LLMA2_ORIG_RATE_SPREAD', 'LLMA2_ORIG_RATE_SPREAD_NAN', 'AGI', 'AGI_NAN', 'UR', 'UR_NAN', 'LLMA2_ORIG_RATE_ORIG_MR_SPREAD', 'LLMA2_ORIG_RATE_ORIG_MR_SPREAD_NAN', 'COUNT_INT_RATE_LESS', 'NUM_PRIME_ZIP', 'NUM_PRIME_ZIP_NAN', 'MBA_DELINQUENCY_STATUS_0', 'MBA_DELINQUENCY_STATUS_3', 'MBA_DELINQUENCY_STATUS_6', 'MBA_DELINQUENCY_STATUS_9', 'MBA_DELINQUENCY_STATUS_C', 'MBA_DELINQUENCY_STATUS_F', 'MBA_DELINQUENCY_STATUS_R', 'BUYDOWN_FLAG_N', 'BUYDOWN_FLAG_U', 'BUYDOWN_FLAG_Y', 'NEGATIVE_AMORTIZATION_FLAG_N', 'NEGATIVE_AMORTIZATION_FLAG_U', 'NEGATIVE_AMORTIZATION_FLAG_Y', 'PREPAY_PENALTY_FLAG_N', 'PREPAY_PENALTY_FLAG_U', 'PREPAY_PENALTY_FLAG_Y', 'OCCUPANCY_TYPE_1', 'OCCUPANCY_TYPE_2', 'OCCUPANCY_TYPE_3', 'OCCUPANCY_TYPE_U', 'PRODUCT_TYPE_10', 'PRODUCT_TYPE_20', 'PRODUCT_TYPE_30', 'PRODUCT_TYPE_40', 'PRODUCT_TYPE_50', 'PRODUCT_TYPE_51', 'PRODUCT_TYPE_52', 'PRODUCT_TYPE_53', 'PRODUCT_TYPE_54', 'PRODUCT_TYPE_5A', 'PRODUCT_TYPE_5Z', 'PRODUCT_TYPE_60', 'PRODUCT_TYPE_61', 'PRODUCT_TYPE_62', 'PRODUCT_TYPE_63', 'PRODUCT_TYPE_6Z', 'PRODUCT_TYPE_70', 'PRODUCT_TYPE_80', 'PRODUCT_TYPE_81', 'PRODUCT_TYPE_82', 'PRODUCT_TYPE_83', 'PRODUCT_TYPE_84', 'PRODUCT_TYPE_8Z', 'PRODUCT_TYPE_U', 'PROPERTY_TYPE_1', 'PROPERTY_TYPE_2', 'PROPERTY_TYPE_3', 'PROPERTY_TYPE_4', 'PROPERTY_TYPE_5', 'PROPERTY_TYPE_6', 'PROPERTY_TYPE_7', 'PROPERTY_TYPE_8', 'PROPERTY_TYPE_9', 'PROPERTY_TYPE_M', 'PROPERTY_TYPE_U', 'PROPERTY_TYPE_Z', 'LOAN_PURPOSE_CATEGORY_P', 'LOAN_PURPOSE_CATEGORY_R', 'LOAN_PURPOSE_CATEGORY_U', 'DOCUMENTATION_TYPE_1', 'DOCUMENTATION_TYPE_2', 'DOCUMENTATION_TYPE_3', 'DOCUMENTATION_TYPE_U', 'CHANNEL_1', 'CHANNEL_2', 'CHANNEL_3', 'CHANNEL_4', 'CHANNEL_5', 'CHANNEL_6', 'CHANNEL_7', 'CHANNEL_8', 'CHANNEL_9', 'CHANNEL_A', 'CHANNEL_B', 'CHANNEL_C', 'CHANNEL_D', 'CHANNEL_U', 'LOAN_TYPE_1', 'LOAN_TYPE_2', 'LOAN_TYPE_3', 'LOAN_TYPE_4', 'LOAN_TYPE_5', 'LOAN_TYPE_6', 'LOAN_TYPE_U', 'IO_FLAG_N', 'IO_FLAG_U', 'IO_FLAG_Y', 'CONVERTIBLE_FLAG_N', 'CONVERTIBLE_FLAG_U', 'CONVERTIBLE_FLAG_Y', 'POOL_INSURANCE_FLAG_N', 'POOL_INSURANCE_FLAG_U', 'POOL_INSURANCE_FLAG_Y', 'STATE_AK', 'STATE_AL', 'STATE_AR', 'STATE_AZ', 'STATE_CA', 'STATE_CO', 'STATE_CT', 'STATE_DC', 'STATE_DE', 'STATE_FL', 'STATE_GA', 'STATE_HI', 'STATE_IA', 'STATE_ID', 'STATE_IL', 'STATE_IN', 'STATE_KS', 'STATE_KY', 'STATE_LA', 'STATE_MA', 'STATE_MD', 'STATE_ME', 'STATE_MI', 'STATE_MN', 'STATE_MO', 'STATE_MS', 'STATE_MT', 'STATE_NC', 'STATE_ND', 'STATE_NE', 'STATE_NH', 'STATE_NJ', 'STATE_NM', 'STATE_NV', 'STATE_NY', 'STATE_OH', 'STATE_OK', 'STATE_OR', 'STATE_PA', 'STATE_PR', 'STATE_RI', 'STATE_SC', 'STATE_SD', 'STATE_TN', 'STATE_TX', 'STATE_UT', 'STATE_VA', 'STATE_VT', 'STATE_WA', 'STATE_WI', 'STATE_WV', 'STATE_WY', 'CURRENT_INVESTOR_CODE_240', 'CURRENT_INVESTOR_CODE_250', 'CURRENT_INVESTOR_CODE_253', 'CURRENT_INVESTOR_CODE_U', 'ORIGINATION_YEAR_B1995', 'ORIGINATION_YEAR_1995', 'ORIGINATION_YEAR_1996', 'ORIGINATION_YEAR_1997', 'ORIGINATION_YEAR_1998', 'ORIGINATION_YEAR_1999', 'ORIGINATION_YEAR_2000', 'ORIGINATION_YEAR_2001', 'ORIGINATION_YEAR_2002', 'ORIGINATION_YEAR_2003', 'ORIGINATION_YEAR_2004', 'ORIGINATION_YEAR_2005', 'ORIGINATION_YEAR_2006', 'ORIGINATION_YEAR_2007', 'ORIGINATION_YEAR_2008', 'ORIGINATION_YEAR_2009', 'ORIGINATION_YEAR_2010', 'ORIGINATION_YEAR_2011', 'ORIGINATION_YEAR_2012', 'ORIGINATION_YEAR_2013', 'ORIGINATION_YEAR_2014', 'ORIGINATION_YEAR_2015', 'ORIGINATION_YEAR_2016', 'ORIGINATION_YEAR_2017', 'ORIGINATION_YEAR_2018']\n",
      "2018-11-01 18:39:07,320 - Xworkers_1mill_4 - INFO - Labels List: ['DELINQUENCY_STATUS_NEXT_0', 'DELINQUENCY_STATUS_NEXT_3', 'DELINQUENCY_STATUS_NEXT_6', 'DELINQUENCY_STATUS_NEXT_9', 'DELINQUENCY_STATUS_NEXT_C', 'DELINQUENCY_STATUS_NEXT_F', 'DELINQUENCY_STATUS_NEXT_R']\n",
      "2018-11-01 18:39:07,320 - Xworkers_1mill_4 - INFO - Labels List: ['DELINQUENCY_STATUS_NEXT_0', 'DELINQUENCY_STATUS_NEXT_3', 'DELINQUENCY_STATUS_NEXT_6', 'DELINQUENCY_STATUS_NEXT_9', 'DELINQUENCY_STATUS_NEXT_C', 'DELINQUENCY_STATUS_NEXT_F', 'DELINQUENCY_STATUS_NEXT_R']\n",
      "38500000\n",
      "38500000\n",
      "2018-11-01 18:39:07,320 - Xworkers_1mill_4 - INFO - METRICS:  Namespace(allow_summaries=True, batch_layer_type='batch', batch_norm=True, batch_size=8850, batch_type='layer', data_dir='/input_data', decay_rate=1, decay_step=3520000, display_every=100, dropout=True, dropout_keep=0.9, epoch_flag=0, epoch_num=15, eval=False, ftp_dir='processed/c1mill', learning_rate=0.1, log_file=None, logdir=PosixPath('/home/ubuntu/summ_15ep_2wrk'), loss_tolerance=0.0001, lr_decay_epochs=30, lr_decay_policy='time', lr_decay_rate=0.1, lr_poly_power=2.0, max_epoch_size=-1, momentum=0.5, n_hidden=3, name='Xworkers_1mill', nstep_burnin=20, rate_min=0.0015, reg_rate=1e-05, s_hidden=[200, 140, 140], save_interval=450, stratified_flag=False, summary_interval=1800, test_batch_size=1200000, test_dir='chuncks_random_c1millx2_test', test_flag=True, test_period=[286, 304], total_examples=38500000, train_dir='chuncks_random_c1millx2_train', train_period=[121, 279], valid_batch_size=150000, valid_dir='chuncks_random_c1millx2_valid', valid_period=[280, 285], weighted_sampling=False, xla=True)\n",
      "\n",
      "2018-11-01 18:39:07,320 - Xworkers_1mill_4 - INFO - METRICS:  Namespace(allow_summaries=True, batch_layer_type='batch', batch_norm=True, batch_size=8850, batch_type='layer', data_dir='/input_data', decay_rate=1, decay_step=3520000, display_every=100, dropout=True, dropout_keep=0.9, epoch_flag=0, epoch_num=15, eval=False, ftp_dir='processed/c1mill', learning_rate=0.1, log_file=None, logdir=PosixPath('/home/ubuntu/summ_15ep_2wrk'), loss_tolerance=0.0001, lr_decay_epochs=30, lr_decay_policy='time', lr_decay_rate=0.1, lr_poly_power=2.0, max_epoch_size=-1, momentum=0.5, n_hidden=3, name='Xworkers_1mill', nstep_burnin=20, rate_min=0.0015, reg_rate=1e-05, s_hidden=[200, 140, 140], save_interval=450, stratified_flag=False, summary_interval=1800, test_batch_size=1200000, test_dir='chuncks_random_c1millx2_test', test_flag=True, test_period=[286, 304], total_examples=38500000, train_dir='chuncks_random_c1millx2_train', train_period=[121, 279], valid_batch_size=150000, valid_dir='chuncks_random_c1millx2_valid', valid_period=[280, 285], weighted_sampling=False, xla=True)\n",
      "\n",
      "2018-11-01 18:39:07,321 - Xworkers_1mill_4 - INFO - Number of total steps: 65255\n",
      "2018-11-01 18:39:07,321 - Xworkers_1mill_4 - INFO - Number of total steps: 65255\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/MLMortgage/data/processed/chuncks_random_c1millx2_train/temporalloandynmodifmrstaticitur4mill-5mil_non_index-train_2.h5  loaded in RAM\n",
      "/home/ubuntu/MLMortgage/data/processed/chuncks_random_c1millx2_train/temporalloandynmodifmrstaticitur-5mill-6mill_non_index-train_2.h5 ...to load\n",
      "/home/ubuntu/MLMortgage/data/processed/chuncks_random_c1millx2_train/temporalloandynmodifmrstaticitur-5mill-6mill_non_index-train_2.h5  loaded in RAM\n",
      "2018-11-01 18:39:10,012 - Xworkers_1mill_3 - INFO - Features List: ['MBA_DAYS_DELINQUENT', 'MBA_DAYS_DELINQUENT_NAN', 'CURRENT_INTEREST_RATE', 'CURRENT_INTEREST_RATE_NAN', 'LOANAGE', 'LOANAGE_NAN', 'CURRENT_BALANCE', 'CURRENT_BALANCE_NAN', 'SCHEDULED_PRINCIPAL', 'SCHEDULED_PRINCIPAL_NAN', 'SCHEDULED_MONTHLY_PANDI', 'SCHEDULED_MONTHLY_PANDI_NAN', 'LLMA2_CURRENT_INTEREST_SPREAD', 'LLMA2_CURRENT_INTEREST_SPREAD_NAN', 'LLMA2_C_IN_LAST_12_MONTHS', 'LLMA2_30_IN_LAST_12_MONTHS', 'LLMA2_60_IN_LAST_12_MONTHS', 'LLMA2_90_IN_LAST_12_MONTHS', 'LLMA2_FC_IN_LAST_12_MONTHS', 'LLMA2_REO_IN_LAST_12_MONTHS', 'LLMA2_0_IN_LAST_12_MONTHS', 'LLMA2_HIST_LAST_12_MONTHS_MIS', 'NUM_MODIF', 'NUM_MODIF_NAN', 'P_RATE_TO_MOD', 'P_RATE_TO_MOD_NAN', 'MOD_RATE', 'MOD_RATE_NAN', 'DIF_RATE', 'DIF_RATE_NAN', 'P_MONTHLY_PAY', 'P_MONTHLY_PAY_NAN', 'MOD_MONTHLY_PAY', 'MOD_MONTHLY_PAY_NAN', 'DIF_MONTHLY_PAY', 'DIF_MONTHLY_PAY_NAN', 'CAPITALIZATION_AMT', 'CAPITALIZATION_AMT_NAN', 'MORTGAGE_RATE', 'MORTGAGE_RATE_NAN', 'FICO_SCORE_ORIGINATION', 'INITIAL_INTEREST_RATE', 'ORIGINAL_LTV', 'ORIGINAL_BALANCE', 'BACKEND_RATIO', 'BACKEND_RATIO_NAN', 'ORIGINAL_TERM', 'ORIGINAL_TERM_NAN', 'SALE_PRICE', 'SALE_PRICE_NAN', 'PREPAY_PENALTY_TERM', 'PREPAY_PENALTY_TERM_NAN', 'NUMBER_OF_UNITS', 'NUMBER_OF_UNITS_NAN', 'MARGIN', 'MARGIN_NAN', 'PERIODIC_RATE_CAP', 'PERIODIC_RATE_CAP_NAN', 'PERIODIC_RATE_FLOOR', 'PERIODIC_RATE_FLOOR_NAN', 'LIFETIME_RATE_CAP', 'LIFETIME_RATE_CAP_NAN', 'LIFETIME_RATE_FLOOR', 'LIFETIME_RATE_FLOOR_NAN', 'RATE_RESET_FREQUENCY', 'RATE_RESET_FREQUENCY_NAN', 'PAY_RESET_FREQUENCY', 'PAY_RESET_FREQUENCY_NAN', 'FIRST_RATE_RESET_PERIOD', 'FIRST_RATE_RESET_PERIOD_NAN', 'LLMA2_PRIME', 'LLMA2_SUBPRIME', 'LLMA2_APPVAL_LT_SALEPRICE', 'LLMA2_ORIG_RATE_SPREAD', 'LLMA2_ORIG_RATE_SPREAD_NAN', 'AGI', 'AGI_NAN', 'UR', 'UR_NAN', 'LLMA2_ORIG_RATE_ORIG_MR_SPREAD', 'LLMA2_ORIG_RATE_ORIG_MR_SPREAD_NAN', 'COUNT_INT_RATE_LESS', 'NUM_PRIME_ZIP', 'NUM_PRIME_ZIP_NAN', 'MBA_DELINQUENCY_STATUS_0', 'MBA_DELINQUENCY_STATUS_3', 'MBA_DELINQUENCY_STATUS_6', 'MBA_DELINQUENCY_STATUS_9', 'MBA_DELINQUENCY_STATUS_C', 'MBA_DELINQUENCY_STATUS_F', 'MBA_DELINQUENCY_STATUS_R', 'BUYDOWN_FLAG_N', 'BUYDOWN_FLAG_U', 'BUYDOWN_FLAG_Y', 'NEGATIVE_AMORTIZATION_FLAG_N', 'NEGATIVE_AMORTIZATION_FLAG_U', 'NEGATIVE_AMORTIZATION_FLAG_Y', 'PREPAY_PENALTY_FLAG_N', 'PREPAY_PENALTY_FLAG_U', 'PREPAY_PENALTY_FLAG_Y', 'OCCUPANCY_TYPE_1', 'OCCUPANCY_TYPE_2', 'OCCUPANCY_TYPE_3', 'OCCUPANCY_TYPE_U', 'PRODUCT_TYPE_10', 'PRODUCT_TYPE_20', 'PRODUCT_TYPE_30', 'PRODUCT_TYPE_40', 'PRODUCT_TYPE_50', 'PRODUCT_TYPE_51', 'PRODUCT_TYPE_52', 'PRODUCT_TYPE_53', 'PRODUCT_TYPE_54', 'PRODUCT_TYPE_5A', 'PRODUCT_TYPE_5Z', 'PRODUCT_TYPE_60', 'PRODUCT_TYPE_61', 'PRODUCT_TYPE_62', 'PRODUCT_TYPE_63', 'PRODUCT_TYPE_6Z', 'PRODUCT_TYPE_70', 'PRODUCT_TYPE_80', 'PRODUCT_TYPE_81', 'PRODUCT_TYPE_82', 'PRODUCT_TYPE_83', 'PRODUCT_TYPE_84', 'PRODUCT_TYPE_8Z', 'PRODUCT_TYPE_U', 'PROPERTY_TYPE_1', 'PROPERTY_TYPE_2', 'PROPERTY_TYPE_3', 'PROPERTY_TYPE_4', 'PROPERTY_TYPE_5', 'PROPERTY_TYPE_6', 'PROPERTY_TYPE_7', 'PROPERTY_TYPE_8', 'PROPERTY_TYPE_9', 'PROPERTY_TYPE_M', 'PROPERTY_TYPE_U', 'PROPERTY_TYPE_Z', 'LOAN_PURPOSE_CATEGORY_P', 'LOAN_PURPOSE_CATEGORY_R', 'LOAN_PURPOSE_CATEGORY_U', 'DOCUMENTATION_TYPE_1', 'DOCUMENTATION_TYPE_2', 'DOCUMENTATION_TYPE_3', 'DOCUMENTATION_TYPE_U', 'CHANNEL_1', 'CHANNEL_2', 'CHANNEL_3', 'CHANNEL_4', 'CHANNEL_5', 'CHANNEL_6', 'CHANNEL_7', 'CHANNEL_8', 'CHANNEL_9', 'CHANNEL_A', 'CHANNEL_B', 'CHANNEL_C', 'CHANNEL_D', 'CHANNEL_U', 'LOAN_TYPE_1', 'LOAN_TYPE_2', 'LOAN_TYPE_3', 'LOAN_TYPE_4', 'LOAN_TYPE_5', 'LOAN_TYPE_6', 'LOAN_TYPE_U', 'IO_FLAG_N', 'IO_FLAG_U', 'IO_FLAG_Y', 'CONVERTIBLE_FLAG_N', 'CONVERTIBLE_FLAG_U', 'CONVERTIBLE_FLAG_Y', 'POOL_INSURANCE_FLAG_N', 'POOL_INSURANCE_FLAG_U', 'POOL_INSURANCE_FLAG_Y', 'STATE_AK', 'STATE_AL', 'STATE_AR', 'STATE_AZ', 'STATE_CA', 'STATE_CO', 'STATE_CT', 'STATE_DC', 'STATE_DE', 'STATE_FL', 'STATE_GA', 'STATE_HI', 'STATE_IA', 'STATE_ID', 'STATE_IL', 'STATE_IN', 'STATE_KS', 'STATE_KY', 'STATE_LA', 'STATE_MA', 'STATE_MD', 'STATE_ME', 'STATE_MI', 'STATE_MN', 'STATE_MO', 'STATE_MS', 'STATE_MT', 'STATE_NC', 'STATE_ND', 'STATE_NE', 'STATE_NH', 'STATE_NJ', 'STATE_NM', 'STATE_NV', 'STATE_NY', 'STATE_OH', 'STATE_OK', 'STATE_OR', 'STATE_PA', 'STATE_PR', 'STATE_RI', 'STATE_SC', 'STATE_SD', 'STATE_TN', 'STATE_TX', 'STATE_UT', 'STATE_VA', 'STATE_VT', 'STATE_WA', 'STATE_WI', 'STATE_WV', 'STATE_WY', 'CURRENT_INVESTOR_CODE_240', 'CURRENT_INVESTOR_CODE_250', 'CURRENT_INVESTOR_CODE_253', 'CURRENT_INVESTOR_CODE_U', 'ORIGINATION_YEAR_B1995', 'ORIGINATION_YEAR_1995', 'ORIGINATION_YEAR_1996', 'ORIGINATION_YEAR_1997', 'ORIGINATION_YEAR_1998', 'ORIGINATION_YEAR_1999', 'ORIGINATION_YEAR_2000', 'ORIGINATION_YEAR_2001', 'ORIGINATION_YEAR_2002', 'ORIGINATION_YEAR_2003', 'ORIGINATION_YEAR_2004', 'ORIGINATION_YEAR_2005', 'ORIGINATION_YEAR_2006', 'ORIGINATION_YEAR_2007', 'ORIGINATION_YEAR_2008', 'ORIGINATION_YEAR_2009', 'ORIGINATION_YEAR_2010', 'ORIGINATION_YEAR_2011', 'ORIGINATION_YEAR_2012', 'ORIGINATION_YEAR_2013', 'ORIGINATION_YEAR_2014', 'ORIGINATION_YEAR_2015', 'ORIGINATION_YEAR_2016', 'ORIGINATION_YEAR_2017', 'ORIGINATION_YEAR_2018']\n",
      "2018-11-01 18:39:10,012 - Xworkers_1mill_3 - INFO - Features List: ['MBA_DAYS_DELINQUENT', 'MBA_DAYS_DELINQUENT_NAN', 'CURRENT_INTEREST_RATE', 'CURRENT_INTEREST_RATE_NAN', 'LOANAGE', 'LOANAGE_NAN', 'CURRENT_BALANCE', 'CURRENT_BALANCE_NAN', 'SCHEDULED_PRINCIPAL', 'SCHEDULED_PRINCIPAL_NAN', 'SCHEDULED_MONTHLY_PANDI', 'SCHEDULED_MONTHLY_PANDI_NAN', 'LLMA2_CURRENT_INTEREST_SPREAD', 'LLMA2_CURRENT_INTEREST_SPREAD_NAN', 'LLMA2_C_IN_LAST_12_MONTHS', 'LLMA2_30_IN_LAST_12_MONTHS', 'LLMA2_60_IN_LAST_12_MONTHS', 'LLMA2_90_IN_LAST_12_MONTHS', 'LLMA2_FC_IN_LAST_12_MONTHS', 'LLMA2_REO_IN_LAST_12_MONTHS', 'LLMA2_0_IN_LAST_12_MONTHS', 'LLMA2_HIST_LAST_12_MONTHS_MIS', 'NUM_MODIF', 'NUM_MODIF_NAN', 'P_RATE_TO_MOD', 'P_RATE_TO_MOD_NAN', 'MOD_RATE', 'MOD_RATE_NAN', 'DIF_RATE', 'DIF_RATE_NAN', 'P_MONTHLY_PAY', 'P_MONTHLY_PAY_NAN', 'MOD_MONTHLY_PAY', 'MOD_MONTHLY_PAY_NAN', 'DIF_MONTHLY_PAY', 'DIF_MONTHLY_PAY_NAN', 'CAPITALIZATION_AMT', 'CAPITALIZATION_AMT_NAN', 'MORTGAGE_RATE', 'MORTGAGE_RATE_NAN', 'FICO_SCORE_ORIGINATION', 'INITIAL_INTEREST_RATE', 'ORIGINAL_LTV', 'ORIGINAL_BALANCE', 'BACKEND_RATIO', 'BACKEND_RATIO_NAN', 'ORIGINAL_TERM', 'ORIGINAL_TERM_NAN', 'SALE_PRICE', 'SALE_PRICE_NAN', 'PREPAY_PENALTY_TERM', 'PREPAY_PENALTY_TERM_NAN', 'NUMBER_OF_UNITS', 'NUMBER_OF_UNITS_NAN', 'MARGIN', 'MARGIN_NAN', 'PERIODIC_RATE_CAP', 'PERIODIC_RATE_CAP_NAN', 'PERIODIC_RATE_FLOOR', 'PERIODIC_RATE_FLOOR_NAN', 'LIFETIME_RATE_CAP', 'LIFETIME_RATE_CAP_NAN', 'LIFETIME_RATE_FLOOR', 'LIFETIME_RATE_FLOOR_NAN', 'RATE_RESET_FREQUENCY', 'RATE_RESET_FREQUENCY_NAN', 'PAY_RESET_FREQUENCY', 'PAY_RESET_FREQUENCY_NAN', 'FIRST_RATE_RESET_PERIOD', 'FIRST_RATE_RESET_PERIOD_NAN', 'LLMA2_PRIME', 'LLMA2_SUBPRIME', 'LLMA2_APPVAL_LT_SALEPRICE', 'LLMA2_ORIG_RATE_SPREAD', 'LLMA2_ORIG_RATE_SPREAD_NAN', 'AGI', 'AGI_NAN', 'UR', 'UR_NAN', 'LLMA2_ORIG_RATE_ORIG_MR_SPREAD', 'LLMA2_ORIG_RATE_ORIG_MR_SPREAD_NAN', 'COUNT_INT_RATE_LESS', 'NUM_PRIME_ZIP', 'NUM_PRIME_ZIP_NAN', 'MBA_DELINQUENCY_STATUS_0', 'MBA_DELINQUENCY_STATUS_3', 'MBA_DELINQUENCY_STATUS_6', 'MBA_DELINQUENCY_STATUS_9', 'MBA_DELINQUENCY_STATUS_C', 'MBA_DELINQUENCY_STATUS_F', 'MBA_DELINQUENCY_STATUS_R', 'BUYDOWN_FLAG_N', 'BUYDOWN_FLAG_U', 'BUYDOWN_FLAG_Y', 'NEGATIVE_AMORTIZATION_FLAG_N', 'NEGATIVE_AMORTIZATION_FLAG_U', 'NEGATIVE_AMORTIZATION_FLAG_Y', 'PREPAY_PENALTY_FLAG_N', 'PREPAY_PENALTY_FLAG_U', 'PREPAY_PENALTY_FLAG_Y', 'OCCUPANCY_TYPE_1', 'OCCUPANCY_TYPE_2', 'OCCUPANCY_TYPE_3', 'OCCUPANCY_TYPE_U', 'PRODUCT_TYPE_10', 'PRODUCT_TYPE_20', 'PRODUCT_TYPE_30', 'PRODUCT_TYPE_40', 'PRODUCT_TYPE_50', 'PRODUCT_TYPE_51', 'PRODUCT_TYPE_52', 'PRODUCT_TYPE_53', 'PRODUCT_TYPE_54', 'PRODUCT_TYPE_5A', 'PRODUCT_TYPE_5Z', 'PRODUCT_TYPE_60', 'PRODUCT_TYPE_61', 'PRODUCT_TYPE_62', 'PRODUCT_TYPE_63', 'PRODUCT_TYPE_6Z', 'PRODUCT_TYPE_70', 'PRODUCT_TYPE_80', 'PRODUCT_TYPE_81', 'PRODUCT_TYPE_82', 'PRODUCT_TYPE_83', 'PRODUCT_TYPE_84', 'PRODUCT_TYPE_8Z', 'PRODUCT_TYPE_U', 'PROPERTY_TYPE_1', 'PROPERTY_TYPE_2', 'PROPERTY_TYPE_3', 'PROPERTY_TYPE_4', 'PROPERTY_TYPE_5', 'PROPERTY_TYPE_6', 'PROPERTY_TYPE_7', 'PROPERTY_TYPE_8', 'PROPERTY_TYPE_9', 'PROPERTY_TYPE_M', 'PROPERTY_TYPE_U', 'PROPERTY_TYPE_Z', 'LOAN_PURPOSE_CATEGORY_P', 'LOAN_PURPOSE_CATEGORY_R', 'LOAN_PURPOSE_CATEGORY_U', 'DOCUMENTATION_TYPE_1', 'DOCUMENTATION_TYPE_2', 'DOCUMENTATION_TYPE_3', 'DOCUMENTATION_TYPE_U', 'CHANNEL_1', 'CHANNEL_2', 'CHANNEL_3', 'CHANNEL_4', 'CHANNEL_5', 'CHANNEL_6', 'CHANNEL_7', 'CHANNEL_8', 'CHANNEL_9', 'CHANNEL_A', 'CHANNEL_B', 'CHANNEL_C', 'CHANNEL_D', 'CHANNEL_U', 'LOAN_TYPE_1', 'LOAN_TYPE_2', 'LOAN_TYPE_3', 'LOAN_TYPE_4', 'LOAN_TYPE_5', 'LOAN_TYPE_6', 'LOAN_TYPE_U', 'IO_FLAG_N', 'IO_FLAG_U', 'IO_FLAG_Y', 'CONVERTIBLE_FLAG_N', 'CONVERTIBLE_FLAG_U', 'CONVERTIBLE_FLAG_Y', 'POOL_INSURANCE_FLAG_N', 'POOL_INSURANCE_FLAG_U', 'POOL_INSURANCE_FLAG_Y', 'STATE_AK', 'STATE_AL', 'STATE_AR', 'STATE_AZ', 'STATE_CA', 'STATE_CO', 'STATE_CT', 'STATE_DC', 'STATE_DE', 'STATE_FL', 'STATE_GA', 'STATE_HI', 'STATE_IA', 'STATE_ID', 'STATE_IL', 'STATE_IN', 'STATE_KS', 'STATE_KY', 'STATE_LA', 'STATE_MA', 'STATE_MD', 'STATE_ME', 'STATE_MI', 'STATE_MN', 'STATE_MO', 'STATE_MS', 'STATE_MT', 'STATE_NC', 'STATE_ND', 'STATE_NE', 'STATE_NH', 'STATE_NJ', 'STATE_NM', 'STATE_NV', 'STATE_NY', 'STATE_OH', 'STATE_OK', 'STATE_OR', 'STATE_PA', 'STATE_PR', 'STATE_RI', 'STATE_SC', 'STATE_SD', 'STATE_TN', 'STATE_TX', 'STATE_UT', 'STATE_VA', 'STATE_VT', 'STATE_WA', 'STATE_WI', 'STATE_WV', 'STATE_WY', 'CURRENT_INVESTOR_CODE_240', 'CURRENT_INVESTOR_CODE_250', 'CURRENT_INVESTOR_CODE_253', 'CURRENT_INVESTOR_CODE_U', 'ORIGINATION_YEAR_B1995', 'ORIGINATION_YEAR_1995', 'ORIGINATION_YEAR_1996', 'ORIGINATION_YEAR_1997', 'ORIGINATION_YEAR_1998', 'ORIGINATION_YEAR_1999', 'ORIGINATION_YEAR_2000', 'ORIGINATION_YEAR_2001', 'ORIGINATION_YEAR_2002', 'ORIGINATION_YEAR_2003', 'ORIGINATION_YEAR_2004', 'ORIGINATION_YEAR_2005', 'ORIGINATION_YEAR_2006', 'ORIGINATION_YEAR_2007', 'ORIGINATION_YEAR_2008', 'ORIGINATION_YEAR_2009', 'ORIGINATION_YEAR_2010', 'ORIGINATION_YEAR_2011', 'ORIGINATION_YEAR_2012', 'ORIGINATION_YEAR_2013', 'ORIGINATION_YEAR_2014', 'ORIGINATION_YEAR_2015', 'ORIGINATION_YEAR_2016', 'ORIGINATION_YEAR_2017', 'ORIGINATION_YEAR_2018']\n",
      "2018-11-01 18:39:10,012 - Xworkers_1mill_3 - INFO - Labels List: ['DELINQUENCY_STATUS_NEXT_0', 'DELINQUENCY_STATUS_NEXT_3', 'DELINQUENCY_STATUS_NEXT_6', 'DELINQUENCY_STATUS_NEXT_9', 'DELINQUENCY_STATUS_NEXT_C', 'DELINQUENCY_STATUS_NEXT_F', 'DELINQUENCY_STATUS_NEXT_R']\n",
      "2018-11-01 18:39:10,012 - Xworkers_1mill_3 - INFO - Labels List: ['DELINQUENCY_STATUS_NEXT_0', 'DELINQUENCY_STATUS_NEXT_3', 'DELINQUENCY_STATUS_NEXT_6', 'DELINQUENCY_STATUS_NEXT_9', 'DELINQUENCY_STATUS_NEXT_C', 'DELINQUENCY_STATUS_NEXT_F', 'DELINQUENCY_STATUS_NEXT_R']\n",
      "38500000\n",
      "38500000\n",
      "2018-11-01 18:39:10,013 - Xworkers_1mill_3 - INFO - METRICS:  Namespace(allow_summaries=True, batch_layer_type='batch', batch_norm=True, batch_size=8850, batch_type='layer', data_dir='/input_data', decay_rate=1, decay_step=3520000, display_every=100, dropout=True, dropout_keep=0.9, epoch_flag=0, epoch_num=15, eval=False, ftp_dir='processed/c1mill', learning_rate=0.1, log_file=None, logdir=PosixPath('/home/ubuntu/summ_15ep_2wrk'), loss_tolerance=0.0001, lr_decay_epochs=30, lr_decay_policy='time', lr_decay_rate=0.1, lr_poly_power=2.0, max_epoch_size=-1, momentum=0.5, n_hidden=3, name='Xworkers_1mill', nstep_burnin=20, rate_min=0.0015, reg_rate=1e-05, s_hidden=[200, 140, 140], save_interval=450, stratified_flag=False, summary_interval=1800, test_batch_size=1200000, test_dir='chuncks_random_c1millx2_test', test_flag=True, test_period=[286, 304], total_examples=38500000, train_dir='chuncks_random_c1millx2_train', train_period=[121, 279], valid_batch_size=150000, valid_dir='chuncks_random_c1millx2_valid', valid_period=[280, 285], weighted_sampling=False, xla=True)\n",
      "\n",
      "2018-11-01 18:39:10,013 - Xworkers_1mill_3 - INFO - METRICS:  Namespace(allow_summaries=True, batch_layer_type='batch', batch_norm=True, batch_size=8850, batch_type='layer', data_dir='/input_data', decay_rate=1, decay_step=3520000, display_every=100, dropout=True, dropout_keep=0.9, epoch_flag=0, epoch_num=15, eval=False, ftp_dir='processed/c1mill', learning_rate=0.1, log_file=None, logdir=PosixPath('/home/ubuntu/summ_15ep_2wrk'), loss_tolerance=0.0001, lr_decay_epochs=30, lr_decay_policy='time', lr_decay_rate=0.1, lr_poly_power=2.0, max_epoch_size=-1, momentum=0.5, n_hidden=3, name='Xworkers_1mill', nstep_burnin=20, rate_min=0.0015, reg_rate=1e-05, s_hidden=[200, 140, 140], save_interval=450, stratified_flag=False, summary_interval=1800, test_batch_size=1200000, test_dir='chuncks_random_c1millx2_test', test_flag=True, test_period=[286, 304], total_examples=38500000, train_dir='chuncks_random_c1millx2_train', train_period=[121, 279], valid_batch_size=150000, valid_dir='chuncks_random_c1millx2_valid', valid_period=[280, 285], weighted_sampling=False, xla=True)\n",
      "\n",
      "2018-11-01 18:39:10,013 - Xworkers_1mill_3 - INFO - Number of total steps: 65255\n",
      "2018-11-01 18:39:10,013 - Xworkers_1mill_3 - INFO - Number of total steps: 65255\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-01 18:39:10,534 - Xworkers_1mill_0 - INFO - Writing summaries to /home/ubuntu/summ_15ep_2wrk\n",
      "2018-11-01 18:39:10,534 - Xworkers_1mill_0 - INFO - Writing summaries to /home/ubuntu/summ_15ep_2wrk\n",
      "2018-11-01 18:39:10,534 - Xworkers_1mill_0 - INFO - Training\n",
      "2018-11-01 18:39:10,534 - Xworkers_1mill_0 - INFO - Training\n",
      "2018-11-01 18:39:10,534 - Xworkers_1mill_0 - INFO -   Step; Epoch; time-per-record(sec);  batchtime/worker(sec);  Loss;   Learning Rate; Accuracy; better_acc\n",
      "2018-11-01 18:39:10,534 - Xworkers_1mill_0 - INFO -   Step; Epoch; time-per-record(sec);  batchtime/worker(sec);  Loss;   Learning Rate; Accuracy; better_acc\n",
      "2018-11-01 18:39:10,579 - Xworkers_1mill_1 - INFO - Writing summaries to /home/ubuntu/summ_15ep_2wrk\n",
      "2018-11-01 18:39:10,579 - Xworkers_1mill_1 - INFO - Writing summaries to /home/ubuntu/summ_15ep_2wrk\n",
      "2018-11-01 18:39:10,580 - Xworkers_1mill_1 - INFO - Training\n",
      "2018-11-01 18:39:10,572 - Xworkers_1mill_2 - INFO - Writing summaries to /home/ubuntu/summ_15ep_2wrk\n",
      "2018-11-01 18:39:10,572 - Xworkers_1mill_2 - INFO - Writing summaries to /home/ubuntu/summ_15ep_2wrk\n",
      "2018-11-01 18:39:10,572 - Xworkers_1mill_2 - INFO - Training\n",
      "2018-11-01 18:39:10,580 - Xworkers_1mill_1 - INFO - Training\n",
      "2018-11-01 18:39:10,572 - Xworkers_1mill_2 - INFO - Training\n",
      "2018-11-01 18:39:10,580 - Xworkers_1mill_1 - INFO -   Step; Epoch; time-per-record(sec);  batchtime/worker(sec);  Loss;   Learning Rate; Accuracy; better_acc\n",
      "2018-11-01 18:39:10,572 - Xworkers_1mill_2 - INFO -   Step; Epoch; time-per-record(sec);  batchtime/worker(sec);  Loss;   Learning Rate; Accuracy; better_acc\n",
      "2018-11-01 18:39:10,572 - Xworkers_1mill_2 - INFO -   Step; Epoch; time-per-record(sec);  batchtime/worker(sec);  Loss;   Learning Rate; Accuracy; better_acc\n",
      "2018-11-01 18:39:10,580 - Xworkers_1mill_1 - INFO -   Step; Epoch; time-per-record(sec);  batchtime/worker(sec);  Loss;   Learning Rate; Accuracy; better_acc\n",
      "2018-11-01 18:39:10,611 - Xworkers_1mill_4 - INFO - Writing summaries to /home/ubuntu/summ_15ep_2wrk\n",
      "2018-11-01 18:39:10,611 - Xworkers_1mill_4 - INFO - Writing summaries to /home/ubuntu/summ_15ep_2wrk\n",
      "2018-11-01 18:39:10,580 - Xworkers_1mill_3 - INFO - Writing summaries to /home/ubuntu/summ_15ep_2wrk\n",
      "2018-11-01 18:39:10,580 - Xworkers_1mill_3 - INFO - Writing summaries to /home/ubuntu/summ_15ep_2wrk\n",
      "2018-11-01 18:39:10,612 - Xworkers_1mill_4 - INFO - Training\n",
      "2018-11-01 18:39:10,581 - Xworkers_1mill_3 - INFO - Training\n",
      "2018-11-01 18:39:10,581 - Xworkers_1mill_3 - INFO - Training\n",
      "2018-11-01 18:39:10,612 - Xworkers_1mill_4 - INFO - Training\n",
      "2018-11-01 18:39:10,581 - Xworkers_1mill_3 - INFO -   Step; Epoch; time-per-record(sec);  batchtime/worker(sec);  Loss;   Learning Rate; Accuracy; better_acc\n",
      "2018-11-01 18:39:10,581 - Xworkers_1mill_3 - INFO -   Step; Epoch; time-per-record(sec);  batchtime/worker(sec);  Loss;   Learning Rate; Accuracy; better_acc\n",
      "2018-11-01 18:39:10,612 - Xworkers_1mill_4 - INFO -   Step; Epoch; time-per-record(sec);  batchtime/worker(sec);  Loss;   Learning Rate; Accuracy; better_acc\n",
      "2018-11-01 18:39:10,612 - Xworkers_1mill_4 - INFO -   Step; Epoch; time-per-record(sec);  batchtime/worker(sec);  Loss;   Learning Rate; Accuracy; better_acc\n",
      "df_train: \n",
      " Empty DataFrame\n",
      "Columns: [step, epoch, batch_time, Loss, LogLoss, Accuracy, Better-Accuracy, M-Measure Mean, AUC_AOC Mean, AUC_PR Mean]\n",
      "Index: []\n",
      "df_valid: \n",
      " Empty DataFrame\n",
      "Columns: [step, epoch, batch_time, Loss, LogLoss, Accuracy, Better-Accuracy]\n",
      "Index: []\n",
      "ip-172-31-38-14:1631:1642 [0] INFO NET : Using interface eth0:172.31.38.14<0>\n",
      "ip-172-31-38-14:1631:1642 [0] INFO NET/IB : Using interface eth0 for sideband communication\n",
      "ip-172-31-38-14:1631:1642 [0] INFO Using internal Network Socket\n",
      "ip-172-31-38-14:1631:1642 [0] INFO Using NCCL Low-latency algorithm for sizes below 16384\n",
      "ip-172-31-38-14:1631:1642 [0] INFO NET : Using interface eth0:172.31.38.14<0>\n",
      "ip-172-31-38-14:1631:1642 [0] INFO NET/Socket : 1 interfaces found\n",
      "NCCL version 2.1.2+cuda9.0\n",
      "ip-172-31-45-114:9200:9211 [0] INFO NET : Using interface eth0:172.31.45.114<0>\n",
      "ip-172-31-45-114:9200:9211 [0] INFO NET/IB : Using interface eth0 for sideband communication\n",
      "ip-172-31-45-114:9200:9211 [0] INFO Using internal Network Socket\n",
      "ip-172-31-45-114:9200:9211 [0] INFO Using NCCL Low-latency algorithm for sizes below 16384\n",
      "ip-172-31-42-56:8773:8784 [0] INFO NET : Using interface eth0:172.31.42.56<0>\n",
      "ip-172-31-42-56:8773:8784 [0] INFO NET/IB : Using interface eth0 for sideband communication\n",
      "ip-172-31-42-56:8773:8784 [0] INFO Using internal Network Socket\n",
      "ip-172-31-42-56:8773:8784 [0] INFO Using NCCL Low-latency algorithm for sizes below 16384\n",
      "ip-172-31-41-175:8944:8955 [0] INFO NET : Using interface eth0:172.31.41.175<0>\n",
      "ip-172-31-41-175:8944:8955 [0] INFO NET/IB : Using interface eth0 for sideband communication\n",
      "ip-172-31-41-175:8944:8955 [0] INFO Using internal Network Socket\n",
      "ip-172-31-41-175:8944:8955 [0] INFO Using NCCL Low-latency algorithm for sizes below 16384\n",
      "ip-172-31-34-193:8627:8638 [0] INFO NET : Using interface eth0:172.31.34.193<0>\n",
      "ip-172-31-34-193:8627:8638 [0] INFO NET/IB : Using interface eth0 for sideband communication\n",
      "ip-172-31-34-193:8627:8638 [0] INFO Using internal Network Socket\n",
      "ip-172-31-34-193:8627:8638 [0] INFO Using NCCL Low-latency algorithm for sizes below 16384\n",
      "ip-172-31-41-175:8944:8955 [0] INFO NET : Using interface eth0:172.31.41.175<0>\n",
      "ip-172-31-41-175:8944:8955 [0] INFO NET/Socket : 1 interfaces found\n",
      "ip-172-31-45-114:9200:9211 [0] INFO NET : Using interface eth0:172.31.45.114<0>\n",
      "ip-172-31-45-114:9200:9211 [0] INFO NET/Socket : 1 interfaces found\n",
      "ip-172-31-42-56:8773:8784 [0] INFO NET : Using interface eth0:172.31.42.56<0>\n",
      "ip-172-31-42-56:8773:8784 [0] INFO NET/Socket : 1 interfaces found\n",
      "ip-172-31-34-193:8627:8638 [0] INFO NET : Using interface eth0:172.31.34.193<0>\n",
      "ip-172-31-34-193:8627:8638 [0] INFO NET/Socket : 1 interfaces found\n",
      "ip-172-31-38-14:1631:1642 [0] INFO Using 512 threads\n",
      "ip-172-31-38-14:1631:1642 [0] INFO Min Comp Cap 3\n",
      "ip-172-31-38-14:1631:1642 [0] INFO NCCL_SINGLE_RING_THRESHOLD=131072\n",
      "ip-172-31-38-14:1631:1642 [0] INFO [0] Ring 0 :    0   1   2   3   4\n",
      "ip-172-31-38-14:1631:1642 [0] INFO 4 -> 0 via NET/Socket/0\n",
      "ip-172-31-42-56:8773:8784 [0] INFO 1 -> 2 via NET/Socket/0\n",
      "ip-172-31-45-114:9200:9211 [0] INFO 0 -> 1 via NET/Socket/0\n",
      "ip-172-31-34-193:8627:8638 [0] INFO 2 -> 3 via NET/Socket/0\n",
      "ip-172-31-41-175:8944:8955 [0] INFO 3 -> 4 via NET/Socket/0\n",
      "ip-172-31-38-14:1631:1642 [0] INFO Launch mode Parallel\n",
      "2018-11-01 18:39:27,761 - Xworkers_1mill_2 - INFO -  46700;    11; 314079.3;   0.141; 0.18009; 0.49345; 0.94825; 0.54426\n",
      "2018-11-01 18:39:27,761 - Xworkers_1mill_2 - INFO -  46700;    11; 314079.3;   0.141; 0.18009; 0.49345; 0.94825; 0.54426\n",
      "2018-11-01 18:39:27,769 - Xworkers_1mill_1 - INFO -  46700;    11; 315771.1;   0.140; 0.18924; 0.49345; 0.94362; 0.54662\n",
      "2018-11-01 18:39:27,769 - Xworkers_1mill_1 - INFO -  46700;    11; 315771.1;   0.140; 0.18924; 0.49345; 0.94362; 0.54662\n",
      "2018-11-01 18:39:27,770 - Xworkers_1mill_3 - INFO -  46700;    11; 314722.7;   0.141; 0.19461; 0.49345; 0.94497; 0.51000\n",
      "2018-11-01 18:39:27,770 - Xworkers_1mill_3 - INFO -  46700;    11; 314722.7;   0.141; 0.19461; 0.49345; 0.94497; 0.51000\n",
      "2018-11-01 18:39:27,801 - Xworkers_1mill_4 - INFO -  46700;    11; 316674.1;   0.140; 0.18997; 0.49345; 0.94542; 0.51257\n",
      "2018-11-01 18:39:27,801 - Xworkers_1mill_4 - INFO -  46700;    11; 316674.1;   0.140; 0.18997; 0.49345; 0.94542; 0.51257\n",
      "2018-11-01 18:39:27,731 - Xworkers_1mill_0 - INFO -  46700;    11; 313761.3;   0.141; 0.19560; 0.49345; 0.94237; 0.47457\n",
      "2018-11-01 18:39:27,731 - Xworkers_1mill_0 - INFO -  46700;    11; 313761.3;   0.141; 0.19560; 0.49345; 0.94237; 0.47457\n",
      "2018-11-01 18:39:43,130 - Xworkers_1mill_2 - INFO -  46800;    11; 297159.7;   0.149; 0.17815; 0.49344; 0.94859; 0.56406\n",
      "2018-11-01 18:39:43,130 - Xworkers_1mill_2 - INFO -  46800;    11; 297159.7;   0.149; 0.17815; 0.49344; 0.94859; 0.56406\n",
      "2018-11-01 18:39:43,138 - Xworkers_1mill_3 - INFO -  46800;    11; 296489.9;   0.149; 0.18834; 0.49344; 0.94780; 0.56449\n",
      "2018-11-01 18:39:43,138 - Xworkers_1mill_3 - INFO -  46800;    11; 296489.9;   0.149; 0.18834; 0.49344; 0.94780; 0.56449\n",
      "2018-11-01 18:39:43,101 - Xworkers_1mill_0 - INFO -  46800;    11; 295887.7;   0.150; 0.17976; 0.49344; 0.94983; 0.55534\n",
      "2018-11-01 18:39:43,101 - Xworkers_1mill_0 - INFO -  46800;    11; 295887.7;   0.150; 0.17976; 0.49344; 0.94983; 0.55534\n",
      "2018-11-01 18:39:43,170 - Xworkers_1mill_4 - INFO -  46800;    11; 296984.3;   0.149; 0.17864; 0.49344; 0.94927; 0.52147\n",
      "2018-11-01 18:39:43,170 - Xworkers_1mill_4 - INFO -  46800;    11; 296984.3;   0.149; 0.17864; 0.49344; 0.94927; 0.52147\n",
      "2018-11-01 18:39:43,138 - Xworkers_1mill_1 - INFO -  46800;    11; 297097.0;   0.149; 0.17354; 0.49344; 0.94949; 0.50867\n",
      "2018-11-01 18:39:43,138 - Xworkers_1mill_1 - INFO -  46800;    11; 297097.0;   0.149; 0.17354; 0.49344; 0.94949; 0.50867\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-01 18:39:58,356 - Xworkers_1mill_0 - INFO -  46900;    11; 322707.2;   0.137; 0.18418; 0.49343; 0.94463; 0.53382\n",
      "2018-11-01 18:39:58,356 - Xworkers_1mill_0 - INFO -  46900;    11; 322707.2;   0.137; 0.18418; 0.49343; 0.94463; 0.53382\n",
      "2018-11-01 18:39:58,391 - Xworkers_1mill_1 - INFO -  46900;    11; 322159.9;   0.137; 0.17841; 0.49343; 0.94893; 0.55627\n",
      "2018-11-01 18:39:58,391 - Xworkers_1mill_1 - INFO -  46900;    11; 322159.9;   0.137; 0.17841; 0.49343; 0.94893; 0.55627\n",
      "2018-11-01 18:39:58,423 - Xworkers_1mill_4 - INFO -  46900;    11; 321636.3;   0.138; 0.19887; 0.49343; 0.94328; 0.54450\n",
      "2018-11-01 18:39:58,423 - Xworkers_1mill_4 - INFO -  46900;    11; 321636.3;   0.138; 0.19887; 0.49343; 0.94328; 0.54450\n",
      "2018-11-01 18:39:58,383 - Xworkers_1mill_2 - INFO -  46900;    11; 322848.1;   0.137; 0.18565; 0.49343; 0.94678; 0.54561\n",
      "2018-11-01 18:39:58,383 - Xworkers_1mill_2 - INFO -  46900;    11; 322848.1;   0.137; 0.18565; 0.49343; 0.94678; 0.54561\n",
      "2018-11-01 18:39:58,392 - Xworkers_1mill_3 - INFO -  46900;    11; 323523.4;   0.137; 0.16458; 0.49343; 0.95119; 0.56528\n",
      "2018-11-01 18:39:58,392 - Xworkers_1mill_3 - INFO -  46900;    11; 323523.4;   0.137; 0.16458; 0.49343; 0.95119; 0.56528\n",
      "2018-11-01 18:40:13,707 - Xworkers_1mill_0 - INFO -  47000;    11; 297698.3;   0.149; 0.20560; 0.49341; 0.94486; 0.55834\n",
      "2018-11-01 18:40:13,741 - Xworkers_1mill_3 - INFO -  47000;    11; 299008.2;   0.148; 0.19037; 0.49341; 0.94610; 0.54681\n",
      "2018-11-01 18:40:13,741 - Xworkers_1mill_3 - INFO -  47000;    11; 299008.2;   0.148; 0.19037; 0.49341; 0.94610; 0.54681\n",
      "2018-11-01 18:40:13,707 - Xworkers_1mill_0 - INFO -  47000;    11; 297698.3;   0.149; 0.20560; 0.49341; 0.94486; 0.55834\n",
      "2018-11-01 18:40:13,773 - Xworkers_1mill_4 - INFO -  47000;    11; 299608.6;   0.148; 0.18276; 0.49341; 0.94949; 0.55559\n",
      "2018-11-01 18:40:13,773 - Xworkers_1mill_4 - INFO -  47000;    11; 299608.6;   0.148; 0.18276; 0.49341; 0.94949; 0.55559\n",
      "2018-11-01 18:40:13,733 - Xworkers_1mill_2 - INFO -  47000;    11; 295127.4;   0.150; 0.18166; 0.49341; 0.94768; 0.57987\n",
      "2018-11-01 18:40:13,733 - Xworkers_1mill_2 - INFO -  47000;    11; 295127.4;   0.150; 0.18166; 0.49341; 0.94768; 0.57987\n",
      "2018-11-01 18:40:13,741 - Xworkers_1mill_1 - INFO -  47000;    11; 297525.6;   0.149; 0.19397; 0.49341; 0.94418; 0.51799\n",
      "2018-11-01 18:40:13,741 - Xworkers_1mill_1 - INFO -  47000;    11; 297525.6;   0.149; 0.19397; 0.49341; 0.94418; 0.51799\n",
      "2018-11-01 18:40:29,210 - Xworkers_1mill_2 - INFO -  47100;    11; 311757.5;   0.142; 0.20329; 0.49340; 0.93944; 0.56558\n",
      "2018-11-01 18:40:29,210 - Xworkers_1mill_2 - INFO -  47100;    11; 311757.5;   0.142; 0.20329; 0.49340; 0.93944; 0.56558\n",
      "2018-11-01 18:40:29,219 - Xworkers_1mill_1 - INFO -  47100;    11; 309638.0;   0.143; 0.18339; 0.49340; 0.94667; 0.54079\n",
      "2018-11-01 18:40:29,219 - Xworkers_1mill_1 - INFO -  47100;    11; 309638.0;   0.143; 0.18339; 0.49340; 0.94667; 0.54079\n",
      "2018-11-01 18:40:29,219 - Xworkers_1mill_3 - INFO -  47100;    11; 309737.7;   0.143; 0.18223; 0.49340; 0.94542; 0.52784\n",
      "2018-11-01 18:40:29,219 - Xworkers_1mill_3 - INFO -  47100;    11; 309737.7;   0.143; 0.18223; 0.49340; 0.94542; 0.52784\n",
      "2018-11-01 18:40:29,188 - Xworkers_1mill_0 - INFO -  47100;    11; 305806.5;   0.145; 0.17895; 0.49340; 0.94870; 0.55325\n",
      "2018-11-01 18:40:29,188 - Xworkers_1mill_0 - INFO -  47100;    11; 305806.5;   0.145; 0.17895; 0.49340; 0.94870; 0.55325\n",
      "2018-11-01 18:40:29,252 - Xworkers_1mill_4 - INFO -  47100;    11; 307234.1;   0.144; 0.19264; 0.49340; 0.94362; 0.51995\n",
      "2018-11-01 18:40:29,252 - Xworkers_1mill_4 - INFO -  47100;    11; 307234.1;   0.144; 0.19264; 0.49340; 0.94362; 0.51995\n",
      "2018-11-01 18:40:44,582 - Xworkers_1mill_0 - INFO -  47200;    11; 294924.8;   0.150; 0.16806; 0.49338; 0.95277; 0.54745\n",
      "2018-11-01 18:40:44,612 - Xworkers_1mill_1 - INFO -  47200;    11; 295760.9;   0.150; 0.18989; 0.49338; 0.94305; 0.54055\n",
      "2018-11-01 18:40:44,612 - Xworkers_1mill_1 - INFO -  47200;    11; 295760.9;   0.150; 0.18989; 0.49338; 0.94305; 0.54055\n",
      "2018-11-01 18:40:44,582 - Xworkers_1mill_0 - INFO -  47200;    11; 294924.8;   0.150; 0.16806; 0.49338; 0.95277; 0.54745\n",
      "2018-11-01 18:40:44,604 - Xworkers_1mill_2 - INFO -  47200;    11; 294322.4;   0.150; 0.18959; 0.49338; 0.94610; 0.53596\n",
      "2018-11-01 18:40:44,604 - Xworkers_1mill_2 - INFO -  47200;    11; 294322.4;   0.150; 0.18959; 0.49338; 0.94610; 0.53596\n",
      "2018-11-01 18:40:44,612 - Xworkers_1mill_3 - INFO -  47200;    11; 294671.5;   0.150; 0.17146; 0.49338; 0.94915; 0.60509\n",
      "2018-11-01 18:40:44,612 - Xworkers_1mill_3 - INFO -  47200;    11; 294671.5;   0.150; 0.17146; 0.49338; 0.94915; 0.60509\n",
      "2018-11-01 18:40:44,644 - Xworkers_1mill_4 - INFO -  47200;    11; 294486.3;   0.150; 0.18595; 0.49338; 0.94621; 0.53494\n",
      "2018-11-01 18:40:44,644 - Xworkers_1mill_4 - INFO -  47200;    11; 294486.3;   0.150; 0.18595; 0.49338; 0.94621; 0.53494\n",
      "2018-11-01 18:40:59,737 - Xworkers_1mill_2 - INFO -  47300;    11; 317174.7;   0.140; 0.17713; 0.49337; 0.94734; 0.54489\n",
      "2018-11-01 18:40:59,737 - Xworkers_1mill_2 - INFO -  47300;    11; 317174.7;   0.140; 0.17713; 0.49337; 0.94734; 0.54489\n",
      "2018-11-01 18:40:59,746 - Xworkers_1mill_3 - INFO -  47300;    11; 317298.3;   0.139; 0.18584; 0.49337; 0.94531; 0.51733\n",
      "2018-11-01 18:40:59,746 - Xworkers_1mill_3 - INFO -  47300;    11; 317298.3;   0.139; 0.18584; 0.49337; 0.94531; 0.51733\n",
      "2018-11-01 18:40:59,718 - Xworkers_1mill_0 - INFO -  47300;    11; 312177.0;   0.142; 0.21635; 0.49337; 0.93492; 0.56421\n",
      "2018-11-01 18:40:59,718 - Xworkers_1mill_0 - INFO -  47300;    11; 312177.0;   0.142; 0.21635; 0.49337; 0.93492; 0.56421\n",
      "2018-11-01 18:40:59,746 - Xworkers_1mill_1 - INFO -  47300;    11; 313957.1;   0.141; 0.19778; 0.49337; 0.94260; 0.52910\n",
      "2018-11-01 18:40:59,746 - Xworkers_1mill_1 - INFO -  47300;    11; 313957.1;   0.141; 0.19778; 0.49337; 0.94260; 0.52910\n",
      "2018-11-01 18:40:59,778 - Xworkers_1mill_4 - INFO -  47300;    11; 315285.1;   0.140; 0.18867; 0.49337; 0.94644; 0.58074\n",
      "2018-11-01 18:40:59,778 - Xworkers_1mill_4 - INFO -  47300;    11; 315285.1;   0.140; 0.18867; 0.49337; 0.94644; 0.58074\n",
      "2018-11-01 18:41:14,845 - Xworkers_1mill_2 - INFO -  47400;    11; 316436.6;   0.140; 0.20298; 0.49336; 0.94147; 0.55414\n",
      "2018-11-01 18:41:14,845 - Xworkers_1mill_2 - INFO -  47400;    11; 316436.6;   0.140; 0.20298; 0.49336; 0.94147; 0.55414\n",
      "2018-11-01 18:41:14,854 - Xworkers_1mill_1 - INFO -  47400;    11; 314538.7;   0.141; 0.19643; 0.49336; 0.94237; 0.57733\n",
      "2018-11-01 18:41:14,854 - Xworkers_1mill_1 - INFO -  47400;    11; 314538.7;   0.141; 0.19643; 0.49336; 0.94237; 0.57733\n",
      "2018-11-01 18:41:14,854 - Xworkers_1mill_3 - INFO -  47400;    11; 317514.4;   0.139; 0.17292; 0.49336; 0.95028; 0.55444\n",
      "2018-11-01 18:41:14,854 - Xworkers_1mill_3 - INFO -  47400;    11; 317514.4;   0.139; 0.17292; 0.49336; 0.95028; 0.55444\n",
      "2018-11-01 18:41:14,827 - Xworkers_1mill_0 - INFO -  47400;    11; 312062.0;   0.142; 0.19864; 0.49336; 0.94305; 0.52462\n",
      "2018-11-01 18:41:14,827 - Xworkers_1mill_0 - INFO -  47400;    11; 312062.0;   0.142; 0.19864; 0.49336; 0.94305; 0.52462\n",
      "2018-11-01 18:41:14,887 - Xworkers_1mill_4 - INFO -  47400;    11; 312177.5;   0.142; 0.18850; 0.49336; 0.94712; 0.55466\n",
      "2018-11-01 18:41:14,887 - Xworkers_1mill_4 - INFO -  47400;    11; 312177.5;   0.142; 0.18850; 0.49336; 0.94712; 0.55466\n",
      "2018-11-01 18:41:30,121 - Xworkers_1mill_2 - INFO -  47500;    11; 276984.6;   0.160; 0.18873; 0.49334; 0.94610; 0.55888\n",
      "2018-11-01 18:41:30,121 - Xworkers_1mill_2 - INFO -  47500;    11; 276984.6;   0.160; 0.18873; 0.49334; 0.94610; 0.55888\n",
      "2018-11-01 18:41:30,130 - Xworkers_1mill_3 - INFO -  47500;    11; 276279.1;   0.160; 0.19270; 0.49334; 0.94497; 0.51899\n",
      "2018-11-01 18:41:30,105 - Xworkers_1mill_0 - INFO -  47500;    11; 274791.8;   0.161; 0.19440; 0.49334; 0.94621; 0.54174\n",
      "2018-11-01 18:41:30,105 - Xworkers_1mill_0 - INFO -  47500;    11; 274791.8;   0.161; 0.19440; 0.49334; 0.94621; 0.54174\n",
      "2018-11-01 18:41:30,131 - Xworkers_1mill_1 - INFO -  47500;    11; 274568.2;   0.161; 0.16489; 0.49334; 0.95220; 0.52174\n",
      "2018-11-01 18:41:30,131 - Xworkers_1mill_1 - INFO -  47500;    11; 274568.2;   0.161; 0.16489; 0.49334; 0.95220; 0.52174\n",
      "2018-11-01 18:41:30,130 - Xworkers_1mill_3 - INFO -  47500;    11; 276279.1;   0.160; 0.19270; 0.49334; 0.94497; 0.51899\n",
      "2018-11-01 18:41:30,163 - Xworkers_1mill_4 - INFO -  47500;    11; 275701.3;   0.160; 0.18095; 0.49334; 0.94859; 0.55355\n",
      "2018-11-01 18:41:30,163 - Xworkers_1mill_4 - INFO -  47500;    11; 275701.3;   0.160; 0.18095; 0.49334; 0.94859; 0.55355\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-01 18:41:45,353 - Xworkers_1mill_2 - INFO -  47600;    11; 302775.3;   0.146; 0.18045; 0.49333; 0.94723; 0.55181\n",
      "2018-11-01 18:41:45,353 - Xworkers_1mill_2 - INFO -  47600;    11; 302775.3;   0.146; 0.18045; 0.49333; 0.94723; 0.55181\n",
      "2018-11-01 18:41:45,362 - Xworkers_1mill_1 - INFO -  47600;    11; 302891.5;   0.146; 0.19169; 0.49333; 0.94508; 0.56668\n",
      "2018-11-01 18:41:45,362 - Xworkers_1mill_1 - INFO -  47600;    11; 302891.5;   0.146; 0.19169; 0.49333; 0.94508; 0.56668\n",
      "2018-11-01 18:41:45,338 - Xworkers_1mill_0 - INFO -  47600;    11; 302981.4;   0.146; 0.18492; 0.49333; 0.94350; 0.48731\n",
      "2018-11-01 18:41:45,338 - Xworkers_1mill_0 - INFO -  47600;    11; 302981.4;   0.146; 0.18492; 0.49333; 0.94350; 0.48731\n",
      "2018-11-01 18:41:45,363 - Xworkers_1mill_3 - INFO -  47600;    11; 303551.3;   0.146; 0.17953; 0.49333; 0.94621; 0.58214\n",
      "2018-11-01 18:41:45,363 - Xworkers_1mill_3 - INFO -  47600;    11; 303551.3;   0.146; 0.17953; 0.49333; 0.94621; 0.58214\n",
      "2018-11-01 18:41:45,395 - Xworkers_1mill_4 - INFO -  47600;    11; 304215.1;   0.145; 0.19405; 0.49333; 0.94452; 0.54641\n",
      "2018-11-01 18:41:45,395 - Xworkers_1mill_4 - INFO -  47600;    11; 304215.1;   0.145; 0.19405; 0.49333; 0.94452; 0.54641\n",
      "2018-11-01 18:42:00,478 - Xworkers_1mill_1 - INFO -  47700;    11; 305133.3;   0.145; 0.18503; 0.49332; 0.94576; 0.52990\n",
      "2018-11-01 18:42:00,478 - Xworkers_1mill_1 - INFO -  47700;    11; 305133.3;   0.145; 0.18503; 0.49332; 0.94576; 0.52990\n",
      "2018-11-01 18:42:00,469 - Xworkers_1mill_2 - INFO -  47700;    11; 302636.1;   0.146; 0.17966; 0.49332; 0.94859; 0.51026\n",
      "2018-11-01 18:42:00,469 - Xworkers_1mill_2 - INFO -  47700;    11; 302636.1;   0.146; 0.17966; 0.49332; 0.94859; 0.51026\n",
      "2018-11-01 18:42:00,456 - Xworkers_1mill_0 - INFO -  47700;    11; 302506.9;   0.146; 0.18848; 0.49332; 0.94531; 0.55226\n",
      "2018-11-01 18:42:00,456 - Xworkers_1mill_0 - INFO -  47700;    11; 302506.9;   0.146; 0.18848; 0.49332; 0.94531; 0.55226\n",
      "2018-11-01 18:42:00,512 - Xworkers_1mill_4 - INFO -  47700;    11; 302199.5;   0.146; 0.18668; 0.49332; 0.94757; 0.53867\n",
      "2018-11-01 18:42:00,512 - Xworkers_1mill_4 - INFO -  47700;    11; 302199.5;   0.146; 0.18668; 0.49332; 0.94757; 0.53867\n",
      "2018-11-01 18:42:00,479 - Xworkers_1mill_3 - INFO -  47700;    11; 302296.5;   0.146; 0.18508; 0.49332; 0.94678; 0.56627\n",
      "2018-11-01 18:42:00,479 - Xworkers_1mill_3 - INFO -  47700;    11; 302296.5;   0.146; 0.18508; 0.49332; 0.94678; 0.56627\n",
      "2018-11-01 18:42:15,585 - Xworkers_1mill_0 - INFO -  47800;    11; 280610.8;   0.158; 0.19129; 0.49330; 0.94260; 0.55160\n",
      "2018-11-01 18:42:15,585 - Xworkers_1mill_0 - INFO -  47800;    11; 280610.8;   0.158; 0.19129; 0.49330; 0.94260; 0.55160\n",
      "2018-11-01 18:42:15,607 - Xworkers_1mill_1 - INFO -  47800;    11; 281679.3;   0.157; 0.18502; 0.49330; 0.94667; 0.52739\n",
      "2018-11-01 18:42:15,607 - Xworkers_1mill_1 - INFO -  47800;    11; 281679.3;   0.157; 0.18502; 0.49330; 0.94667; 0.52739\n",
      "2018-11-01 18:42:15,639 - Xworkers_1mill_4 - INFO -  47800;    11; 282248.6;   0.157; 0.18189; 0.49330; 0.94746; 0.49523\n",
      "2018-11-01 18:42:15,639 - Xworkers_1mill_4 - INFO -  47800;    11; 282248.6;   0.157; 0.18189; 0.49330; 0.94746; 0.49523\n",
      "2018-11-01 18:42:15,598 - Xworkers_1mill_2 - INFO -  47800;    11; 282161.5;   0.157; 0.19003; 0.49330; 0.94565; 0.56916\n",
      "2018-11-01 18:42:15,598 - Xworkers_1mill_2 - INFO -  47800;    11; 282161.5;   0.157; 0.19003; 0.49330; 0.94565; 0.56916\n",
      "2018-11-01 18:42:15,607 - Xworkers_1mill_3 - INFO -  47800;    11; 279869.5;   0.158; 0.18741; 0.49330; 0.94701; 0.52614\n",
      "2018-11-01 18:42:15,607 - Xworkers_1mill_3 - INFO -  47800;    11; 279869.5;   0.158; 0.18741; 0.49330; 0.94701; 0.52614\n"
     ]
    }
   ],
   "source": [
    "!mpirun -np 5 -H ec2-54-152-110-152.compute-1.amazonaws.com,ec2-52-202-218-55.compute-1.amazonaws.com,ec2-18-206-226-135.compute-1.amazonaws.com,ec2-52-23-164-180.compute-1.amazonaws.com,ec2-52-70-194-2.compute-1.amazonaws.com  --mca plm_rsh_no_tree_spawn 1 --prefix /usr/local/mpi --bind-to none --map-by slot -x NCCL_DEBUG=INFO -x NCCL_MIN_NRINGS=2 -x LD_LIBRARY_PATH -x PATH  -mca pml ob1 -mca btl ^openib python nn_real_hvd-ntb-v5-batch_size.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow_p36]",
   "language": "python",
   "name": "conda-env-tensorflow_p36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
