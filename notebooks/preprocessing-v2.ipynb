{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel package will be renamed from release 2.8; in order to keep installing from binary please use \"pip install psycopg2-binary\" instead. For details see: <http://initd.org/psycopg/docs/install.html#binary-install-from-pypi>.\n",
      "  \"\"\")\n",
      "2018-12-18 16:13:24,356 - matplotlib - DEBUG - CACHEDIR=/home/ubuntu/.cache/matplotlib\n",
      "2018-12-18 16:13:24,361 - matplotlib.font_manager - DEBUG - Using fontManager instance from /home/ubuntu/.cache/matplotlib/fontList.json\n",
      "2018-12-18 16:13:24,497 - matplotlib.backends - DEBUG - backend module://ipykernel.pylab.backend_inline version unknown\n",
      "2018-12-18 16:13:24,510 - matplotlib.backends - DEBUG - backend module://ipykernel.pylab.backend_inline version unknown\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/MLMortgage/data/raw /home/ubuntu/MLMortgage/data/processed\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "import os\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "import datetime\n",
    "import glob\n",
    "from os.path import abspath\n",
    "from pathlib import Path\n",
    "from inspect import getsourcefile\n",
    "from datetime import datetime\n",
    "import math\n",
    "import argparse\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "nb_dir = os.path.join(Path(os.getcwd()).parents[0], 'src', 'data')\n",
    "if nb_dir not in sys.path:\n",
    "    sys.path.insert(0, nb_dir)\n",
    "import get_raw_data as grd\n",
    "import data_classes\n",
    "import Normalizer\n",
    "\n",
    "DT_FLOAT = np.float32 \n",
    "DT_BOOL = np.uint8\n",
    "RANDOM_SEED = 123\n",
    "logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "# logger.propagate = False # it will not log to console.\n",
    "\n",
    "RAW_DIR = os.path.join(Path(os.getcwd()).parents[0], 'data', 'raw') \n",
    "PRO_DIR = os.path.join(Path(os.getcwd()).parents[0], 'data', 'processed')\n",
    "\n",
    "print(RAW_DIR, PRO_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parser(parser):\n",
    "    \"\"\"Parse the arguments from the CLI and update the parser.\"\"\"    \n",
    "    parser.add_argument(\n",
    "        '--prepro_step',\n",
    "        type=str,\n",
    "        default='preprocessing', #'slicing', 'preprocessing'\n",
    "        help='To execute a preprocessing method')    \n",
    "    #this is for allfeatures_preprocessing:\n",
    "    parser.add_argument(\n",
    "        '--train_period',\n",
    "        type=int,\n",
    "        nargs='*',\n",
    "        default=[121,323], #[121,279], #[156, 180], [121,143],  # 279],\n",
    "        help='Training Period')\n",
    "    parser.add_argument(\n",
    "        '--valid_period',\n",
    "        type=int,\n",
    "        nargs='*',\n",
    "        default=[324,329], #[280,285], #[181,185], [144,147],\n",
    "        help='Validation Period')    \n",
    "    parser.add_argument(\n",
    "        '--test_period',\n",
    "        type=int,\n",
    "        nargs='*',\n",
    "        default=[330, 342], #[286, 304], # [186,191], [148, 155],\n",
    "        help='Testing Period')    \n",
    "    parser.add_argument(\n",
    "        '--prepro_dir',\n",
    "        type=str,\n",
    "        default='chuncks_random_c1mill',\n",
    "        help='Directory with raw data inside data/raw/ and it will be the output directory inside data/processed/')    \n",
    "    parser.add_argument(\n",
    "        '--prepro_chunksize',\n",
    "        type=int,\n",
    "        default=500000,\n",
    "        help='Chunk size to put into the h5 file...')    \n",
    "    parser.add_argument(\n",
    "        '--prepro_with_index',\n",
    "        type=bool,\n",
    "        default=True,\n",
    "        help='To keep indexes for each record')\n",
    "    parser.add_argument(\n",
    "        '--ref_norm',\n",
    "        type=bool,\n",
    "        default=True,\n",
    "        help='To execute the normalization over the raw inputs')\n",
    "        \n",
    "    return parser.parse_known_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(prepro_chunksize=500000, prepro_dir='chuncks_random_c1mill', prepro_step='preprocessing', prepro_with_index=True, ref_norm=True, test_period=[330, 342], train_period=[121, 323], valid_period=[324, 329])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "FLAGS, UNPARSED = update_parser(argparse.ArgumentParser())    \n",
    "#these are the more important parameters for preprocessing:\n",
    "FLAGS.prepro_dir='chuncks_random_c1mill' #this directory must be the same inside 'raw' and processed directories.\n",
    "FLAGS.prepro_chunksize=500000 \n",
    "FLAGS.train_period=[121,323] #[121,279] #[121, 143] \n",
    "FLAGS.valid_period=[324,329] #[280,285] #[144, 147] \n",
    "FLAGS.test_period=[330,342] #[286,304] #[148, 155]                                                \n",
    "FLAGS.prepro_with_index = True\n",
    "\n",
    "print(FLAGS)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/ubuntu/MLMortgage/data/raw/chuncks_random_c1mill/temporalloandynmodifmrstaticitur_3Trans_0Lab_100th.txt',\n",
       " '/home/ubuntu/MLMortgage/data/raw/chuncks_random_c1mill/temporalloandynmodifmrstaticitur_CTrans_3Lab_100th.txt']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glob.glob(os.path.join(RAW_DIR, FLAGS.prepro_dir,\"*.txt\"))\n",
    "# from IPython.core.debugger import Tracer; Tracer()()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def allfeatures_extract_labels(data, columns='MBA_DELINQUENCY_STATUS_next'):\n",
    "    '''Extract the labels from Dataset, order-and-transform them into one-hot matrix of labels.\n",
    "        Args: \n",
    "            data (DataFrame): Input Dataset which is modified in place.\n",
    "            columns (string): Name of the class column.\n",
    "        Returns: \n",
    "            one-hot matrix of labels of shape: [data.shape[0], 7]. \n",
    "        Raises:        \n",
    "    '''    \n",
    "    logger.name = 'allfeatures_extract_labels'\n",
    "    if (type(columns)==str):\n",
    "         indices = [i for i, elem in enumerate(data.columns) if columns in elem] # (alphabetically ordered)\n",
    "    else:\n",
    "        indices =  columns \n",
    "\n",
    "    if indices:\n",
    "        labels = data[data.columns[indices]]\n",
    "        data.drop(data.columns[indices], axis=1, inplace=True)    \n",
    "        logger.info('...Labels extracted from Dataset...')\n",
    "        return labels\n",
    "    else: return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_chunk(tag, label, chunk, chunk_periods, tag_period, log_file, with_index, tag_index, hdf=None, tfrec=None):\n",
    "    '''Extract records filtering by chunk_periods parameter, define indexes in case of with_index=True, \n",
    "        extract labels and save the results into the target file.\n",
    "        Args: \n",
    "            chunk (DataFrame): Input Dataset which is modified in place.\n",
    "            tag (string): 'train', 'valid' or 'test'\n",
    "            chunk_periods (integer array): an array containing all periods into the chunk.\n",
    "            tag_period (integer array): an array of form [init_period, end_period] for the correspond tag.\n",
    "            log_file (Logger): An object of the log file.\n",
    "            with_index (boolean): If true it will be saved the indexes.\n",
    "            tag_index (int): an index that accumulates the size of the processed chunk. \n",
    "            hdf or tfrec (HDFStore or TFRecords): an object of the target file. Only one must be distint of None.\n",
    "        Returns: \n",
    "            tag_index (int): tag_index updated.\n",
    "        Raises:        \n",
    "    '''    \n",
    "    \n",
    "    inter_periods = list(chunk_periods.intersection(set(range(tag_period[0], tag_period[1]+1))))\n",
    "    log_file.write('Periods corresponding to ' + tag +' period: %s\\r\\n' % str(inter_periods))\n",
    "    p_chunk = chunk.loc[(slice(None), inter_periods), :]\n",
    "    log_file.write('Records for ' + tag +  ' Set - Number of rows: %d\\r\\n' % (p_chunk.shape[0]))\n",
    "    print('Records for ' + tag + ' Set - Number of rows:', p_chunk.shape[0])\n",
    "    if (p_chunk.shape[0] > 0):\n",
    "        if (with_index==True):\n",
    "            # p_chunk.index = pd.MultiIndex.from_tuples([(i, x[1], x[2],x[3]) for x,i in zip(p_chunk.index, range(tag_index, tag_index + p_chunk.shape[0]))])                                \n",
    "            p_chunk.reset_index(inplace=True)\n",
    "            allfeatures_drop_cols(p_chunk, ['PERIOD'])      \n",
    "            p_chunk.set_index('DELINQUENCY_STATUS_NEXT', inplace=True) #1 index                                      \n",
    "        else:\n",
    "            p_chunk.reset_index(drop=True, inplace=True)\n",
    "            \n",
    "        labels = allfeatures_extract_labels(p_chunk, columns=label)\n",
    "        p_chunk = p_chunk.astype(DT_FLOAT)\n",
    "        labels = labels.astype(np.int8)\n",
    "        if (p_chunk.shape[0] != labels.shape[0]) : \n",
    "            print('Error in shapes:', p_chunk.shape, labels.shape)\n",
    "        else :\n",
    "            if (hdf!=None):\n",
    "                hdf.put(tag + '/features', p_chunk, append=True, index=True) #data_columns=p_chunk.columns.values), index=False\n",
    "                hdf.put(tag + '/labels', labels, append=True, index=True) #data_columns=labels.columns.values)                         \n",
    "                hdf.flush()                      \n",
    "            elif (tfrec!=None):\n",
    "                for row, lab in zip(p_chunk.values, labels.values):\n",
    "                    feature = {tag + '/labels': _int64_feature(lab),\n",
    "                               tag + '/features': _float_feature(row)}\n",
    "                    # Create an example protocol buffer\n",
    "                    example = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "                    tfrec.write(example.SerializeToString())                            \n",
    "                tfrec.flush()\n",
    "            tag_index += p_chunk.shape[0]\n",
    "\n",
    "    return tag_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def allfeatures_drop_cols(data, columns):\n",
    "    '''Exclude from the dataset 'data' the descriptive columns as parameters.\n",
    "        Args: \n",
    "            data (DataFrame): Input Dataset which is modified in place.\n",
    "        Returns: \n",
    "            None\n",
    "        Raises:        \n",
    "    '''\n",
    "    logger.name = 'allfeatures_drop_cols'    \n",
    "    data.drop(columns, axis=1, inplace=True)\n",
    "    logger.info('...Columns Excluded from dataset...')\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oneHotDummies_column(column, categories):\n",
    "    '''Convert categorical variable into dummy/indicator variables.\n",
    "    \n",
    "    Args: \n",
    "        column (Series): Input String Categorical Column.\n",
    "    Returns: \n",
    "        DataFrame. Integer Sparse binary matrix of categorical features.\n",
    "    Raises:        \n",
    "    '''    \n",
    "    logger.name = 'oneHotDummies_column: ' +  column.name\n",
    "    cat_column = pd.Categorical(column.astype('str'), categories=categories)\n",
    "    cat_column = pd.get_dummies(cat_column)   # in the same order as categories! (alphabetically ordered) \n",
    "    cat_column = cat_column.add_prefix(column.name + '_')\n",
    "    if (cat_column.isnull().any().any()):\n",
    "        null_cols = cat_column.columns[cat_column.isnull().any()]\n",
    "        print(cat_column[null_cols].isnull().sum())\n",
    "        print(cat_column[cat_column.isnull().any(axis=1)][null_cols].head(50))\n",
    "    return cat_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imputing_nan_values(nan_dict, distribution):        \n",
    "    '''Replace nan values with a value according the nan_dict dictionary and distribution of this feature.\n",
    "        Args: \n",
    "            nan_dict (Dictionary): the key values are the name of features, the values could be a literal or \n",
    "            values belonging to the distribution.\n",
    "            distribution (DataFrame): Contains the median value for numerical features.\n",
    "        Returns: \n",
    "            new_dict (Dictionary): contains the values updated.\n",
    "        Raises:        \n",
    "    '''    \n",
    "    new_dict = {}\n",
    "    for k,v in nan_dict.items():\n",
    "        if v=='median':\n",
    "            new_dict[k] = float(distribution[k+'_MEDIAN'])    \n",
    "        elif v=='mean':\n",
    "            new_dict[k] = float(distribution[k+'_MEAN'])                \n",
    "        else:\n",
    "            new_dict[k] = v\n",
    "            \n",
    "    return new_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_invalid_delinquency_status(data, gflag, log_file):   \n",
    "    '''Delete all subsecuent records of a loan when the feature delinquency_status_next \n",
    "       contains any of the following invalid status: S,T,X or Z.\n",
    "        Args: \n",
    "            data (DataFrame): Input Dataset which is modified in place.\n",
    "            gflag (int): Loan_id of the last loan in previous data, in case this contains some invalid status, \n",
    "            to delete all records inside the current data.\n",
    "            log_file (Logger): An object of the log file.\n",
    "        Returns: \n",
    "            gflag (int): Loan_id of the last loan in current data, in case this contains some invalid status.\n",
    "        Raises:        \n",
    "    '''        \n",
    "    logger.name = 'drop_invalid_delinquency_status'\n",
    "    delinq_ids =  data[data['MBA_DELINQUENCY_STATUS'].isin(['0', 'R', 'S', 'T', 'X', 'Z'])]['LOAN_ID']\n",
    "    groups = data[data['LOAN_ID'].isin(delinq_ids)][['LOAN_ID', 'PERIOD', 'MBA_DELINQUENCY_STATUS', 'DELINQUENCY_STATUS_NEXT']].groupby('LOAN_ID') \n",
    "    groups_list = list(groups)\n",
    "    \n",
    "    iuw= pd.Index([])\n",
    "    \n",
    "    if gflag != '': \n",
    "        try:\n",
    "            iuw= iuw.union(groups.get_group(gflag).index[0:])\n",
    "        except  Exception  as e:\n",
    "            print(str(e))\n",
    "                \n",
    "    if data.iloc[-1]['LOAN_ID'] in groups.groups.keys():\n",
    "        gflag = data.iloc[-1]['LOAN_ID']\n",
    "    else:\n",
    "        gflag = ''\n",
    "                \n",
    "    for k, group in groups_list: \n",
    "        li= group.index[(group['MBA_DELINQUENCY_STATUS'] =='S') | (group['MBA_DELINQUENCY_STATUS'] =='T') \n",
    "                         | (group['MBA_DELINQUENCY_STATUS'] =='X') | (group['MBA_DELINQUENCY_STATUS'] =='Z')].tolist()\n",
    "        if li: iuw= iuw.union(group.index[group.index.get_loc(li[0]):])\n",
    "        # In case of REO or Paid-Off, we need to exclude since the next record:\n",
    "        df_delinq_01 = group[(group['MBA_DELINQUENCY_STATUS'] =='0') | (group['MBA_DELINQUENCY_STATUS'] =='R')]\n",
    "        if df_delinq_01.shape[0]>0: \n",
    "            track_i = df_delinq_01.index[0]\n",
    "            iuw= iuw.union(group.index[group.index.get_loc(track_i)+1:])\n",
    "        \n",
    "    if iuw!=[]:\n",
    "        log_file.write('drop_invalid_delinquency_status - Total rows: %d\\r\\n' % len(iuw)) # (log_df.shape[0])\n",
    "        data.drop(iuw, inplace=True) \n",
    "        logger.info('invalid_delinquency_status dropped')             \n",
    "    \n",
    "    return gflag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zscore(x,mean,stdd):\n",
    "    return (x - mean) / stdd\n",
    "\n",
    "def zscore_apply(dist_file, data):            \n",
    "    stddv_0 = []\n",
    "    nnorm_cols = []\n",
    "    for col_name in data.columns.values:                                \n",
    "        mean = pd.Series(dist_file.iloc[0, np.where(pd.DataFrame(dist_file.columns.values)[0].str.contains(col_name+'_MEAN'))[0]], dtype='float32')    \n",
    "        stddev = dist_file.iloc[0, np.where(pd.DataFrame(dist_file.columns.values)[0].str.contains(col_name+'_STDD'))[0]]    \n",
    "        if not mean.empty and not stddev.empty:  \n",
    "            mean = np.float32(mean.values[0])\n",
    "            stddev = np.float32(stddev.values[0])            \n",
    "            if stddev == 0: \n",
    "                stddv_0.append(col_name)        \n",
    "            else:        \n",
    "                data[col_name] = data[col_name].apply(lambda x: zscore(x, mean, stddev))                        \n",
    "        else: \n",
    "            nnorm_cols.append(col_name)\n",
    "    print('STANDARD DEV zero: ', stddv_0)        \n",
    "    return data, nnorm_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepro_chunk(file_name, file_path, chunksize, label, log_file, nan_cols, categorical_cols, descriptive_cols, time_cols,\n",
    "                 dist_file, with_index, refNorm, train_period, valid_period, test_period, robust_cols, \n",
    "                 minmax_cols=None, hdf=None, tfrec=None, filtering_cols=None):\n",
    "    gflag = ''    \n",
    "    i = 1                  \n",
    "    train_index = 0\n",
    "    valid_index = 0\n",
    "    test_index = 0\n",
    "    for chunk in pd.read_csv(file_path, chunksize = chunksize, sep=',', low_memory=False):    \n",
    "        print('chunk: ', i, ' chunk size: ', chunk.shape[0])\n",
    "        log_file.write('chunk: %d, chunk size: %d \\n' % (i, chunk.shape[0]))\n",
    "        chunk.columns = chunk.columns.str.upper()                            \n",
    "        \n",
    "        log_df = chunk[chunk[label].isnull()]\n",
    "        log_file.write('Dropping Rows with Null Labels - Number of rows: %d\\r\\n' % (log_df.shape[0]))\n",
    "        chunk.drop(chunk.index[chunk[label].isnull()], axis=0, inplace=True)\n",
    "        \n",
    "        log_df = chunk[chunk['INVALID_TRANSITIONS']==1]\n",
    "        log_file.write('Dropping Rows with Invalid Transitions - Number of rows: %d\\r\\n' % (log_df.shape[0]))                                \n",
    "        chunk.drop(chunk.index[chunk['INVALID_TRANSITIONS']==1], axis=0, inplace=True)    \n",
    "        #print('chunk with missing MBA_DELINQUENCY_STATUS', chunk[(chunk['MBA_DELINQUENCY_STATUS']=='') | (chunk['MBA_DELINQUENCY_STATUS'].isna())])\n",
    "        chunk.drop(chunk.index[(chunk['MBA_DELINQUENCY_STATUS'].astype('str')=='')], axis=0, inplace=True) #| (chunk['MBA_DELINQUENCY_STATUS'].isna())        \n",
    "        \n",
    "        gflag = drop_invalid_delinquency_status(chunk, gflag, log_file)               \n",
    "                    \n",
    "        null_columns=chunk.columns[chunk.isnull().any()]\n",
    "        log_df = chunk[chunk.isnull().any(axis=1)][null_columns]\n",
    "        log_file.write('Filling NULL values - (rows, cols) : %d, %d\\r\\n' % (log_df.shape[0], log_df.shape[1]))                    \n",
    "        log_df = chunk[null_columns].isnull().sum().to_frame().reset_index()\n",
    "        log_df.to_csv(log_file, index=False, mode='a')                                    \n",
    "        nan_cols = imputing_nan_values(nan_cols, dist_file)            \n",
    "        chunk.fillna(value=nan_cols, inplace=True)   \n",
    "        \n",
    "        chunk.drop_duplicates(inplace=True) # Follow this instruction!!                        \n",
    "        logger.info('dropping invalid transitions and delinquency status, fill nan values, drop duplicates')                  \n",
    "        log_file.write('Drop duplicates - new size : %d\\r\\n' % (chunk.shape[0]))\n",
    "                               \n",
    "        chunk.reset_index(drop=True, inplace=True)  #don't remove this line! otherwise NaN values appears.\n",
    "        #chunk['ORIGINATION_YEAR'][chunk['ORIGINATION_YEAR']<1995] = \"B1995\"\n",
    "        #chunk['ORIGINATION_YEAR'][(chunk['ORIGINATION_YEAR']<>\"B1995\") & (chunk['ORIGINATION_YEAR']>2018)] = \"nan\"\n",
    "        chunk['ORIGINATION_YEAR'] = chunk['ORIGINATION_YEAR'].apply(lambda x: \"B1995\" if x<1995 else '' if (x>2018 or x is None) else x) #.isna()\n",
    "        for k,v in categorical_cols.items():\n",
    "            # if (chunk[k].dtype=='O'):                \n",
    "            chunk[k] = chunk[k].astype('str')\n",
    "            chunk[k] = chunk[k].str.strip()\n",
    "            chunk[k].replace(['\\.0$'], [''], regex=True,  inplace=True)\n",
    "            new_cols = oneHotDummies_column(chunk[k], v)\n",
    "            if (chunk[k].value_counts().sum()!=new_cols.sum().sum()):\n",
    "                print('Error at categorization, different sizes', k)\n",
    "                print(chunk[k].value_counts(), new_cols.sum())                \n",
    "                log_file.write('Error at categorization, different sizes %s\\r\\n' % str(k))\n",
    "                chunk[new_cols.columns] = new_cols\n",
    "            else:\n",
    "                chunk[new_cols.columns] = new_cols\n",
    "                log_file.write('New columns added: %s\\r\\n' % str(new_cols.columns.values))\n",
    "            \n",
    "                    \n",
    "        allfeatures_drop_cols(chunk, descriptive_cols)                    \n",
    "        #np.savetxt(log_file, descriptive_cols, header='descriptive_cols dropped:', newline=\" \")\n",
    "        log_file.write('descriptive_cols dropped: %s\\r\\n' % str(descriptive_cols))\n",
    "        allfeatures_drop_cols(chunk, time_cols)\n",
    "        #np.savetxt(log_file, time_cols, header='time_cols dropped:', newline=\" \")\n",
    "        log_file.write('time_cols dropped: %s\\r\\n' % str(time_cols))\n",
    "        cat_list = list(categorical_cols.keys())\n",
    "        cat_list.remove('DELINQUENCY_STATUS_NEXT')\n",
    "        #np.savetxt(log_file, cat_list, header='categorical_cols dropped:', newline=\" \")\n",
    "        log_file.write('categorical_cols dropped: %s\\r\\n' % str(cat_list))\n",
    "        allfeatures_drop_cols(chunk, cat_list)\n",
    "\n",
    "        chunk.reset_index(drop=True, inplace=True)  \n",
    "        chunk.set_index(['DELINQUENCY_STATUS_NEXT', 'PERIOD'], append=False, inplace=True) #2 indexes\n",
    "        # np.savetxt(log_file, str(chunk.index.names), header='Indexes created:', newline=\" \")\n",
    "        log_file.write('Indexes created: %s\\r\\n' % str(chunk.index.names))\n",
    "         \n",
    "        if (filtering_cols!=None):\n",
    "            chunk = chunk[filtering_cols]\n",
    "            robust_cols = list(set(robust_cols).intersection(filtering_cols))\n",
    "            log_file.write('Columns Filtered: %s\\r\\n' % str(chunk.columns.values))\n",
    "        \n",
    "        if chunk.isnull().any().any(): \n",
    "            # from IPython.core.debugger import Tracer; Tracer()()\n",
    "            raise ValueError('There are null values...File: ' + file_name)   \n",
    "                        \n",
    "        if (refNorm==True):            \n",
    "            chunk[robust_cols], nnorm_cols =  zscore_apply(dist_file, chunk[robust_cols]) #robust_normalizer.transform(chunk[robust_cols])            \n",
    "            log_file.write('Columns not normalized: %s\\r\\n' % str(nnorm_cols))            \n",
    "            log_file.write('Columns normalized: %s\\r\\n' % str(set(robust_cols)-set(nnorm_cols)))\n",
    "            \n",
    "        \n",
    "        if chunk.isnull().any().any(): raise ValueError('There are null values...File: ' + file_name)       \n",
    "        \n",
    "        chunk_periods = set(list(chunk.index.get_level_values('PERIOD')))\n",
    "        #print(tfrec)\n",
    "        if (tfrec!=None):\n",
    "            train_index = tag_chunk('train', label, chunk, chunk_periods, train_period, log_file, with_index, train_index, tfrec=tfrec[0])\n",
    "            valid_index = tag_chunk('valid', label, chunk, chunk_periods, valid_period, log_file, with_index, valid_index, tfrec=tfrec[1])\n",
    "            test_index = tag_chunk('test', label, chunk, chunk_periods, test_period, log_file, with_index, test_index, tfrec=tfrec[2])\n",
    "            sys.stdout.flush()\n",
    "        elif (hdf!=None):\n",
    "            train_index = tag_chunk('train', label, chunk, chunk_periods, train_period, log_file, with_index, train_index, hdf=hdf[0])\n",
    "            valid_index = tag_chunk('valid', label, chunk, chunk_periods, valid_period, log_file, with_index, valid_index, hdf=hdf[1])\n",
    "            test_index = tag_chunk('test', label, chunk, chunk_periods, test_period, log_file, with_index, test_index, hdf=hdf[2])                \n",
    "        \n",
    "        inter_periods = list(chunk_periods.intersection(set(range(test_period[1]+1,355))))    \n",
    "        log_file.write('Periods greater than test_period: %s\\r\\n' % str(inter_periods))\n",
    "        p_chunk = chunk.loc[(slice(None), inter_periods), :]\n",
    "        log_file.write('Records greater than test_period - Number of rows: %d\\r\\n' % (p_chunk.shape[0]))\n",
    "        \n",
    "        del chunk        \n",
    "        i +=  1   \n",
    "    \n",
    "    return train_index, valid_index, test_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_robust_normalizer(ncols, dist_file, normalizer_type='robust_scaler_sk', center_value='median'):            \n",
    "    norm_cols = []\n",
    "    scales = []\n",
    "    centers = []\n",
    "    scales_0 =[]\n",
    "    for i, x in enumerate (ncols):                        \n",
    "        x_frame = dist_file.iloc[:, np.where(pd.DataFrame(dist_file.columns.values)[0].str.contains(x+'_Q'))[0]]    \n",
    "        if not x_frame.empty and (x_frame.shape[1]>1):       \n",
    "            iqr = float(pd.to_numeric(x_frame[x+'_Q3'], errors='coerce').subtract(pd.to_numeric(x_frame[x+'_Q1'], errors='coerce')))\n",
    "            if iqr == 0: scales_0.append(x)\n",
    "            if iqr!=0: \n",
    "                norm_cols.append(x)                \n",
    "                scales.append(iqr)                    \n",
    "                if center_value == 'median':\n",
    "                    centers.append( float(x_frame[x+'_MEDIAN']) )   \n",
    "                else:\n",
    "                    centers.append( float(x_frame[x+'_Q1']) )                                       \n",
    "    if (normalizer_type == 'robust_scaler_sk'):    \n",
    "        normalizer = RobustScaler()\n",
    "        normalizer.scale_ = scales\n",
    "        normalizer.center_ = centers        \n",
    "    elif (normalizer_type == 'percentile_scaler'):    \n",
    "        normalizer = Normalizer.Normalizer(scales, centers)     \n",
    "    else: normalizer=None                  \n",
    "    \n",
    "    print(scales_0)\n",
    "    \n",
    "    return norm_cols, normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_minmax_normalizer(ncols, scales, dist_file):    \n",
    "    norm_cols = []\n",
    "    minmax_scales = []\n",
    "    centers = []\n",
    "    for i, x in enumerate (ncols):  \n",
    "        x_min = dist_file.iloc[0, np.where(pd.DataFrame(dist_file.columns.values)[0].str.contains(x+'_MIN'))[0]]\n",
    "        x_max = dist_file.iloc[0, np.where(pd.DataFrame(dist_file.columns.values)[0].str.contains(x+'_MAX'))[0]]\n",
    "        if not(x_min.empty) and not(x_max.empty):            \n",
    "            x_min = np.float32(x_min.values[0])\n",
    "            x_max = np.float32(x_max.values[0])\n",
    "            minmax_scales.append(x_max - x_min)                            \n",
    "            centers.append(x_min)\n",
    "            norm_cols.append(x)\n",
    "            # to_delete.append(i)\n",
    "        \n",
    "    normalizer = Normalizer.Normalizer(minmax_scales, centers)         \n",
    "    \n",
    "    return norm_cols, normalizer #, to_delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def allfeatures_preprocessing(RAW_DIR, PRO_DIR, raw_dir, train_period, valid_period, test_period, dividing='percentage', \n",
    "                              chunksize=500000, refNorm=True, with_index=True, output_hdf=True, \n",
    "                              label='DELINQUENCY_STATUS_NEXT', filtering_cols=None):            \n",
    "\n",
    "    descriptive_cols = [\n",
    "    'LOAN_ID',\n",
    "    'ASOFMONTH',        \n",
    "    'PERIOD_NEXT',\n",
    "    'MOD_PER_FROM',\n",
    "    'MOD_PER_TO',\n",
    "    'PROPERTY_ZIP',\n",
    "    'INVALID_TRANSITIONS',\n",
    "    'CONSECUTIVE'\n",
    "    ]\n",
    "\n",
    "    numeric_cols = ['MBA_DAYS_DELINQUENT', 'MBA_DAYS_DELINQUENT_NAN',\n",
    "       'CURRENT_INTEREST_RATE', 'CURRENT_INTEREST_RATE_NAN', 'LOANAGE', 'LOANAGE_NAN',\n",
    "       'CURRENT_BALANCE', 'CURRENT_BALANCE_NAN', 'SCHEDULED_PRINCIPAL',\n",
    "       'SCHEDULED_PRINCIPAL_NAN', 'SCHEDULED_MONTHLY_PANDI',\n",
    "       'SCHEDULED_MONTHLY_PANDI_NAN', \n",
    "       'LLMA2_CURRENT_INTEREST_SPREAD', 'LLMA2_CURRENT_INTEREST_SPREAD_NAN',  \n",
    "       'LLMA2_C_IN_LAST_12_MONTHS',\n",
    "       'LLMA2_30_IN_LAST_12_MONTHS', 'LLMA2_60_IN_LAST_12_MONTHS',\n",
    "       'LLMA2_90_IN_LAST_12_MONTHS', 'LLMA2_FC_IN_LAST_12_MONTHS',\n",
    "       'LLMA2_REO_IN_LAST_12_MONTHS', 'LLMA2_0_IN_LAST_12_MONTHS',       \n",
    "       'NUM_MODIF', 'NUM_MODIF_NAN', 'P_RATE_TO_MOD', 'P_RATE_TO_MOD_NAN', 'MOD_RATE',\n",
    "       'MOD_RATE_NAN', 'DIF_RATE', 'DIF_RATE_NAN', 'P_MONTHLY_PAY',\n",
    "       'P_MONTHLY_PAY_NAN', 'MOD_MONTHLY_PAY', 'MOD_MONTHLY_PAY_NAN',\n",
    "       'DIF_MONTHLY_PAY', 'DIF_MONTHLY_PAY_NAN', 'CAPITALIZATION_AMT',\n",
    "       'CAPITALIZATION_AMT_NAN', 'MORTGAGE_RATE', 'MORTGAGE_RATE_NAN',\n",
    "       'FICO_SCORE_ORIGINATION', 'INITIAL_INTEREST_RATE', 'ORIGINAL_LTV',\n",
    "       'ORIGINAL_BALANCE', 'BACKEND_RATIO', 'BACKEND_RATIO_NAN',\n",
    "       'ORIGINAL_TERM', 'ORIGINAL_TERM_NAN', 'SALE_PRICE', 'SALE_PRICE_NAN', \t   \n",
    "       'PREPAY_PENALTY_TERM', 'PREPAY_PENALTY_TERM_NAN', \n",
    "        'NUMBER_OF_UNITS', 'NUMBER_OF_UNITS_NAN', 'MARGIN',\n",
    "       'MARGIN_NAN', 'PERIODIC_RATE_CAP', 'PERIODIC_RATE_CAP_NAN',\n",
    "       'PERIODIC_RATE_FLOOR', 'PERIODIC_RATE_FLOOR_NAN', 'LIFETIME_RATE_CAP',\n",
    "       'LIFETIME_RATE_CAP_NAN', 'LIFETIME_RATE_FLOOR',\n",
    "       'LIFETIME_RATE_FLOOR_NAN', 'RATE_RESET_FREQUENCY',\n",
    "       'RATE_RESET_FREQUENCY_NAN', 'PAY_RESET_FREQUENCY',\n",
    "       'PAY_RESET_FREQUENCY_NAN', 'FIRST_RATE_RESET_PERIOD',\n",
    "       'FIRST_RATE_RESET_PERIOD_NAN', \t           \n",
    "       'LLMA2_ORIG_RATE_SPREAD', 'LLMA2_ORIG_RATE_SPREAD_NAN', \n",
    "       'AGI', 'AGI_NAN', 'UR', 'UR_NAN', 'COUNT_INT_RATE_LESS', 'LLMA2_ORIG_RATE_ORIG_MR_SPREAD', \n",
    "       'LLMA2_ORIG_RATE_ORIG_MR_SPREAD_NAN', 'NUM_PRIME_ZIP', 'NUM_PRIME_ZIP_NAN'\n",
    "       ]\n",
    "    \n",
    "    binary_cols = ['LLMA2_HIST_LAST_12_MONTHS_MIS', 'LLMA2_PRIME', \n",
    "                   'LLMA2_SUBPRIME', 'LLMA2_APPVAL_LT_SALEPRICE']\n",
    "\n",
    "    '''\n",
    "    nan_cols = {'MBA_DAYS_DELINQUENT': 'median', 'CURRENT_INTEREST_RATE': 'median', 'LOANAGE': 'median',\n",
    "                'CURRENT_BALANCE' : 'median', 'SCHEDULED_PRINCIPAL': 'median', 'SCHEDULED_MONTHLY_PANDI': 'median',       \n",
    "                'LLMA2_CURRENT_INTEREST_SPREAD': 'median', 'NUM_MODIF': 0, 'P_RATE_TO_MOD': 0, 'MOD_RATE': 0,\n",
    "                'DIF_RATE': 0, 'P_MONTHLY_PAY': 0, 'MOD_MONTHLY_PAY': 0, 'DIF_MONTHLY_PAY': 0, 'CAPITALIZATION_AMT': 0,\n",
    "                'MORTGAGE_RATE': 'median', 'FICO_SCORE_ORIGINATION': 'median', 'INITIAL_INTEREST_RATE': 'median', 'ORIGINAL_LTV': 'median',\n",
    "                'ORIGINAL_BALANCE': 'median', 'BACKEND_RATIO': 'median', 'ORIGINAL_TERM': 'median', 'SALE_PRICE': 'median', 'PREPAY_PENALTY_TERM': 'median',\n",
    "                'NUMBER_OF_UNITS': 'median', 'MARGIN': 'median', 'PERIODIC_RATE_CAP': 'median', 'PERIODIC_RATE_FLOOR': 'median', 'LIFETIME_RATE_CAP': 'median',\n",
    "                'LIFETIME_RATE_FLOOR': 'median', 'RATE_RESET_FREQUENCY': 'median', 'PAY_RESET_FREQUENCY': 'median',\n",
    "                'FIRST_RATE_RESET_PERIOD': 'median', 'LLMA2_ORIG_RATE_SPREAD': 'median', 'AGI': 'median', 'UR': 'median',\n",
    "                'LLMA2_C_IN_LAST_12_MONTHS': 'median', 'LLMA2_30_IN_LAST_12_MONTHS': 'median', 'LLMA2_60_IN_LAST_12_MONTHS': 'median',\n",
    "                'LLMA2_90_IN_LAST_12_MONTHS': 'median', 'LLMA2_FC_IN_LAST_12_MONTHS': 'median',\n",
    "                'LLMA2_REO_IN_LAST_12_MONTHS': 'median', 'LLMA2_0_IN_LAST_12_MONTHS': 'median', \n",
    "                'LLMA2_ORIG_RATE_ORIG_MR_SPREAD':0, 'NUM_PRIME_ZIP':'median'\n",
    "                }\n",
    "    '''\n",
    "    '''\n",
    "    set(nan_cols) - set(nan_cols_nonan)\n",
    "    Out[56]: \n",
    "    {'COUNT_INT_RATE_LESS', # never missed\n",
    "     'FICO_SCORE_ORIGINATION', # never missed\n",
    "     'INITIAL_INTEREST_RATE', # never missed\n",
    "     'LLMA2_0_IN_LAST_12_MONTHS', #In average, 14% of missing data!\n",
    "     'LLMA2_30_IN_LAST_12_MONTHS',\n",
    "     'LLMA2_60_IN_LAST_12_MONTHS',\n",
    "     'LLMA2_90_IN_LAST_12_MONTHS',\n",
    "     'LLMA2_C_IN_LAST_12_MONTHS',\n",
    "     'LLMA2_FC_IN_LAST_12_MONTHS',\n",
    "     'LLMA2_REO_IN_LAST_12_MONTHS',\n",
    "     'ORIGINAL_BALANCE', # never missed\n",
    "     'ORIGINAL_LTV'} # never missed\n",
    "    '''\n",
    "    nan_cols = {'MBA_DAYS_DELINQUENT': 'mean', 'CURRENT_INTEREST_RATE': 'mean', 'LOANAGE': 'mean',\n",
    "                'CURRENT_BALANCE' : 'mean', 'SCHEDULED_PRINCIPAL': 'mean', 'SCHEDULED_MONTHLY_PANDI': 'mean',       \n",
    "                'LLMA2_CURRENT_INTEREST_SPREAD': 'mean', 'NUM_MODIF': 0, 'P_RATE_TO_MOD': 0, 'MOD_RATE': 0,\n",
    "                'DIF_RATE': 0, 'P_MONTHLY_PAY': 0, 'MOD_MONTHLY_PAY': 0, 'DIF_MONTHLY_PAY': 0, 'CAPITALIZATION_AMT': 0,\n",
    "                'MORTGAGE_RATE': 'mean', 'FICO_SCORE_ORIGINATION': 'mean', 'INITIAL_INTEREST_RATE': 'mean', 'ORIGINAL_LTV': 'mean',\n",
    "                'ORIGINAL_BALANCE': 'mean', 'BACKEND_RATIO': 'mean', 'ORIGINAL_TERM': 'mean', 'SALE_PRICE': 'mean', 'PREPAY_PENALTY_TERM': 'mean',\n",
    "                'NUMBER_OF_UNITS': 'mean', 'MARGIN': 'mean', 'PERIODIC_RATE_CAP': 'mean', 'PERIODIC_RATE_FLOOR': 'mean', 'LIFETIME_RATE_CAP': 'mean',\n",
    "                'LIFETIME_RATE_FLOOR': 'mean', 'RATE_RESET_FREQUENCY': 'mean', 'PAY_RESET_FREQUENCY': 'mean',\n",
    "                'FIRST_RATE_RESET_PERIOD': 'mean', 'LLMA2_ORIG_RATE_SPREAD': 'mean', 'AGI': 'mean', 'UR': 'mean',\n",
    "                'LLMA2_C_IN_LAST_12_MONTHS': 'mean', 'LLMA2_30_IN_LAST_12_MONTHS': 'mean', 'LLMA2_60_IN_LAST_12_MONTHS': 'mean',\n",
    "                'LLMA2_90_IN_LAST_12_MONTHS': 'mean', 'LLMA2_FC_IN_LAST_12_MONTHS': 'mean',\n",
    "                'LLMA2_REO_IN_LAST_12_MONTHS': 'mean', 'LLMA2_0_IN_LAST_12_MONTHS': 'mean', \n",
    "                'LLMA2_ORIG_RATE_ORIG_MR_SPREAD':0, 'COUNT_INT_RATE_LESS' :'median', 'NUM_PRIME_ZIP':'mean'\n",
    "                }\n",
    "    \n",
    "    categorical_cols = {'MBA_DELINQUENCY_STATUS':  ['0','3','6','9','C','F','R'], 'DELINQUENCY_STATUS_NEXT': ['0','3','6','9','C','F','R'],  #,'S','T','X'\n",
    "                           'BUYDOWN_FLAG': ['N','U','Y'], 'NEGATIVE_AMORTIZATION_FLAG': ['N','U','Y'], 'PREPAY_PENALTY_FLAG': ['N','U','Y'],\n",
    "                           'OCCUPANCY_TYPE': ['1','2','3','U'], 'PRODUCT_TYPE': ['10','20','30','40','50','51','52','53','54','5A','5Z',\n",
    "                                            '60','61','62','63','6Z','70','80','81','82','83','84','8Z','U'], \n",
    "                           'PROPERTY_TYPE': ['1','2','3','4','5','6','7','8','9','L','M','U','Z'], 'LOAN_PURPOSE_CATEGORY': ['P','R','U'], \n",
    "                           'DOCUMENTATION_TYPE': ['1','2','3','U'], 'CHANNEL': ['1','2','3','4','5','6','7','8','9','A','B','C','D','U'], \n",
    "                           'LOAN_TYPE': ['1','2','3','4','5','6','7','U'], 'IO_FLAG': ['N','U','Y'], \n",
    "                           'CONVERTIBLE_FLAG': ['N','U','Y'], 'POOL_INSURANCE_FLAG': ['N','U','Y'], 'STATE': ['AK', 'AL', 'AR', 'AZ', 'CA', 'CO',\n",
    "                                               'CT', 'DC', 'DE', 'FL', 'GA', 'HI', 'IA', 'ID', 'IL', 'IN', 'KS', 'KY', 'LA', 'MA', \n",
    "                                               'MD', 'ME', 'MI', 'MN', 'MO', 'MS', 'MT', 'NC', 'ND', 'NE', 'NH', 'NJ', 'NM', 'NV', \n",
    "                                               'NY', 'OH', 'OK', 'OR', 'PA', 'PR', 'RI', 'SC', 'SD', 'TN', 'TX', 'UT', 'VA', 'VT', \n",
    "                                               'WA', 'WI', 'WV', 'WY'], \n",
    "                           'CURRENT_INVESTOR_CODE': ['240', '250', '253', 'U'], 'ORIGINATION_YEAR': ['B1995','1995','1996','1997','1998','1999','2000','2001','2002','2003',\n",
    "                                                    '2004','2005','2006','2007','2008','2009','2010','2011','2012','2013','2014','2015','2016','2017','2018','nan']}\n",
    "\n",
    "    time_cols = ['YEAR', 'MONTH'] #, 'PERIOD'] #no nan values        \n",
    "\n",
    "    total_cols = numeric_cols.copy() \n",
    "    total_cols.extend(descriptive_cols)\n",
    "    total_cols.extend(categorical_cols.keys())\n",
    "    total_cols.extend(time_cols)\n",
    "    print('total_cols size: ', len(total_cols)) #110 !=112?? set(chunk_cols) - set(total_cols): {'LOAN_ID', 'PERIOD'}\n",
    "    \n",
    "    pd.set_option('io.hdf.default_format','table')\n",
    "\n",
    "    dist_file = pd.read_csv(os.path.join(RAW_DIR, \"percentile features3-mean.csv\"), sep=';', low_memory=False)\n",
    "    dist_file.columns = dist_file.columns.str.upper()\n",
    "\n",
    "    ncols = [x for x in numeric_cols if x.find('NAN')<0]\n",
    "    print(ncols)\n",
    "\n",
    "    #sum = 0\n",
    "    #for elem in categorical_cols.values():\n",
    "    #    sum += len(elem)\n",
    "    #print('total categorical values: ', sum) #181\n",
    "\n",
    "    for file_path in glob.glob(os.path.join(RAW_DIR, raw_dir,\"*.txt\")):  \n",
    "        file_name = os.path.basename(file_path)\n",
    "        if with_index==True:\n",
    "            target_path = os.path.join(PRO_DIR, raw_dir,file_name[:-4])        \n",
    "        else:\n",
    "            target_path = os.path.join(PRO_DIR, raw_dir,file_name[:-4]+'_non_index')\n",
    "        log_file=open(target_path+'-log.txt', 'w+', 1)        \n",
    "        print('Preprocessing File: ' + file_path)\n",
    "        log_file.write('Preprocessing File:  %s\\r\\n' % file_path)\n",
    "        startTime = datetime.now()      \n",
    "        \n",
    "        if (output_hdf == True):\n",
    "            #with  pd.HDFStore(target_path +'-pp.h5', complib='lzo', complevel=9) as hdf: #complib='lzo', complevel=9\n",
    "            train_writer = pd.HDFStore(target_path +'-train_.h5', complib='lzo', complevel=9) \n",
    "            valid_writer = pd.HDFStore(target_path +'-valid_.h5', complib='lzo', complevel=9)\n",
    "            test_writer = pd.HDFStore(target_path +'-test_.h5', complib='lzo', complevel=9) \n",
    "\n",
    "            print('generating: ', target_path +'-pp.h5')\n",
    "            train_index, valid_index, test_index = prepro_chunk(file_name, file_path, chunksize, label, log_file, \n",
    "                                                                nan_cols, categorical_cols, descriptive_cols, time_cols,\n",
    "                                                                dist_file, with_index, \n",
    "                                                                refNorm, train_period, valid_period, test_period, ncols,                                                                \n",
    "                                                                hdf=[train_writer, valid_writer, test_writer], tfrec=None,\n",
    "                                                                filtering_cols=filtering_cols)            \n",
    "\n",
    "\n",
    "            if train_writer.get_storer('train/features').nrows != train_writer.get_storer('train/labels').nrows:\n",
    "                    raise ValueError('Train-DataSet: Sizes should match!')  \n",
    "            if valid_writer.get_storer('valid/features').nrows != valid_writer.get_storer('valid/labels').nrows:\n",
    "                    raise ValueError('Valid-DataSet: Sizes should match!')  \n",
    "            if test_writer.get_storer('test/features').nrows != test_writer.get_storer('test/labels').nrows:\n",
    "                    raise ValueError('Test-DataSet: Sizes should match!')  \n",
    "\n",
    "            print('train/features size: ', train_writer.get_storer('train/features').nrows)\n",
    "            print('valid/features size: ', valid_writer.get_storer('valid/features').nrows)\n",
    "            print('test/features size: ', test_writer.get_storer('test/features').nrows)\n",
    "\n",
    "            log_file.write('***SUMMARY***\\n')\n",
    "            log_file.write('train/features size: %d\\r\\n' %(train_writer.get_storer('train/features').nrows))\n",
    "            log_file.write('valid/features size: %d\\r\\n' %(valid_writer.get_storer('valid/features').nrows))\n",
    "            log_file.write('test/features size: %d\\r\\n' %(test_writer.get_storer('test/features').nrows))\n",
    "\n",
    "            logger.info('training, validation and testing set into .h5 file')        \n",
    "        else:        \n",
    "            train_writer = tf.python_io.TFRecordWriter(target_path +'-train_.tfrecords')\n",
    "            valid_writer = tf.python_io.TFRecordWriter(target_path +'-valid_.tfrecords')\n",
    "            test_writer = tf.python_io.TFRecordWriter(target_path +'-test_.tfrecords')\n",
    "            train_index, valid_index, test_index = prepro_chunk(file_name, file_path, chunksize, label, log_file, \n",
    "                                                                nan_cols, categorical_cols, descriptive_cols, time_cols,\n",
    "                                                                dist_file, with_index, \n",
    "                                                                refNorm, train_period, valid_period, test_period, ncols,\n",
    "                                                                hdf=None, tfrec=[train_writer, valid_writer, test_writer],\n",
    "                                                                filtering_cols=filtering_cols) \n",
    "        print(train_index, valid_index, test_index)\n",
    "        train_writer.close()\n",
    "        valid_writer.close()\n",
    "        test_writer.close()        \n",
    "        \n",
    "        #def allfeatures_prepro_file(RAW_DIR, file_path, raw_dir, file_name, target_path, train_period, valid_period, test_period, log_file, dividing='percentage', chunksize=500000, \n",
    "        #                    refNorm=True, , with_index=True, output_hdf=True):\n",
    "\n",
    "        #allfeatures_prepro_file(RAW_DIR, file_path, raw_dir, file_name, target_path, train_num, valid_num, test_num, log_file, dividing=dividing, chunksize=chunksize, \n",
    "        #                        refNorm=refNorm, with_index=with_index, output_hdf=output_hdf)          \n",
    "        \n",
    "        startTime = datetime.now() - startTime\n",
    "        print('Preprocessing Time per file: ', startTime)     \n",
    "        log_file.write('Preprocessing Time per file:  %s\\r\\n' % str(startTime))\n",
    "        log_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def allclasses_Ncomp_71feat():\n",
    "    cols = ['PRODUCT_TYPE_20',\n",
    "    'IO_FLAG_U',\n",
    "    'NEGATIVE_AMORTIZATION_FLAG_N',\n",
    "    'LOAN_TYPE_1',\n",
    "    'NEGATIVE_AMORTIZATION_FLAG_U',\n",
    "    'IO_FLAG_N',\n",
    "    'CURRENT_INVESTOR_CODE_250',\n",
    "    'NEGATIVE_AMORTIZATION_FLAG_Y',\n",
    "    'LOAN_PURPOSE_CATEGORY_U',\n",
    "    'PREPAY_PENALTY_FLAG_U',\n",
    "    'LOAN_PURPOSE_CATEGORY_P',\n",
    "    'CHANNEL_D',\n",
    "    'CONVERTIBLE_FLAG_N',\n",
    "    'IO_FLAG_Y',\n",
    "    'CONVERTIBLE_FLAG_U',\n",
    "    'LOAN_PURPOSE_CATEGORY_R',\n",
    "    'ORIGINATION_YEAR_B1995',\n",
    "    'CHANNEL_U',\n",
    "    'POOL_INSURANCE_FLAG_U',\n",
    "    'CHANNEL_2',\n",
    "    'PREPAY_PENALTY_FLAG_Y',\n",
    "    'PROPERTY_TYPE_6',\n",
    "    'DOCUMENTATION_TYPE_U',\n",
    "    'PRODUCT_TYPE_10',\n",
    "    'CURRENT_INVESTOR_CODE_U',\n",
    "    'PERIODIC_RATE_FLOOR_NAN',\n",
    "    'PERIODIC_RATE_CAP_NAN',\n",
    "    'LIFETIME_RATE_FLOOR_NAN',\n",
    "    'PAY_RESET_FREQUENCY_NAN',\n",
    "    'CONVERTIBLE_FLAG_Y',\n",
    "    'DOCUMENTATION_TYPE_2',\n",
    "    'POOL_INSURANCE_FLAG_N',\n",
    "    'RATE_RESET_FREQUENCY_NAN',\n",
    "    'FIRST_RATE_RESET_PERIOD_NAN',\n",
    "    'PROPERTY_TYPE_2',\n",
    "    'CURRENT_INVESTOR_CODE_253',\n",
    "    'LOAN_TYPE_3',\n",
    "    'LIFETIME_RATE_CAP_NAN',\n",
    "    'PREPAY_PENALTY_FLAG_N',\n",
    "    'OCCUPANCY_TYPE_U',\n",
    "    'SCHEDULED_MONTHLY_PANDI_NAN',\n",
    "    'ORIGINATION_YEAR_2012',\n",
    "    'BUYDOWN_FLAG_N',\n",
    "    'ORIGINATION_YEAR_2008',\n",
    "    'BUYDOWN_FLAG_U',\n",
    "    'MARGIN',\n",
    "    'LOAN_TYPE_2',\n",
    "    'ORIGINATION_YEAR_2007',\n",
    "    'LLMA2_ORIG_RATE_ORIG_MR_SPREAD',\n",
    "    'AGI_NAN',\n",
    "    'ORIGINATION_YEAR_2006',\n",
    "    'DOCUMENTATION_TYPE_1',\n",
    "    'CHANNEL_1',\n",
    "    'ORIGINATION_YEAR_1999',\n",
    "    'CURRENT_INVESTOR_CODE_240',\n",
    "    'PROPERTY_TYPE_U',\n",
    "    'MARGIN_NAN',\n",
    "    'ORIGINATION_YEAR_2013',\n",
    "    'ORIGINATION_YEAR_2004',\n",
    "    'ORIGINATION_YEAR_1998',\n",
    "    'OCCUPANCY_TYPE_2',\n",
    "    'CHANNEL_3',\n",
    "    'LIFETIME_RATE_FLOOR',\n",
    "    'PROPERTY_TYPE_1',\n",
    "    'PERIODIC_RATE_CAP',\n",
    "    'ORIGINATION_YEAR_2005',\n",
    "    'PRODUCT_TYPE_82',\n",
    "    'LLMA2_HIST_LAST_12_MONTHS_MIS',\n",
    "    'LOANAGE',\n",
    "    'PROPERTY_TYPE_5',\n",
    "    'SCHEDULED_PRINCIPAL_NAN']\n",
    "    return cols\n",
    "\n",
    "def perclass_Ncomp_71feat():\n",
    "    # 71 selected features from allcols(size=257) using a per-class dataset with n_components=None:     \n",
    "    cols = [\n",
    "    'PRODUCT_TYPE_20', \n",
    "    'NEGATIVE_AMORTIZATION_FLAG_N', \n",
    "    'NEGATIVE_AMORTIZATION_FLAG_U', \n",
    "    'CONVERTIBLE_FLAG_N', \n",
    "    'CONVERTIBLE_FLAG_U', \n",
    "    'IO_FLAG_U', \n",
    "    'NEGATIVE_AMORTIZATION_FLAG_Y', \n",
    "    'LOAN_TYPE_1', \n",
    "    'CHANNEL_U', \n",
    "    'LOAN_PURPOSE_CATEGORY_U', \n",
    "    'PRODUCT_TYPE_10', \n",
    "    'BUYDOWN_FLAG_N', \n",
    "    'BUYDOWN_FLAG_U', \n",
    "    'DOCUMENTATION_TYPE_U', \n",
    "    'CHANNEL_2', \n",
    "    'LOAN_PURPOSE_CATEGORY_R', \n",
    "    'PREPAY_PENALTY_FLAG_Y', \n",
    "    'IO_FLAG_N', \n",
    "    'LOAN_PURPOSE_CATEGORY_P', \n",
    "    'CHANNEL_D', \n",
    "    'POOL_INSURANCE_FLAG_U', \n",
    "    'LOAN_TYPE_3', \n",
    "    'PREPAY_PENALTY_FLAG_U', \n",
    "    'PROPERTY_TYPE_6', \n",
    "    'LIFETIME_RATE_CAP_NAN', \n",
    "    'CURRENT_INVESTOR_CODE_253', \n",
    "    'POOL_INSURANCE_FLAG_N', \n",
    "    'CURRENT_INVESTOR_CODE_U', \n",
    "    'PERIODIC_RATE_FLOOR_NAN', \n",
    "    'OCCUPANCY_TYPE_U', \n",
    "    'IO_FLAG_Y', \n",
    "    'DOCUMENTATION_TYPE_2', \n",
    "    'LIFETIME_RATE_FLOOR_NAN', \n",
    "    'RATE_RESET_FREQUENCY_NAN', \n",
    "    'PERIODIC_RATE_CAP_NAN', \n",
    "    'PROPERTY_TYPE_2', \n",
    "    'OCCUPANCY_TYPE_3', \n",
    "    'PAY_RESET_FREQUENCY_NAN', \n",
    "    'PREPAY_PENALTY_FLAG_N', \n",
    "    'FIRST_RATE_RESET_PERIOD_NAN', \n",
    "    'CHANNEL_1', \n",
    "    'PROPERTY_TYPE_U', \n",
    "    'ORIGINATION_YEAR_2007', \n",
    "    'CURRENT_INVESTOR_CODE_240', \n",
    "    'CHANNEL_3', \n",
    "    'DOCUMENTATION_TYPE_1', \n",
    "    'ORIGINATION_YEAR_B1995', \n",
    "    'LLMA2_ORIG_RATE_ORIG_MR_SPREAD', \n",
    "    'ORIGINATION_YEAR_2008', \n",
    "    'PRODUCT_TYPE_80', \n",
    "    'CURRENT_INVESTOR_CODE_250', \n",
    "    'MARGIN_NAN',  \n",
    "    'ORIGINATION_YEAR_2006', \n",
    "    'PERIODIC_RATE_CAP', \n",
    "    'ORIGINATION_YEAR_2005', \n",
    "    'SCHEDULED_MONTHLY_PANDI_NAN', \n",
    "    'ORIGINATION_YEAR_2003', \n",
    "    'ORIGINATION_YEAR_2000', \n",
    "    'ORIGINATION_YEAR_2004', \n",
    "    'PROPERTY_TYPE_1', \n",
    "    'LOAN_TYPE_2', \n",
    "    'SCHEDULED_PRINCIPAL_NAN', \n",
    "    'BUYDOWN_FLAG_Y', \n",
    "    'CONVERTIBLE_FLAG_Y', \n",
    "    'STATE_CA', \n",
    "    'PERIODIC_RATE_FLOOR', \n",
    "    'AGI_NAN', \n",
    "    'OCCUPANCY_TYPE_1', \n",
    "    'PRODUCT_TYPE_82', \n",
    "    'LIFETIME_RATE_FLOOR',\n",
    "    'MARGIN']    \n",
    "    return cols\n",
    "    \n",
    "def filtering_allfeatures(cols):    \n",
    "    allcols = cols + ['DELINQUENCY_STATUS_NEXT_0', 'DELINQUENCY_STATUS_NEXT_3',\n",
    "    'DELINQUENCY_STATUS_NEXT_6', 'DELINQUENCY_STATUS_NEXT_9',\n",
    "    'DELINQUENCY_STATUS_NEXT_C', 'DELINQUENCY_STATUS_NEXT_F',\n",
    "    'DELINQUENCY_STATUS_NEXT_R']    \n",
    "    return allcols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def allclass_Ncomp_26numfeat():\n",
    "    # 26 selected features from numerical_cols(size=50)  using the whole dataset with n_components=None:     \n",
    "    cols = ['LOANAGE',     \n",
    "    'COUNT_INT_RATE_LESS',\n",
    "    'MORTGAGE_RATE',\n",
    "    'LLMA2_ORIG_RATE_ORIG_MR_SPREAD',\n",
    "    'LLMA2_HIST_LAST_12_MONTHS_MIS',\n",
    "    'ORIGINAL_LTV',\n",
    "    'ORIGINAL_BALANCE',\n",
    "    'UR',\n",
    "    'INITIAL_INTEREST_RATE',\n",
    "    'CURRENT_BALANCE',\n",
    "    'ORIGINAL_TERM',\n",
    "    'LLMA2_PRIME',\n",
    "    'MARGIN',\n",
    "    'LLMA2_90_IN_LAST_12_MONTHS',\n",
    "    'LLMA2_ORIG_RATE_SPREAD',\n",
    "    'LLMA2_30_IN_LAST_12_MONTHS',\n",
    "    'LLMA2_SUBPRIME',\n",
    "    'NUM_PRIME_ZIP',\n",
    "    'LLMA2_FC_IN_LAST_12_MONTHS',\n",
    "    'LLMA2_CURRENT_INTEREST_SPREAD',\n",
    "    'AGI',\n",
    "    'MBA_DAYS_DELINQUENT',\n",
    "    'LLMA2_C_IN_LAST_12_MONTHS',\n",
    "    'CURRENT_INTEREST_RATE',\n",
    "    'LIFETIME_RATE_FLOOR',\n",
    "    'LLMA2_60_IN_LAST_12_MONTHS']\n",
    "    return cols\n",
    "\n",
    "\n",
    "def perclass_Ncomp_26numfeat():\n",
    "    # 26 selected features from numerical_cols(size=50)  using a per-class dataset with n_components=None:     \n",
    "    cols = ['LOANAGE',\n",
    "    'MARGIN', \n",
    "    'MORTGAGE_RATE',\n",
    "    'LLMA2_ORIG_RATE_ORIG_MR_SPREAD', \n",
    "    'LLMA2_HIST_LAST_12_MONTHS_MIS', \n",
    "    'COUNT_INT_RATE_LESS',\n",
    "    'LIFETIME_RATE_FLOOR', \n",
    "    'INITIAL_INTEREST_RATE',\n",
    "    'LIFETIME_RATE_CAP', \n",
    "    'LLMA2_PRIME',\n",
    "    'LLMA2_ORIG_RATE_SPREAD', \n",
    "    'ORIGINAL_BALANCE',\n",
    "    'CURRENT_BALANCE',\n",
    "    'UR',\n",
    "    'LLMA2_SUBPRIME',\n",
    "    'MOD_RATE', \n",
    "    'LLMA2_CURRENT_INTEREST_SPREAD',\n",
    "    'RATE_RESET_FREQUENCY',\n",
    "    'CURRENT_INTEREST_RATE', \n",
    "    'PAY_RESET_FREQUENCY', \n",
    "    'DIF_RATE',  \n",
    "    'NUM_MODIF', \n",
    "    'AGI', \n",
    "    'PERIODIC_RATE_FLOOR',\n",
    "    'LLMA2_30_IN_LAST_12_MONTHS',\n",
    "    'LLMA2_C_IN_LAST_12_MONTHS'] \n",
    "    return cols\n",
    "\n",
    "def filtering_num_features(ncols):\n",
    "    all_nan_cols = ['MBA_DAYS_DELINQUENT_NAN',\n",
    "     'CURRENT_INTEREST_RATE_NAN',\n",
    "     'LOANAGE_NAN',\n",
    "     'CURRENT_BALANCE_NAN',\n",
    "     'SCHEDULED_PRINCIPAL_NAN',\n",
    "     'SCHEDULED_MONTHLY_PANDI_NAN',\n",
    "     'LLMA2_CURRENT_INTEREST_SPREAD_NAN',\n",
    "     'NUM_MODIF_NAN',\n",
    "     'P_RATE_TO_MOD_NAN',\n",
    "     'MOD_RATE_NAN',\n",
    "     'DIF_RATE_NAN',\n",
    "     'P_MONTHLY_PAY_NAN',\n",
    "     'MOD_MONTHLY_PAY_NAN',\n",
    "     'DIF_MONTHLY_PAY_NAN',\n",
    "     'CAPITALIZATION_AMT_NAN',\n",
    "     'MORTGAGE_RATE_NAN',\n",
    "     'BACKEND_RATIO_NAN',\n",
    "     'ORIGINAL_TERM_NAN',\n",
    "     'SALE_PRICE_NAN',\n",
    "     'PREPAY_PENALTY_TERM_NAN',\n",
    "     'NUMBER_OF_UNITS_NAN',\n",
    "     'MARGIN_NAN',\n",
    "     'PERIODIC_RATE_CAP_NAN',\n",
    "     'PERIODIC_RATE_FLOOR_NAN',\n",
    "     'LIFETIME_RATE_CAP_NAN',\n",
    "     'LIFETIME_RATE_FLOOR_NAN',\n",
    "     'RATE_RESET_FREQUENCY_NAN',\n",
    "     'PAY_RESET_FREQUENCY_NAN',\n",
    "     'FIRST_RATE_RESET_PERIOD_NAN',\n",
    "     'LLMA2_ORIG_RATE_SPREAD_NAN',\n",
    "     'AGI_NAN',\n",
    "     'UR_NAN',\n",
    "     'LLMA2_ORIG_RATE_ORIG_MR_SPREAD_NAN',\n",
    "     'NUM_PRIME_ZIP_NAN']\n",
    "\n",
    "    sel_nan_cols = [x for x in all_nan_cols for y in ncols if x.find(y)==0]\n",
    "\n",
    "    cat_cols = ['MBA_DELINQUENCY_STATUS_0', 'MBA_DELINQUENCY_STATUS_3',\n",
    "                'MBA_DELINQUENCY_STATUS_6', 'MBA_DELINQUENCY_STATUS_9', \n",
    "                'MBA_DELINQUENCY_STATUS_C', 'MBA_DELINQUENCY_STATUS_F', 'MBA_DELINQUENCY_STATUS_R'] + \\\n",
    "     ['BUYDOWN_FLAG_N', 'BUYDOWN_FLAG_U', 'BUYDOWN_FLAG_Y'] + \\\n",
    "     ['NEGATIVE_AMORTIZATION_FLAG_N', 'NEGATIVE_AMORTIZATION_FLAG_U', 'NEGATIVE_AMORTIZATION_FLAG_Y'] +\\\n",
    "     ['PREPAY_PENALTY_FLAG_N', 'PREPAY_PENALTY_FLAG_U', 'PREPAY_PENALTY_FLAG_Y'] +\\\n",
    "     ['OCCUPANCY_TYPE_1', 'OCCUPANCY_TYPE_2', 'OCCUPANCY_TYPE_3', 'OCCUPANCY_TYPE_U'] +\\\n",
    "     ['PRODUCT_TYPE_10', 'PRODUCT_TYPE_20', 'PRODUCT_TYPE_30', 'PRODUCT_TYPE_40',\n",
    "     'PRODUCT_TYPE_50', 'PRODUCT_TYPE_51', 'PRODUCT_TYPE_52', 'PRODUCT_TYPE_53',\n",
    "     'PRODUCT_TYPE_54', 'PRODUCT_TYPE_5A', 'PRODUCT_TYPE_5Z', 'PRODUCT_TYPE_60',\n",
    "     'PRODUCT_TYPE_61', 'PRODUCT_TYPE_62', 'PRODUCT_TYPE_63', 'PRODUCT_TYPE_6Z',\n",
    "     'PRODUCT_TYPE_70', 'PRODUCT_TYPE_80', 'PRODUCT_TYPE_81', 'PRODUCT_TYPE_82',\n",
    "     'PRODUCT_TYPE_83', 'PRODUCT_TYPE_84', 'PRODUCT_TYPE_8Z', 'PRODUCT_TYPE_U'] +\\\n",
    "     ['PROPERTY_TYPE_1', 'PROPERTY_TYPE_2', 'PROPERTY_TYPE_3', 'PROPERTY_TYPE_4',\n",
    "     'PROPERTY_TYPE_5', 'PROPERTY_TYPE_6', 'PROPERTY_TYPE_7', 'PROPERTY_TYPE_8',\n",
    "     'PROPERTY_TYPE_9', 'PROPERTY_TYPE_M', 'PROPERTY_TYPE_U', 'PROPERTY_TYPE_Z'] +\\\n",
    "     ['LOAN_PURPOSE_CATEGORY_P', 'LOAN_PURPOSE_CATEGORY_R', 'LOAN_PURPOSE_CATEGORY_U'] +\\\n",
    "     ['DOCUMENTATION_TYPE_1', 'DOCUMENTATION_TYPE_2', 'DOCUMENTATION_TYPE_3', 'DOCUMENTATION_TYPE_U'] +\\\n",
    "     ['CHANNEL_1', 'CHANNEL_2', 'CHANNEL_3', 'CHANNEL_4', 'CHANNEL_5', 'CHANNEL_6',\n",
    "     'CHANNEL_7', 'CHANNEL_8', 'CHANNEL_9', 'CHANNEL_A', 'CHANNEL_B', 'CHANNEL_C',\n",
    "     'CHANNEL_D', 'CHANNEL_U'] +\\\n",
    "     ['LOAN_TYPE_1', 'LOAN_TYPE_2', 'LOAN_TYPE_3', 'LOAN_TYPE_4', 'LOAN_TYPE_5', 'LOAN_TYPE_6', 'LOAN_TYPE_U'] +\\\n",
    "     ['IO_FLAG_N', 'IO_FLAG_U', 'IO_FLAG_Y'] +\\\n",
    "     ['CONVERTIBLE_FLAG_N', 'CONVERTIBLE_FLAG_U', 'CONVERTIBLE_FLAG_Y'] +\\\n",
    "     ['POOL_INSURANCE_FLAG_N', 'POOL_INSURANCE_FLAG_U', 'POOL_INSURANCE_FLAG_Y'] +\\\n",
    "     ['STATE_AK', 'STATE_AL', 'STATE_AR', 'STATE_AZ', 'STATE_CA', 'STATE_CO',\n",
    "     'STATE_CT', 'STATE_DC', 'STATE_DE', 'STATE_FL', 'STATE_GA', 'STATE_HI',\n",
    "     'STATE_IA', 'STATE_ID', 'STATE_IL', 'STATE_IN', 'STATE_KS', 'STATE_KY',\n",
    "     'STATE_LA', 'STATE_MA', 'STATE_MD', 'STATE_ME', 'STATE_MI', 'STATE_MN',\n",
    "     'STATE_MO', 'STATE_MS', 'STATE_MT', 'STATE_NC', 'STATE_ND', 'STATE_NE',\n",
    "     'STATE_NH', 'STATE_NJ', 'STATE_NM', 'STATE_NV', 'STATE_NY', 'STATE_OH',\n",
    "     'STATE_OK', 'STATE_OR', 'STATE_PA', 'STATE_PR', 'STATE_RI', 'STATE_SC',\n",
    "     'STATE_SD', 'STATE_TN', 'STATE_TX', 'STATE_UT', 'STATE_VA', 'STATE_VT',\n",
    "     'STATE_WA', 'STATE_WI', 'STATE_WV', 'STATE_WY'] +\\\n",
    "     ['CURRENT_INVESTOR_CODE_240', 'CURRENT_INVESTOR_CODE_250', 'CURRENT_INVESTOR_CODE_253', 'CURRENT_INVESTOR_CODE_U'] +\\\n",
    "     ['ORIGINATION_YEAR_B1995', 'ORIGINATION_YEAR_1995', 'ORIGINATION_YEAR_1996',\n",
    "     'ORIGINATION_YEAR_1997', 'ORIGINATION_YEAR_1998', 'ORIGINATION_YEAR_1999',\n",
    "     'ORIGINATION_YEAR_2000', 'ORIGINATION_YEAR_2001', 'ORIGINATION_YEAR_2002',\n",
    "     'ORIGINATION_YEAR_2003', 'ORIGINATION_YEAR_2004', 'ORIGINATION_YEAR_2005',\n",
    "     'ORIGINATION_YEAR_2006', 'ORIGINATION_YEAR_2007', 'ORIGINATION_YEAR_2008',\n",
    "     'ORIGINATION_YEAR_2009', 'ORIGINATION_YEAR_2010', 'ORIGINATION_YEAR_2011',\n",
    "     'ORIGINATION_YEAR_2012', 'ORIGINATION_YEAR_2013', 'ORIGINATION_YEAR_2014',\n",
    "     'ORIGINATION_YEAR_2015', 'ORIGINATION_YEAR_2016', 'ORIGINATION_YEAR_2017',\n",
    "     'ORIGINATION_YEAR_2018']\n",
    "\n",
    "    lab_cols = ['DELINQUENCY_STATUS_NEXT_0', 'DELINQUENCY_STATUS_NEXT_3',\n",
    "    'DELINQUENCY_STATUS_NEXT_6', 'DELINQUENCY_STATUS_NEXT_9',\n",
    "    'DELINQUENCY_STATUS_NEXT_C', 'DELINQUENCY_STATUS_NEXT_F',\n",
    "    'DELINQUENCY_STATUS_NEXT_R']\n",
    "\n",
    "    allcols = ncols + sel_nan_cols + cat_cols + lab_cols\n",
    "    return allcols    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_cols size:  107\n",
      "['MBA_DAYS_DELINQUENT', 'CURRENT_INTEREST_RATE', 'LOANAGE', 'CURRENT_BALANCE', 'SCHEDULED_PRINCIPAL', 'SCHEDULED_MONTHLY_PANDI', 'LLMA2_CURRENT_INTEREST_SPREAD', 'LLMA2_C_IN_LAST_12_MONTHS', 'LLMA2_30_IN_LAST_12_MONTHS', 'LLMA2_60_IN_LAST_12_MONTHS', 'LLMA2_90_IN_LAST_12_MONTHS', 'LLMA2_FC_IN_LAST_12_MONTHS', 'LLMA2_REO_IN_LAST_12_MONTHS', 'LLMA2_0_IN_LAST_12_MONTHS', 'NUM_MODIF', 'P_RATE_TO_MOD', 'MOD_RATE', 'DIF_RATE', 'P_MONTHLY_PAY', 'MOD_MONTHLY_PAY', 'DIF_MONTHLY_PAY', 'CAPITALIZATION_AMT', 'MORTGAGE_RATE', 'FICO_SCORE_ORIGINATION', 'INITIAL_INTEREST_RATE', 'ORIGINAL_LTV', 'ORIGINAL_BALANCE', 'BACKEND_RATIO', 'ORIGINAL_TERM', 'SALE_PRICE', 'PREPAY_PENALTY_TERM', 'NUMBER_OF_UNITS', 'MARGIN', 'PERIODIC_RATE_CAP', 'PERIODIC_RATE_FLOOR', 'LIFETIME_RATE_CAP', 'LIFETIME_RATE_FLOOR', 'RATE_RESET_FREQUENCY', 'PAY_RESET_FREQUENCY', 'FIRST_RATE_RESET_PERIOD', 'LLMA2_ORIG_RATE_SPREAD', 'AGI', 'UR', 'LLMA2_ORIG_RATE_ORIG_MR_SPREAD', 'NUM_PRIME_ZIP']\n",
      "Preprocessing File: /home/ubuntu/MLMortgage/data/raw/chuncks_random_c1mill/temporalloandynmodifmrstaticitur_CTrans_CLab_100th.txt\n",
      "generating:  /home/ubuntu/MLMortgage/data/processed/chuncks_random_c1mill/temporalloandynmodifmrstaticitur_CTrans_CLab_100th-pp.h5\n",
      "chunk:  1  chunk size:  100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py:41: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "2018-12-19 00:29:10,808 - drop_invalid_delinquency_status - INFO - dropping invalid transitions and delinquency status, fill nan values, drop duplicates\n",
      "2018-12-19 00:29:13,559 - allfeatures_drop_cols - INFO - ...Columns Excluded from dataset...\n",
      "2018-12-19 00:29:13,619 - allfeatures_drop_cols - INFO - ...Columns Excluded from dataset...\n",
      "2018-12-19 00:29:13,670 - allfeatures_drop_cols - INFO - ...Columns Excluded from dataset...\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STANDARD DEV zero:  []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/pandas/core/frame.py:3694: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  errors=errors)\n",
      "2018-12-19 00:29:31,807 - allfeatures_drop_cols - INFO - ...Columns Excluded from dataset...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Records for train Set - Number of rows: 83641\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-12-19 00:29:31,905 - allfeatures_extract_labels - INFO - ...Labels extracted from Dataset...\n",
      "2018-12-19 00:29:32,816 - allfeatures_drop_cols - INFO - ...Columns Excluded from dataset...\n",
      "2018-12-19 00:29:32,889 - allfeatures_extract_labels - INFO - ...Labels extracted from Dataset...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Records for valid Set - Number of rows: 2899\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-12-19 00:29:33,025 - allfeatures_drop_cols - INFO - ...Columns Excluded from dataset...\n",
      "2018-12-19 00:29:33,100 - allfeatures_extract_labels - INFO - ...Labels extracted from Dataset...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Records for test Set - Number of rows: 6335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-12-19 00:29:33,195 - allfeatures_extract_labels - INFO - training, validation and testing set into .h5 file\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train/features size:  83641\n",
      "valid/features size:  2899\n",
      "test/features size:  6335\n",
      "83641 2899 6335\n",
      "Preprocessing Time per file:  0:00:25.153288\n",
      "Preprocessing - Time:  0:00:25.170052\n"
     ]
    }
   ],
   "source": [
    "startTime = datetime.now()\n",
    "\n",
    "if not os.path.exists(os.path.join(PRO_DIR, FLAGS.prepro_dir)): #os.path.exists\n",
    "    os.makedirs(os.path.join(PRO_DIR, FLAGS.prepro_dir))\n",
    "\n",
    "#filtering_num_features(allclass_Ncomp_26numfeat())\n",
    "allcols = None #filtering_num_features(allclass_Ncomp_26numfeat()) # filtering_allfeatures(allclasses_Ncomp_71feat()) # filtering_allfeatures(perclass_Ncomp_71feat()), filtering_num_features(perclass_Ncomp_26numfeat())\n",
    "\n",
    "allfeatures_preprocessing(RAW_DIR, PRO_DIR, FLAGS.prepro_dir, FLAGS.train_period, FLAGS.valid_period, FLAGS.test_period, dividing='percentage', \n",
    "                          chunksize=FLAGS.prepro_chunksize, refNorm=FLAGS.ref_norm, with_index=FLAGS.prepro_with_index, output_hdf=True, filtering_cols=allcols)        \n",
    "print('Preprocessing - Time: ', datetime.now() - startTime)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
