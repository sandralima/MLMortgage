{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beginning Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel package will be renamed from release 2.8; in order to keep installing from binary please use \"pip install psycopg2-binary\" instead. For details see: <http://initd.org/psycopg/docs/install.html#binary-install-from-pypi>.\n",
      "  \"\"\")\n",
      "2018-11-20 20:35:33,683 - matplotlib - DEBUG - CACHEDIR=/home/ubuntu/.cache/matplotlib\n",
      "2018-11-20 20:35:33,687 - matplotlib.font_manager - DEBUG - Using fontManager instance from /home/ubuntu/.cache/matplotlib/fontList.json\n",
      "2018-11-20 20:35:33,819 - matplotlib.backends - DEBUG - backend module://ipykernel.pylab.backend_inline version unknown\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/ubuntu/MLMortgage/src/data', '', '/home/ubuntu/src/cntk/bindings/python', '/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python36.zip', '/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6', '/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/lib-dynload', '/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages', '/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/IPython/extensions', '/home/ubuntu/.ipython']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-20 20:35:34,055 - matplotlib.backends - DEBUG - backend module://ipykernel.pylab.backend_inline version unknown\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import argparse\n",
    "import psutil\n",
    "import numpy as np\n",
    "import logging\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import glob\n",
    "from tensorflow.python.framework import ops\n",
    "import math\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "import ftplib\n",
    "\n",
    "\n",
    "nb_dir = os.path.join(Path(os.getcwd()).parents[0], 'src', 'data')\n",
    "if nb_dir not in sys.path:\n",
    "    sys.path.insert(0, nb_dir)\n",
    "print(sys.path)\n",
    "import features_selection as fs\n",
    "import make_dataset as md\n",
    "import build_data as bd\n",
    "import get_raw_data as grd\n",
    "# import data_classes\n",
    "\n",
    "models_dir = os.path.join(Path(os.getcwd()).parents[0], 'src', 'models')\n",
    "if models_dir not in sys.path:\n",
    "    sys.path.insert(0, models_dir)\n",
    "import nn_real as nn\n",
    "\n",
    "try:\n",
    "    import horovod.tensorflow as hvd\n",
    "except:\n",
    "    print(\"Failed to import horovod module. \"\n",
    "          \"%s is intended for use with Uber's Horovod distributed training \"\n",
    "          \"framework. To create a Docker image with Horovod support see \"\n",
    "          \"docker-examples/Dockerfile.horovod.\" % __file__)\n",
    "    raise\n",
    "\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/MLMortgage/data/raw /home/ubuntu/MLMortgage/data/processed\n"
     ]
    }
   ],
   "source": [
    "RAW_DIR = os.path.join(Path(os.getcwd()).parents[0], 'data', 'raw') \n",
    "PRO_DIR = os.path.join(Path(os.getcwd()).parents[0], 'data', 'processed')\n",
    "RANDOM_SEED = 123  # Set the seed to get reproducable results.\n",
    "DT_FLOAT = tf.float32\n",
    "NP_FLOAT = np.dtype('float32')\n",
    "\n",
    "print(RAW_DIR, PRO_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hvd.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FLAGS_setting(FLAGS, net_number):\n",
    "    # To determine an optimal set of hyperparameters, see Section 11.4.2 of the\n",
    "    # deep learning book. Has (1) grid, (2) random, and (3) Bayesian\n",
    "    # model-based search methods.Swersky et al. have a paper mentioned in that\n",
    "    # section (published in 2014).\n",
    "\n",
    "    # Hyperparameters\n",
    "    # FLAGS.epoch_num = 2  # 14  # 17  # 35  # 15\n",
    "    #print(\"FLAGS.epoch_num\", FLAGS.epoch_num)\n",
    "    # FLAGS.batch_size = 141600 # 4425 # 4000  \n",
    "    FLAGS.dropout_keep = 0.9  # 0.9  # 0.95  # .75  # .6\n",
    "    # ### parameters for training optimizer.\n",
    "    #FLAGS.learning_rate = .1  # .075  # .15  # .25\n",
    "    FLAGS.momentum = .5  # used by the momentum SGD.\n",
    "\n",
    "    # ### parameters for inverse_time_decay\n",
    "    FLAGS.decay_rate = 1\n",
    "    FLAGS.decay_step = 800 * 4400 #steps_per_epoch 1 * 80000 #according to paper: 800 epochs\n",
    "    FLAGS.rate_min = .0015\n",
    "    # ### parameters for exponential_decay\n",
    "    # FLAGS.decay_base = .96  # .96\n",
    "    # FLAGS.decay_step = 15000  # 12320  # 4 * 8700\n",
    "\n",
    "    # ### parameters for regularization\n",
    "    FLAGS.reg_rate = .01 * 1e-3  # * 1e-3\n",
    "\n",
    "    FLAGS.batch_norm = True  # False  #\n",
    "    FLAGS.dropout = True\n",
    "    # A flag to show the results on the held-out test set. Keep this at False.\n",
    "    FLAGS.test_flag = True\n",
    "    FLAGS.xla = True  # False\n",
    "    FLAGS.stratified_flag = False\n",
    "    #FLAGS.batch_type = 'batch'    \n",
    "    FLAGS.weighted_sampling = False  # True  #\n",
    "    # FLAGS.logdir =  os.path.join(Path.home(), 'real_summaries')  # \n",
    "    #FLAGS.n_hidden = 3\n",
    "    #FLAGS.s_hidden = [200, 140, 140]\n",
    "    # FLAGS.allow_summaries = False\n",
    "    FLAGS.epoch_flag = 0    \n",
    "    \n",
    "    #FLAGS.max_epoch_size = 141600*70 #137 # -1\n",
    "    \n",
    "    FLAGS.valid_batch_size = 1200000\n",
    "    FLAGS.test_batch_size = 1200000\n",
    "    \n",
    "    FLAGS.train_dir = 'chuncks_random_c1millx2_train'\n",
    "    FLAGS.valid_dir = 'chuncks_random_c1millx2_valid'\n",
    "    FLAGS.test_dir = 'chuncks_random_c1millx2_test'\n",
    "    FLAGS.train_period=[121,323] #[121,279] #[121, 143] \n",
    "    FLAGS.valid_period=[324,329] #[280,285] #[144, 147] \n",
    "    FLAGS.test_period=[330,342] #[286,304] #[148, 155]\n",
    "\n",
    "    FLAGS.epoch_num=15 \n",
    "    FLAGS.max_epoch_size=-1 \n",
    "    FLAGS.batch_size=4425*2 # two files per worker except at master!\n",
    "    FLAGS.lr_decay_policy       = 'time'\n",
    "    FLAGS.lr_decay_epochs       = 30\n",
    "    FLAGS.lr_decay_rate         = 0.1\n",
    "    FLAGS.lr_poly_power         = 2.\n",
    "    FLAGS.eval = False # True=Evaluation else Training\n",
    "    FLAGS.save_interval = 450\n",
    "    FLAGS.nstep_burnin = 20 # step from to count consuming time for a batch\n",
    "    FLAGS.summary_interval = 1800 # Time in seconds between saves of summary statistics\n",
    "    FLAGS.display_every = 100 # How often (in iterations) to print out running information\n",
    "    FLAGS.total_examples = -1 #-1 to training all dataset, otherwise the training will have a fixed length\n",
    "    \n",
    "    #Retrieveng from ftp:\n",
    "    FLAGS.ftp_dir = 'processed/c1mill'\n",
    "    \n",
    "    \n",
    "    if FLAGS.n_hidden < 0 : raise ValueError('The size of hidden layer must be at least 0')\n",
    "    if (FLAGS.n_hidden > 0) and (FLAGS.n_hidden != len(FLAGS.s_hidden)) : raise ValueError('Sizes in hidden layers should match!')\n",
    "    \n",
    "    if (net_number==0):\n",
    "        FLAGS.name ='default_settings'        \n",
    "    elif (net_number==1):\n",
    "        FLAGS.name ='Xworkers_1mill'\n",
    "        FLAGS.batch_layer_type = 'batch'        \n",
    "        \n",
    "    return FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNPARSED ['-f', '/run/user/1000/jupyter/kernel-37c107ff-8262-44e5-82e3-de0eeadaca45.json']\n",
      "existent directory\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "FLAGS, UNPARSED = nn.update_parser(argparse.ArgumentParser())\n",
    "print(\"UNPARSED\", UNPARSED)\n",
    "FLAGS.logdir = Path(str('/home/ubuntu/summ_15ep_2wrk/'))\n",
    "if not os.path.exists(os.path.join(FLAGS.logdir)): #os.path.exists\n",
    "    os.makedirs(os.path.join(FLAGS.logdir))\n",
    "else:\n",
    "    print('existent directory')\n",
    "FLAGS = FLAGS_setting(FLAGS, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FLAGS Namespace(allow_summaries=True, batch_layer_type='batch', batch_norm=True, batch_size=8850, batch_type='layer', data_dir='/input_data', decay_rate=1, decay_step=3520000, display_every=100, dropout=True, dropout_keep=0.9, epoch_flag=0, epoch_num=15, eval=False, ftp_dir='processed/c1mill', learning_rate=0.1, log_file=None, logdir=PosixPath('/home/ubuntu/summ_15ep_2wrk'), loss_tolerance=0.0001, lr_decay_epochs=30, lr_decay_policy='time', lr_decay_rate=0.1, lr_poly_power=2.0, max_epoch_size=-1, momentum=0.5, n_hidden=3, name='Xworkers_1mill', nstep_burnin=20, rate_min=0.0015, reg_rate=1e-05, s_hidden=[200, 140, 140], save_interval=450, stratified_flag=False, summary_interval=1800, test_batch_size=1200000, test_dir='chuncks_random_c1millx2_test', test_flag=True, test_period=[330, 342], total_examples=-1, train_dir='chuncks_random_c1millx2_train', train_period=[121, 323], valid_batch_size=1200000, valid_dir='chuncks_random_c1millx2_valid', valid_period=[324, 329], weighted_sampling=False, xla=True)\n"
     ]
    }
   ],
   "source": [
    "print(\"FLAGS\", FLAGS) #you can change the FLAGS by adding the setting before this line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Builder, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPUNetworkBuilder(object):\n",
    "    \"\"\"This class provides convenient methods for constructing feed-forward\n",
    "    networks with internal data layout of 'NCHW'.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 # is_training,\n",
    "                 dtype=DT_FLOAT,\n",
    "                 activation='RELU',\n",
    "                 use_batch_norm=True,\n",
    "                 batch_norm_config = {'decay':   0.9,\n",
    "                                      'epsilon': 1e-4,\n",
    "                                      'scale':   True,\n",
    "                                      'zero_debias_moving_mean': False}):\n",
    "        self.dtype             = dtype\n",
    "        self.activation_func   = activation\n",
    "        # self.is_training       = is_training\n",
    "        self.use_batch_norm    = use_batch_norm\n",
    "        self.batch_norm_config = batch_norm_config\n",
    "        #self._layer_counts     = defaultdict(lambda: 0)        \n",
    "        \n",
    "    def variable_summaries(self, name, var, allow_summaries):\n",
    "        \"\"\"Create summaries for the given Tensor (for TensorBoard visualization (TB graphs)).\n",
    "            Calculate the mean, min, max, histogram and standardeviation for 'var' variable and save the information\n",
    "            in tf.summary.\n",
    "\n",
    "        Args: \n",
    "             name (String): the of the scope for summaring. For min, max and standardeviation 'calculate_std' is used as sub-scope.\n",
    "             var (Tensor): This is the tensor variable for building summaries.\n",
    "        Returns: \n",
    "            None\n",
    "        Raises:        \n",
    "        \"\"\"\n",
    "        if allow_summaries:\n",
    "            with tf.name_scope(name):\n",
    "                mean = tf.reduce_mean(var)\n",
    "                tf.summary.scalar('mean', mean)\n",
    "                with tf.name_scope('calculate_std'):\n",
    "                    stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "                tf.summary.scalar('stddev', stddev)\n",
    "                tf.summary.scalar('min', tf.reduce_min(var))\n",
    "                tf.summary.scalar('max', tf.reduce_max(var))\n",
    "                tf.summary.histogram('histogram', var)\n",
    "                \n",
    "    def _variable_on_cpu(self, name,\n",
    "                     shape,\n",
    "                     initializer=None,\n",
    "                     regularizer=None,\n",
    "                     dtype=DT_FLOAT):\n",
    "        \"\"\"Create a Variable or get an existing one stored on CPU memory.    \n",
    "\n",
    "        Args:\n",
    "            name (String): name of the variable.\n",
    "            shape (list of ints): Shape of the variable.\n",
    "            initializer: Default None. Initializer for Variable.\n",
    "            regularizer (A (Tensor -> Tensor or None) function): Default None. Regularizer for Variable.\n",
    "            dtype (TYPE): Type of the new variable.\n",
    "        Returns:\n",
    "            Variable Tensor\n",
    "        \"\"\"\n",
    "        with tf.device('/gpu:1'): # this operation is assigned to this device, but this make a copy of data when is transferred on and off the device, which is expensive.\n",
    "            var = tf.get_variable(\n",
    "                name,\n",
    "                shape,\n",
    "                initializer=initializer,\n",
    "                regularizer=regularizer,\n",
    "                dtype=dtype)\n",
    "        return var\n",
    "\n",
    "    def _create_variable(self, name,\n",
    "                         shape, allow_summaries, \n",
    "                         initializer=None,\n",
    "                         regularizer=None,\n",
    "                         dtype=DT_FLOAT):\n",
    "        \"\"\"Call _variable_on_cpu methods and variable_summaries for the 'name' tensor variable. \n",
    "\n",
    "        Args:\n",
    "            name (String): name of the variable.\n",
    "            shape (list of ints): Shape of the variable.\n",
    "            initializer: Default None. Initializer for Variable.\n",
    "            regularizer (A (Tensor -> Tensor or None) function): Default None. Regularizer for Variable.\n",
    "            dtype (TYPE): Type of the new variable.\n",
    "        Returns:\n",
    "            Variable Tensor\n",
    "        \"\"\"\n",
    "        var = self._variable_on_cpu(name, shape, initializer, regularizer, dtype)\n",
    "        self.variable_summaries(name + '/summaries', var, allow_summaries)\n",
    "        return var\n",
    "\n",
    "    def create_weights(self, name, shape, reg_rate, allow_summaries):\n",
    "        \"\"\"Create a Variable initialized with weights which are truncated normal distribution and regularized by\n",
    "        l1_regularizer (L1 regularization encourages sparsity, Regularization can help prevent overfitting).    \n",
    "\n",
    "        Args:\n",
    "            name (String): name of the variable.\n",
    "            shape (list of ints): Shape of the variable.\n",
    "        Returns:\n",
    "            Variable Tensor\n",
    "        \"\"\"    \n",
    "        dtype = DT_FLOAT\n",
    "        # kernel_initializer = tf.uniform_unit_scaling_initializer(\n",
    "        #     factor=1.43, dtype=DT_FLOAT)\n",
    "        # kernel_initializer = tf.contrib.layers.xavier_initializer(\n",
    "        #     uniform=True, dtype=DT_FLOAT)\n",
    "        kernel_initializer = tf.truncated_normal_initializer(\n",
    "            stddev=(1.0 / np.sqrt(shape[0])), dtype=dtype)\n",
    "\n",
    "        regularizer = tf.contrib.layers.l2_regularizer(\n",
    "            np.float32(reg_rate), 'penalty')\n",
    "        return self._create_variable(name, shape, allow_summaries, kernel_initializer, regularizer,\n",
    "                                dtype)\n",
    "\n",
    "    def bias_variable(self, name, shape, layer_name, weighted_sampling): # FLAGS.weighted_sampling\n",
    "        \"\"\"Create a bias variable with appropriate initialization. In case of FLAGS.weighted_sampling==False\n",
    "        and layer_name contains 'soft' the bias variable will contain a np.array of Negative values. Otherwise\n",
    "        the bias variable will be initialized in zero.\n",
    "\n",
    "        Args:\n",
    "            name (String): name of the variable.\n",
    "            shape (list of ints): Shape of the variable.\n",
    "            layer_name (String): name of the layer.\n",
    "        Returns:\n",
    "            Variable Tensor.\n",
    "        \"\"\"\n",
    "        def initial_bias(layer_name):\n",
    "            \"\"\"Get the initial value for the bias of the layer with layer_name.\"\"\"\n",
    "            if (not weighted_sampling) and 'soft' in layer_name:\n",
    "                return np.array(\n",
    "                    [-4.66, -3.81, -4.81, -3.90, -0.08, -3.90, -7.51],\n",
    "                    dtype=NP_FLOAT) + NP_FLOAT(4.1)\n",
    "            return 0.0\n",
    "\n",
    "        initial_value = initial_bias(layer_name)\n",
    "        with tf.name_scope(name) as scope:\n",
    "            initial = tf.constant(initial_value, shape=shape)\n",
    "            bias = tf.Variable(initial, name=scope)\n",
    "            self.variable_summaries('summaries', bias)\n",
    "        return bias        \n",
    "    \n",
    "    def dropout_layer(self, name, tensor_before, FLAGS):\n",
    "        \"\"\"Compute dropout to tensor_before with name scoping and a placeholder for keep_prob. \n",
    "        With probability keep_prob, outputs the input element scaled up by 1 / keep_prob, otherwise outputs 0.\n",
    "\n",
    "        Args:\n",
    "            name (String): name of the scope.\n",
    "            tensor_before (Tensor): Variable Tensor.        \n",
    "        Returns:\n",
    "            Variable Tensor of the same shape of tensor_before.\n",
    "        \"\"\"   \n",
    "        if not FLAGS.dropout:\n",
    "            print('There is not dropout for' + name)\n",
    "            return tensor_before\n",
    "        with tf.name_scope(name) as scope:\n",
    "            keep_prob = tf.placeholder(DT_FLOAT, None, name='keep_proba')\n",
    "            tf.summary.scalar('keep_probability', keep_prob)\n",
    "            dropped = tf.nn.dropout(tensor_before, keep_prob=keep_prob, name=scope)\n",
    "            self.variable_summaries('input_dropped_out', dropped, FLAGS.allow_summaries)\n",
    "        return dropped\n",
    "\n",
    "    def batch_normalization(self, name, input_tensor, train_flag, FLAGS):\n",
    "        \"\"\"Perform batch normalization over the input tensor.\n",
    "        Batch normalization helps avoid overfitting and we're able to use more\n",
    "        aggressive (larger) learning rates, resulting in faster convergence.\n",
    "        training parameter: Either a Python boolean, or a TensorFlow boolean scalar tensor (e.g. a placeholder). \n",
    "        Whether to return the output in training mode (normalized with statistics of the current batch) or in \n",
    "        inference mode (normalized with moving statistics). NOTE: make sure to set this parameter correctly, \n",
    "        or else your training/inference will not work properly.\n",
    "\n",
    "        Args:\n",
    "            name (String): name of the scope and the name of the layer.\n",
    "            input_tensor (Tensor): Variable Tensor.        \n",
    "        Returns:\n",
    "            Variable Tensor # the same shape of input_tensor??.\n",
    "        \"\"\"\n",
    "        # if not FLAGS.batch_norm:\n",
    "        #     return input_tensor\n",
    "        # train_flag = tf.get_default_graph().get_tensor_by_name('train_flag:0')\n",
    "        with tf.name_scope(name):\n",
    "            normalized = tf.layers.batch_normalization(\n",
    "                input_tensor,\n",
    "                center=True,\n",
    "                scale=True,\n",
    "                training=train_flag,\n",
    "                name=name)  # renorm=True, renorm_momentum=0.99)\n",
    "            self.variable_summaries('normalized_batch', normalized, FLAGS.allow_summaries)\n",
    "        return normalized\n",
    "\n",
    "    def layer_normalization(self, name, input_tensor, FLAGS):\n",
    "        \"\"\"Perform layer normalization.\n",
    "\n",
    "        Layer normalization helps avoid overfitting and we're able to use more\n",
    "        aggressive (larger) learning rates, resulting in faster convergence.\n",
    "        Can be used as a normalizer function for conv2d and fully_connected.\n",
    "\n",
    "        Given a tensor inputs of rank R, moments are calculated and normalization \n",
    "        is performed over axes begin_norm_axis ... R - 1. \n",
    "        Scaling and centering, if requested, is performed over axes begin_params_axis .. R - 1.\n",
    "        \"\"\"\n",
    "        # if not FLAGS.batch_norm:\n",
    "        #     return input_tensor\n",
    "        with tf.name_scope(name):\n",
    "            normalized = tf.contrib.layers.layer_norm(\n",
    "                input_tensor, center=True, scale=True, scope=name)\n",
    "            self.variable_summaries('normalized_layer', normalized, FLAGS.allow_summaries)\n",
    "        return normalized\n",
    "\n",
    "\n",
    "    def normalize(self, name, input_tensor, train_flag, FLAGS):\n",
    "        \"\"\"Perform either type (batch/layer) of normalization.\"\"\"\n",
    "        if not FLAGS.batch_norm:\n",
    "            return input_tensor\n",
    "        if FLAGS.batch_type.lower() == 'batch':\n",
    "            return self.batch_normalization(name, input_tensor, train_flag, FLAGS)\n",
    "        if FLAGS.batch_type.lower() == 'layer':\n",
    "            return self.layer_normalization(name, input_tensor, FLAGS)\n",
    "        raise ValueError('Invalid value for batch_type: ' + FLAGS.batch_type)\n",
    "\n",
    "    def nn_layer(self, input_tensor, output_dim, layer_name, FLAGS, act, train_flag):\n",
    "        \"\"\"Create a simple neural net layer.\n",
    "\n",
    "        It performs the affine transformation and uses the activation function to\n",
    "        nonlinearize. It further sets up name scoping so that the resultant graph\n",
    "        is easy to read, and adds a number of summary ops.\n",
    "        \"\"\"\n",
    "        input_dim = input_tensor.shape[1].value    \n",
    "        with tf.variable_scope(layer_name): # A context manager for defining ops that creates variables (layers).\n",
    "            weights = self.create_weights('weights', [input_dim, output_dim], FLAGS.reg_rate, FLAGS.allow_summaries)\n",
    "            # This is outdated and no longer applies: Do not change the order of\n",
    "            # batch normalization and drop out. batch # normalization has to stay\n",
    "            # __before__ the drop out layer.\n",
    "            self.variable_summaries('input', input_tensor, FLAGS.allow_summaries)\n",
    "            input_tensor = self.dropout_layer('dropout', input_tensor, FLAGS)\n",
    "            with tf.name_scope('mix'):\n",
    "                mixed = tf.matmul(input_tensor, weights)\n",
    "                tf.summary.histogram('maybe_guassian', mixed)\n",
    "            # Batch or layer normalization has to stay __after__ the affine\n",
    "            # transformation (the bias term doens't really matter because of the\n",
    "            # beta term in the normalization equation).\n",
    "            # See pp. 5 of the batch normalization paper:\n",
    "            # ```We add the BN transform immediately before the nonlinearity, by\n",
    "            # normalizing x = W u + b```\n",
    "            # biases = bias_variable('biases', [output_dim], layer_name)\n",
    "            preactivate = self.normalize('layer_normalization', mixed, train_flag, FLAGS)  # + biases\n",
    "            # tf.summary.histogram('pre_activations', preactivate)\n",
    "            # preactivate = dropout_layer('dropout', preactivate)\n",
    "            with tf.name_scope('activation') as scope:\n",
    "                activations = self.activate(preactivate, funcname=act)\n",
    "                tf.summary.histogram('activations', activations)\n",
    "        return activations        \n",
    "    \n",
    "    def activate(self, input_layer, funcname=None):\n",
    "        \"\"\"Applies an activation function\"\"\"\n",
    "        if isinstance(funcname, tuple):\n",
    "            funcname = funcname[0]\n",
    "            params = funcname[1:]\n",
    "        if funcname is None:\n",
    "            funcname = self.activation_func\n",
    "        if funcname == 'LINEAR':\n",
    "            return input_layer\n",
    "        activation_map = {\n",
    "            'IDENT':   tf.identity,\n",
    "            'RELU':    tf.nn.relu,\n",
    "            'RELU6':   tf.nn.relu6,\n",
    "            'ELU':     tf.nn.elu,\n",
    "            'SIGMOID': tf.nn.sigmoid,\n",
    "            'TANH':    tf.nn.tanh,\n",
    "            'LRELU':   lambda x, name: tf.maximum(params[0]*x, x, name=name)\n",
    "        }\n",
    "        return activation_map[funcname](input_layer, name=funcname.lower())\n",
    "    \n",
    "    def add_hidden_layers(self, features, architecture, FLAGS, train_flag, act=None):\n",
    "        \"\"\"Add hidden layers to the model using the architecture parameters.\"\"\"\n",
    "        hidden_out = features\n",
    "        jit_scope = tf.contrib.compiler.jit.experimental_jit_scope #JIT compiler compiles and runs parts of TF graphs via XLA, fusing multiple operators (kernel fusion) nto a small number of compiled kernels.\n",
    "        with jit_scope(): #this operation will be compiled with XLA.\n",
    "            for hid_i in range(1, FLAGS.n_hidden + 1):\n",
    "                hidden_out = self.nn_layer(hidden_out,\n",
    "                                      architecture['n_hidden_{:1d}'.format(hid_i)],\n",
    "                                      '{:1d}_hidden'.format(hid_i), FLAGS, act, train_flag)\n",
    "        return hidden_out        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(object):\n",
    "    \n",
    "    def __init__(self, func, nstep_per_epoch=None, dtype='trainer'):\n",
    "        \n",
    "        if dtype == 'trainer':            \n",
    "            self.nstep_per_epoch = nstep_per_epoch\n",
    "            #self.architecture = architecture\n",
    "            #self.FLAGS = FLAGS\n",
    "            with tf.device('/cpu:0'):\n",
    "                #self.global_step = tf.contrib.framework.get_or_create_global_step()\n",
    "                # tf.train.get_global_step()\n",
    "                self.global_step = tf.get_variable(\n",
    "                    'global_step', [],\n",
    "                    initializer=tf.constant_initializer(0),\n",
    "                    dtype=tf.int64,\n",
    "                    trainable=False)\n",
    "        elif dtype != 'evaluator': #Evaluator\n",
    "            raise ValueError('Invalid dtype value: ' + dtype)\n",
    "\n",
    "        self.func = func\n",
    "        self.dtype = dtype\n",
    "\n",
    "    def get_learning_rate(self, initial_learning_rate):\n",
    "        \"\"\"Get the learning rate.\"\"\"\n",
    "        with tf.name_scope('learning_rate') as scope:\n",
    "            if FLAGS.lr_decay_policy == 'poly':\n",
    "                return tf.train.polynomial_decay(\n",
    "                                        initial_learning_rate,\n",
    "                                        self.global_step,\n",
    "                                        decay_steps=FLAGS.epoch_num*self.nstep_per_epoch,\n",
    "                                        end_learning_rate=0.,\n",
    "                                        power=FLAGS.lr_poly_power,\n",
    "                                        cycle=False)\n",
    "            elif FLAGS.lr_decay_policy == 'exp':\n",
    "                return tf.train.exponential_decay(\n",
    "                                        initial_learning_rate,\n",
    "                                        self.global_step,\n",
    "                                        decay_steps=FLAGS.lr_decay_epochs*self.nstep_per_epoch,\n",
    "                                        decay_rate=FLAGS.lr_decay_rate,\n",
    "                                        staircase=True)\n",
    "            else:            \n",
    "                # decayed_lr = tf.train.exponential_decay(\n",
    "                #     initial_learning_rate,\n",
    "                #     global_step,\n",
    "                #     FLAGS.decay_step,\n",
    "                #     FLAGS.decay_base,\n",
    "                #     staircase=False)\n",
    "                decayed_lr = tf.train.inverse_time_decay(\n",
    "                    initial_learning_rate,\n",
    "                    self.global_step,\n",
    "                    decay_steps=FLAGS.decay_step,\n",
    "                    decay_rate=FLAGS.decay_rate)\n",
    "                final_lr = tf.clip_by_value(\n",
    "                    decayed_lr, FLAGS.rate_min, 1000, name=scope)\n",
    "                tf.summary.scalar('value', final_lr)\n",
    "                return final_lr\n",
    "        # return self.learning_rate \n",
    "\n",
    "    def get_accuracy(self, labels_int, logits, name):\n",
    "        \"\"\"Get the accuracy tensor.\"\"\"\n",
    "        with tf.name_scope(name) as scope:\n",
    "            # For a classifier model, we can use the in_top_k Op.\n",
    "            # It returns a bool tensor with shape [batch_size] that is true for\n",
    "            # the examples where the label is in the top k (here k=1)\n",
    "            # of all logits for that example.\n",
    "            correct = tf.nn.in_top_k(\n",
    "                logits, labels_int, 1, name='correct_prediction') # returns a tensor of type bool.\n",
    "            return tf.reduce_mean(tf.cast(correct, DT_FLOAT), name=scope)\n",
    "\n",
    "    # auc = get_auc(labels, probs, True, 'metrics/auc')\n",
    "    def get_auc(self, labels, scores, hist_flag, name):\n",
    "        \"\"\"Calculate the AUC of the two-way classifier for the given class.\"\"\"\n",
    "\n",
    "        def get_auc_using_histogram(labels, scores, class_, name):\n",
    "            \"\"\"Calculate the AUC.\"\"\"\n",
    "            class_ind = class_dict[class_.upper()]\n",
    "            with tf.name_scope(name) as scope:\n",
    "                auc, update_op = tf.contrib.metrics.auc_using_histogram( # his Op maintains Variables containing histograms of the scores associated with True and False labels. \n",
    "                    tf.cast(labels[:, class_ind], tf.bool),\n",
    "                    scores[:, class_ind],\n",
    "                    score_range=[0.0, 1.0],\n",
    "                    nbins=200,\n",
    "                    collections=None,\n",
    "                    name=scope)\n",
    "            ops.add_to_collections(ops.GraphKeys.UPDATE_OPS, update_op)\n",
    "            # print(update_op.name)\n",
    "            # print(auc) # it doesn't work because FailedPreconditionError (see above for traceback): Attempting to use uninitialized value metrics/auc/0//hist_accumulate/hist_true_acc\n",
    "            # aucp = tf.Print(auc,[auc], message='AUC the label: ' + class_) # it doesnt work because it doesnt run in a session\n",
    "            # print(aucp)\n",
    "            return auc\n",
    "\n",
    "        def get_auc_metric(labels, scores, class_, name):\n",
    "            \"\"\"Determine the AUC using conventional methods.\"\"\"\n",
    "            class_ind = class_dict[class_.upper()]\n",
    "            with tf.name_scope(name) as scope:\n",
    "                auc, _ = tf.metrics.auc( # Computes the approximate AUC via a Riemann sum.\n",
    "                    tf.cast(labels[:, class_ind], tf.bool), # ?? Print out!!\n",
    "                    scores[:, class_ind],\n",
    "                    weights=None,\n",
    "                    num_thresholds=200,\n",
    "                    metrics_collections=None,\n",
    "                    updates_collections=ops.GraphKeys.UPDATE_OPS,\n",
    "                    curve='ROC',\n",
    "                    name=scope)\n",
    "            # print(auc.op.name)\n",
    "            return auc\n",
    "\n",
    "        classes = ['0', '3', '6', '9', 'C', 'F', 'R']\n",
    "        class_dict = {classes[ind]: ind for ind in range(len(classes))}\n",
    "        if hist_flag:\n",
    "            auc_func = get_auc_using_histogram\n",
    "        else:\n",
    "            auc_func = get_auc_metric\n",
    "        with tf.name_scope(name) as scope:\n",
    "            aucv = [\n",
    "                    auc_func(labels, scores, class_, str(ind)) for ind, class_ in enumerate(classes) # pair (index ej. 0, value ej. '0')\n",
    "                   ]      \n",
    "            auc_values = tf.stack( # Pack along first dim\n",
    "                aucv,\n",
    "                axis=0,\n",
    "                name=scope)\n",
    "            # aucv = tf.Print(auc_values,[auc_values], message='AUC for all labels: ')\n",
    "            # print(aucv) # or maybe aucv.eval() or var = tf.Variable(aucv) and then var.eval(session=sess), or ovar = sess.run(var) but Attempting to use uninitialized value metrics/auc/Variable\n",
    "            return auc_values\n",
    "\n",
    "\n",
    "    # conf_mtx = get_confusion_matrix(labels_int, predictions, len(classes), 'metrics/confusion')\n",
    "    def get_confusion_matrix(self, labels_int, predictions, num_classes, name):\n",
    "        \"\"\"Get the confusion matrix.\n",
    "        Both prediction and labels must be 1-D arrays of the same shape in order for \n",
    "        this function to work.\n",
    "        \"\"\"\n",
    "        with tf.name_scope(name) as scope:\n",
    "            conf = tf.confusion_matrix(\n",
    "                labels_int,\n",
    "                predictions=predictions,\n",
    "                num_classes=num_classes,\n",
    "                dtype=tf.int32,\n",
    "                name=scope,\n",
    "                weights=None)\n",
    "        # print(conf.op.name)\n",
    "        return conf #return a K x K Matriz K = num_classes\n",
    "\n",
    "\n",
    "    def get_m_hand(self, labels, scores, name):\n",
    "        \"\"\"Implement the M measure described in Hand.\n",
    "\n",
    "        See ```A Simple Generalisation of the Area Under the ROC Curve for Multiple\n",
    "        Class Classification Problems``` Hand, Till 2001.    \n",
    "\n",
    "        \"\"\"\n",
    "        def get_auc_using_histogram(labels, scores, first_ind, second_ind, scope):\n",
    "            \"\"\"Calculate the AUC.\n",
    "            Calculate the AUC value by maintainig histograms of boolean variables (labels and \n",
    "            scores masked by the First-Second Individuals rule).\n",
    "            \"\"\"\n",
    "            mask = (labels[:, first_ind] + labels[:, second_ind]) > 0 #one in at least one column.\n",
    "            auc, update_op = tf.contrib.metrics.auc_using_histogram( # maintains variables containing histograms of the scores associated with True, False labels. \n",
    "                tf.cast(tf.boolean_mask(labels[:, first_ind], mask), tf.bool), # tf.boolean_mask: Apply boolean mask to tensor. Numpy equivalent is tensor[mask].\n",
    "                tf.boolean_mask(scores[:, first_ind], mask),\n",
    "                score_range=[0.0, 1.0],\n",
    "                nbins=500,\n",
    "                collections=None,\n",
    "                name=scope)\n",
    "            ops.add_to_collections(ops.GraphKeys.UPDATE_OPS, update_op)\n",
    "            # print(update_op.name)\n",
    "            return auc\n",
    "\n",
    "        temp_array = []\n",
    "        with tf.name_scope(name) as main_scope:\n",
    "            for first_ind in range(7):\n",
    "                for second_ind in range(7):\n",
    "                    if first_ind != second_ind:\n",
    "                        final_name = '{:d}{:d}'.format(first_ind, second_ind)\n",
    "                        with tf.name_scope(final_name) as scope:\n",
    "                            auc = get_auc_using_histogram(\n",
    "                                labels, scores, first_ind, second_ind, scope)\n",
    "                        temp_array.append(auc)\n",
    "            return tf.stack(temp_array, axis=0, name=main_scope) # Stacks a list of rank-R tensors into one rank-(R+1) tensor.\n",
    "\n",
    "\n",
    "    def get_auc_pr_curve(self, labels, scores, name, num_thresholds):    \n",
    "        with tf.name_scope(name) as scope:                             \n",
    "            AUC_PR = []\n",
    "            AUC_data = []\n",
    "            for i in range(7):  \n",
    "                data, update_op = tf.contrib.metrics.precision_recall_at_equal_thresholds(\n",
    "                                name='pr_data',\n",
    "                                predictions=scores[:, i],\n",
    "                                labels=tf.cast(labels[:, i], tf.bool),\n",
    "                                num_thresholds=10, use_locking=True)\n",
    "                ops.add_to_collections(ops.GraphKeys.UPDATE_OPS, update_op)\n",
    "                AUC_data.append((tf.stack(data.recall), tf.stack(data.precision), tf.stack(data.thresholds)))   # we cant use sklearn with tensorflow definition!\n",
    "                auc, _ = tf.metrics.auc(labels[:, i], scores[:, i], weights=None, num_thresholds=10, \n",
    "                                        curve='PR', updates_collections=ops.GraphKeys.UPDATE_OPS, metrics_collections=None, summation_method='careful_interpolation') # \n",
    "                # ops.add_to_collections(ops.GraphKeys.UPDATE_OPS, update_op)\n",
    "                AUC_PR.append(auc)\n",
    "            # print(AUC_data)\n",
    "            return tf.stack( # Pack the array of scalar tensor along one dim tensor\n",
    "                AUC_PR,\n",
    "                axis=0,\n",
    "                name=scope), AUC_data\n",
    "\n",
    "    def log_loss(self, labels, probs, name):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            labels: Labels tensor, int32 - [batch_size, n_classes], with one-hot\n",
    "            encoded values.\n",
    "            logits: Probabilities tensor, float32 - [batch_size, n_classes].\n",
    "        \"\"\"\n",
    "        with tf.name_scope(name) as scope:\n",
    "            total_loss = 0\n",
    "            for j in range(probs.shape[1].value):\n",
    "                loss = tf.losses.log_loss(labels[:, j], probs[:, j], loss_collection=None)\n",
    "                total_loss += loss\n",
    "\n",
    "            return tf.div(total_loss, np.float32(probs.shape[1].value), name=scope)\n",
    "    \n",
    "    def calculate_metrics(self, labels, logits):\n",
    "        \"\"\"Evaluate the quality of the logits at predicting the label.\n",
    "\n",
    "        Args:\n",
    "            labels: Labels tensor, int32 - [batch_size, n_classes], with one-hot\n",
    "            encoded values.\n",
    "            logits: Logits tensor, float32 - [batch_size, n_classes].\n",
    "        Returns:\n",
    "            A scalar float32 tensor with the fraction of examples (out of\n",
    "            batch_size) that were predicted correctly.\n",
    "        \"\"\"\n",
    "        classes = ['0', '3', '6', '9', 'C', 'F', 'R']\n",
    "        with tf.name_scope('metrics'):\n",
    "            labels_int = tf.argmax(labels, 1, name='intlabels') #tf.argmax: Returns the index with the largest value across axes=1 of a tensor.\t\t\n",
    "            predictions = tf.argmax(logits, 1, name='predictions')        \n",
    "            probs = tf.nn.softmax(logits, name='probs') # Computes softmax activations. softmax = tf.exp(logits) / tf.reduce_sum(tf.exp(logits), axis)        \n",
    "\n",
    "        m_list = self.get_m_hand(labels, probs, 'metrics/m_measure')\n",
    "        accuracy = self.get_accuracy(labels_int, logits, 'metrics/accuracy')    \n",
    "        auc = self.get_auc(labels, probs, True, 'metrics/auc')    \n",
    "        conf_mtx = self.get_confusion_matrix(labels_int, predictions,\n",
    "                                        len(classes), 'metrics/confusion')\n",
    "        lloss = self.log_loss(labels, probs, 'metrics/log_loss')\n",
    "        pr_auc, pr_data = self.get_auc_pr_curve(labels, probs, 'metrics/auc_pr', 200)\n",
    "\n",
    "        # this is for the definition of the graph:\n",
    "        return accuracy, conf_mtx, auc, m_list, lloss, pr_auc, pr_data\n",
    "    \n",
    "    def training_step(self, architecture, FLAGS):        \n",
    "        features = tf.placeholder(\n",
    "            DT_FLOAT, [None, architecture['n_input']], name='features')\n",
    "        labels = tf.placeholder(\n",
    "            DT_FLOAT, [None, architecture['n_classes']], name='targets')\n",
    "        # epoch_flag = tf.placeholder(tf.int32, None, name='epoch_flag')\n",
    "        example_weights = tf.placeholder(\n",
    "            DT_FLOAT, [None], name='example_weights')\n",
    "        \n",
    "        with tf.device('/gpu:0'):\n",
    "            # Evaluate the loss:\n",
    "            loss, logits = self.func(features, labels, example_weights, architecture, FLAGS)\n",
    "\n",
    "        with tf.device('/cpu:0'): # No in_top_k implem on GPU\n",
    "            accuracy, conf_mtx, auc_list, m_list, lloss, auc_pr, auc_data = self.calculate_metrics(labels, logits)\n",
    "            better_acc = tf.reduce_mean(tf.diag_part(conf_mtx / tf.reduce_sum(conf_mtx, axis=1, keepdims=True))) #recall\n",
    "            precision = tf.reduce_mean(tf.diag_part(conf_mtx / tf.reduce_sum(conf_mtx, axis=0, keepdims=True)))            \n",
    "            auc_mean = tf.reduce_mean(auc_list)\n",
    "            m_list_mean = tf.reduce_mean(m_list)\n",
    "            auc_pr_mean = tf.reduce_mean(auc_pr)\n",
    "            \n",
    "            with tf.name_scope('0_performance'):\n",
    "                # Scalar summaries to track the loss and accuracy over time in TB.\n",
    "                tf.summary.scalar('0accuracy', accuracy)\n",
    "                tf.summary.scalar('1better_accuracy', better_acc)\n",
    "                tf.summary.scalar('12precision', precision)\n",
    "                # tf.summary.scalar('f1score', f1score)\n",
    "                tf.summary.scalar('2auc_aoc', auc_mean)\n",
    "                tf.summary.scalar('3m_measure', m_list_mean)\n",
    "                tf.summary.scalar('4loss', loss)\n",
    "                tf.summary.scalar('5log_loss', lloss)\n",
    "                tf.summary.scalar('6auc_pr', auc_pr_mean)\n",
    "\n",
    "        # Apply the gradients to optimize the loss function\n",
    "        with tf.device('/gpu:0'):            \n",
    "            update_ops = ops.get_collection(ops.GraphKeys.UPDATE_OPS)\n",
    "            # print(update_ops)\n",
    "            with ops.control_dependencies(update_ops):\n",
    "                with tf.name_scope('train') as scope:\n",
    "                    # print_loss = tf.Print(loss, [loss], name='print_loss') \n",
    "\n",
    "                    # Create a variable to track the global step.\n",
    "        #            global_step = tf.get_variable(\n",
    "        #                'train/global_step',\n",
    "        #                shape=[],\n",
    "        #                initializer=tf.constant_initializer(0, dtype=tf.int32),\n",
    "        #                trainable=False)            \n",
    "                    # Horovod: adjust learning rate based on number of GPUs.\n",
    "                    # optimizer = tf.train.GradientDescentOptimizer(1.0 * hvd.size())\n",
    "                    final_learning_rate = self.get_learning_rate(FLAGS.learning_rate * hvd.size())\n",
    "\n",
    "                    # optimizer = tf.train.GradientDescentOptimizer(final_learning_rate)\n",
    "                    optimizer = tf.train.MomentumOptimizer(final_learning_rate, FLAGS.momentum, use_nesterov=True)\n",
    "                    # optimizer = tf.train.AdagradOptimizer(final_learning_rate)\n",
    "\n",
    "                    # Use the optimizer to apply the gradients that minimize the loss\n",
    "                    # (and increment the global step counter) as a single training step.\n",
    "        #            return optimizer.minimize(\n",
    "        #                loss, global_step=global_step, name=scope)\n",
    "                    optimizer = hvd.DistributedOptimizer(optimizer) #HVD!!\n",
    "                    train_op = optimizer.minimize(loss, global_step=self.global_step, name=scope)\n",
    "            \n",
    "                        \n",
    "        return train_op, final_learning_rate, conf_mtx, accuracy, better_acc, precision, auc_list, auc_mean, m_list, m_list_mean, loss, lloss, auc_pr, auc_pr_mean, auc_data\n",
    "\n",
    "    def evaluation_step(self, batch_size):\n",
    "        \n",
    "        if (self.dtype!='evaluator'):\n",
    "            raise ValueError('Invalid function for dtype: ' + self.dtype)\n",
    "            \n",
    "        features = tf.placeholder(\n",
    "            DT_FLOAT, [None, architecture['n_input']], name='features')\n",
    "        labels = tf.placeholder(\n",
    "            DT_FLOAT, [None, architecture['n_classes']], name='targets')\n",
    "        example_weights = tf.placeholder(\n",
    "            DT_FLOAT, [None], name='example_weights')\n",
    "\n",
    "        with tf.device('/gpu:0'):\n",
    "            # Evaluate the loss:\n",
    "            logits = self.func(features, labels, architecture, FLAGS)        \n",
    "            accuracy, conf_mtx = self.calculate_metrics(labels, logits)[:2]\n",
    "            better_acc = tf.reduce_mean(tf.diag_part(conf_mtx / tf.reduce_sum(conf_mtx, axis=1, keepdims=True)))\n",
    "            precision = tf.reduce_mean(tf.diag_part(conf_mtx / tf.reduce_sum(conf_mtx, axis=0, keepdims=True)))\n",
    "            #f1score = 2 * precision * better_acc / (precision + better_acc)\n",
    "            \n",
    "\n",
    "        return conf_mtx, accuracy, better_acc, precision #,f1score  #, auc_list, m_list, lloss, auc_pr, auc_data\n",
    "    \n",
    "    def init(self):\n",
    "        # init_op = tf.global_variables_initializer()\n",
    "        # sess.run(init_op)        \n",
    "        \"\"\"Add an Op to the graph to initialize the global and local variables.\"\"\"\n",
    "        with tf.name_scope('init') as scope:\n",
    "            with tf.name_scope('global'):\n",
    "                global_init = tf.global_variables_initializer()\n",
    "            with tf.name_scope('local'):\n",
    "                local_init = tf.local_variables_initializer()\n",
    "                # print(local_init.name)\n",
    "            #init_op = tf.group(global_init, local_init, name=scope)\n",
    "        return global_init, local_init\n",
    "        \n",
    "    def sync(self, sess):\n",
    "        sync_op = hvd.broadcast_global_variables(0)\n",
    "        sess.run(sync_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_func(features, labels, weights, architecture, FLAGS):\n",
    "    # Build the forward model\n",
    "    net = GPUNetworkBuilder(dtype=DT_FLOAT)\n",
    "    train_flag = tf.placeholder(tf.bool, None, name='train_flag')\n",
    "    with tf.name_scope('input_normalization') as scope:\n",
    "        feature_norm = features\n",
    "        net.variable_summaries('input_normalized', feature_norm, FLAGS.allow_summaries)\n",
    "    hidden_out = net.add_hidden_layers(feature_norm, architecture, FLAGS, train_flag)\n",
    "    # Linear output layer for the logits\n",
    "    logits = (net.nn_layer(hidden_out, architecture['n_classes'],'9_softmax_linear', FLAGS, 'IDENT', train_flag))\n",
    "    \n",
    "    with tf.name_scope('loss') as scope:\n",
    "        with tf.name_scope('regularization'):\n",
    "            penalty = tf.losses.get_regularization_loss(name='penalty') #Gets the total regularization loss from an optional scope name (sum for ol + 3h + 2h + 1h).\n",
    "            tf.summary.scalar('weight_norm', penalty / (1e-8 + FLAGS.reg_rate)) #for printing out\n",
    "        with tf.name_scope('cross_entropy') as xentropy_scope:\n",
    "            weighted_cross_entropy = tf.losses.softmax_cross_entropy(\n",
    "                onehot_labels=labels,\n",
    "                logits=logits,\n",
    "                weights=weights,  # 1.0,\n",
    "                scope=xentropy_scope,\n",
    "                loss_collection=ops.GraphKeys.LOSSES)\n",
    "            tf.summary.scalar('weighted_cross_entropy', weighted_cross_entropy)\n",
    "        loss= tf.add(weighted_cross_entropy, penalty, name=scope) # Returns x + y element-wise.    \n",
    "            \n",
    "    return loss, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_func(features, labels, architecture, FLAGS):\n",
    "    # Build the forward model\n",
    "    FLAGS.allow_summaries = False\n",
    "    net = GPUNetworkBuilder(dtype=DT_FLOAT)\n",
    "    train_flag = tf.placeholder(tf.bool, None, name='train_flag')    \n",
    "    hidden_out = net.add_hidden_layers(features, architecture, FLAGS, train_flag)\n",
    "    # Linear output layer for the logits\n",
    "    logits = (net.nn_layer(hidden_out, architecture['n_classes'],'9_softmax_linear', FLAGS, 'IDENT', train_flag))\n",
    "                \n",
    "    return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Logger Xworkers_1mill_0 (DEBUG)>\n"
     ]
    }
   ],
   "source": [
    "global_start_time = time.time()\n",
    "tf.set_random_seed(1234+hvd.rank())\n",
    "np.random.seed(4321+hvd.rank())\n",
    "\n",
    "# create logger:\n",
    "log_name = FLAGS.name + '_' + str(hvd.rank())\n",
    "logger = logging.getLogger(log_name)\n",
    "logger.setLevel(logging.DEBUG)  # INFO, ERROR\n",
    "# file handler which logs debug messages\n",
    "fh = logging.FileHandler(os.path.join(FLAGS.logdir, log_name + '.log'))\n",
    "fh.setLevel(logging.DEBUG)\n",
    "# console handler\n",
    "ch = logging.StreamHandler()\n",
    "ch.setLevel(logging.INFO)\n",
    "# add formatter to the handlers\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "fh.setFormatter(formatter)\n",
    "ch.setFormatter(formatter)\n",
    "# add handlers to logger\n",
    "logger.addHandler(fh)\n",
    "logger.addHandler(ch)\n",
    "print(logger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download the data if it has not been downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-20 20:35:34,510 - Xworkers_1mill_0 - INFO - FTP connection stablished by worker:  0\n",
      "2018-11-20 20:35:34,510 - Xworkers_1mill_0 - INFO - FTP connection stablished by worker:  0\n",
      "2018-11-20 20:35:34,729 - Xworkers_1mill_0 - INFO - FTP connection closed by worker:  0\n",
      "2018-11-20 20:35:34,729 - Xworkers_1mill_0 - INFO - FTP connection closed by worker:  0\n"
     ]
    }
   ],
   "source": [
    "def download_data(rank, FLAGS):\n",
    "    server = ftplib.FTP()\n",
    "    server.connect(str(os.environ.get(\"FTP_HOST\")), int(os.environ.get(\"FTP_PORT\")))\n",
    "    server.login(os.environ.get(\"FTP_USER\"), os.environ.get(\"FTP_PASS\"))\n",
    "\n",
    "    server.cwd(FLAGS.ftp_dir)               # change into ftp_dir directory\n",
    "    logger.info(\"FTP connection stablished by worker:  {}\".format(rank))\n",
    "    \n",
    "    filenames = server.nlst() # get filenames within the directory\n",
    "        \n",
    "    train_suffix = 'train_%d.h5' % rank\n",
    "    if (rank==0):\n",
    "        if FLAGS.eval:        \n",
    "            fname_suffix = 'test_%d.h5' % rank\n",
    "            filenames = [elem for elem in filenames if fname_suffix in elem]        \n",
    "        else:\n",
    "            valid_suffix = 'valid_%d.h5' % rank\n",
    "            filenames = [elem for elem in filenames if (train_suffix in elem or valid_suffix in elem)]                \n",
    "    else:\n",
    "        filenames = [elem for elem in filenames if (train_suffix in elem)]\n",
    "\n",
    "    for filename in filenames:        \n",
    "        if FLAGS.eval:\n",
    "            local_path = os.path.join(PRO_DIR, FLAGS.test_dir, filename)    \n",
    "        else:\n",
    "            if (str('train') in filename[-10:-5]):\n",
    "                local_path = os.path.join(PRO_DIR, FLAGS.train_dir, filename)    \n",
    "            elif (str('valid') in filename[-10:-5]):\n",
    "                local_path = os.path.join(PRO_DIR, FLAGS.valid_dir, filename)   \n",
    "            else: \n",
    "                continue\n",
    "                \n",
    "        if not os.path.exists(local_path):            \n",
    "            file = open(local_path, 'wb')\n",
    "            server.retrbinary('RETR '+ filename, file.write, 8*1024)            \n",
    "            file.close()\n",
    "            logger.info(\"file downloaded:  {}\".format(filename))\n",
    "\n",
    "    server.quit() # This is the polite way to close a connection\n",
    "    logger.info(\"FTP connection closed by worker:  {}\".format(rank))\n",
    "\n",
    "def download_data_by_rank(rank, FLAGS):\n",
    "    server = ftplib.FTP()\n",
    "    server.connect(str(os.environ.get(\"FTP_HOST\")), int(os.environ.get(\"FTP_PORT\")))\n",
    "    server.login(os.environ.get(\"FTP_USER\"), os.environ.get(\"FTP_PASS\"))\n",
    "\n",
    "    server.cwd(FLAGS.ftp_dir)               # change into ftp_dir directory\n",
    "    logger.info(\"FTP connection stablished by worker:  {}\".format(rank))\n",
    "    \n",
    "    filenames = server.nlst() # get filenames within the directory\n",
    "    fname_suffix = '_%d.h5' % rank\n",
    "    filenames = [elem for elem in filenames if fname_suffix in elem]        \n",
    "    \n",
    "    for filename in filenames:            \n",
    "        local_path = os.path.join(PRO_DIR, FLAGS.train_dir, filename)                    \n",
    "        if not os.path.exists(local_path):            \n",
    "            file = open(local_path, 'wb')\n",
    "            server.retrbinary('RETR '+ filename, file.write, 8*1024)            \n",
    "            file.close()\n",
    "            logger.info(\"file downloaded:  {}\".format(filename))\n",
    "\n",
    "    server.quit() \n",
    "    logger.info(\"FTP connection closed by worker:  {}\".format(rank))\n",
    "\n",
    "\n",
    "#download_data_by_rank(hvd.rank(), FLAGS)\n",
    "download_data(hvd.rank(), FLAGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_records(tf_record_pattern):\n",
    "    def count_records(file_name):\n",
    "        count = 0\n",
    "        for _ in tf.python_io.tf_record_iterator(tf_record_filename):\n",
    "            count += 1\n",
    "        return count\n",
    "    filenames = sorted(tf.gfile.Glob(tf_record_pattern))\n",
    "    nfile = len(filenames)\n",
    "    return (count_records(filenames[0])*(nfile-1) +\n",
    "            count_records(filenames[-1]))\n",
    "\n",
    "def get_files_dict(FLAGS):        \n",
    "    ext = \"*.h5\"\n",
    "\n",
    "    if (hvd.rank()==0):\n",
    "        files_dict = {'train': glob.glob(os.path.join(PRO_DIR, FLAGS.train_dir, ext)), \n",
    "                      'valid': glob.glob(os.path.join(PRO_DIR, FLAGS.valid_dir, ext)), \n",
    "                      'test': glob.glob(os.path.join(PRO_DIR, FLAGS.test_dir, ext))}\n",
    "    else:\n",
    "        files_dict = {'train': glob.glob(os.path.join(PRO_DIR, FLAGS.train_dir, ext))}\n",
    "\n",
    "    return files_dict\n",
    "\n",
    "def architecture_settings(files_dict, FLAGS):\n",
    "    architecture = {}\n",
    "    architecture['rank'] = hvd.rank()\n",
    "    ok_inputs = True\n",
    "    for key in files_dict.keys():\n",
    "        total_records = 0\n",
    "        for file in files_dict[key]:                                \n",
    "            with pd.HDFStore(file) as dataset_file:\n",
    "                if (ok_inputs): \n",
    "                    index_length = len(dataset_file.get_storer(key+'/features').attrs.data_columns)\n",
    "                    architecture['n_input'] = dataset_file.get_storer(key+ '/features').ncols - index_length\n",
    "                    architecture['n_classes'] = dataset_file.get_storer(key+'/labels').ncols - index_length\n",
    "                    ok_inputs = False                \n",
    "                total_records += dataset_file.get_storer(key + '/features').nrows\n",
    "        architecture[key + '_num_examples'] = total_records                            \n",
    "    \n",
    "    if FLAGS.eval:\n",
    "        architecture['total_num_examples'] = architecture['test_num_examples']\n",
    "    else:\n",
    "        if FLAGS.total_examples == -1:\n",
    "            architecture['total_num_examples'] = architecture['train_num_examples']\n",
    "        else:\n",
    "            architecture['total_num_examples'] = FLAGS.total_examples \n",
    "    \n",
    "    for hid_i in range(1, FLAGS.n_hidden+1):\n",
    "        architecture['n_hidden_{:1d}'.format(hid_i)] = FLAGS.s_hidden[hid_i-1]\n",
    "    # print('rank: ', hvd.rank(), 'architecture', architecture)   \n",
    "    # time.sleep(5)\n",
    "    return architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To sum up the dataset per worker (assuming the same size of files per worker approximately):\n",
    "files_dict = get_files_dict(FLAGS)\n",
    "architecture = architecture_settings(files_dict, FLAGS)\n",
    "\n",
    "nrecord = architecture['total_num_examples']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-20 20:35:34,807 - Xworkers_1mill_0 - INFO - Num ranks:  1\n",
      "2018-11-20 20:35:34,807 - Xworkers_1mill_0 - INFO - Num ranks:  1\n",
      "2018-11-20 20:35:34,810 - Xworkers_1mill_0 - INFO - Num of records: 26769588\n",
      "2018-11-20 20:35:34,810 - Xworkers_1mill_0 - INFO - Num of records: 26769588\n",
      "2018-11-20 20:35:34,811 - Xworkers_1mill_0 - INFO - Total batch size: 8850\n",
      "2018-11-20 20:35:34,811 - Xworkers_1mill_0 - INFO - Total batch size: 8850\n",
      "2018-11-20 20:35:34,813 - Xworkers_1mill_0 - INFO - 8850, per device\n",
      "2018-11-20 20:35:34,813 - Xworkers_1mill_0 - INFO - 8850, per device\n",
      "2018-11-20 20:35:34,815 - Xworkers_1mill_0 - INFO - Data type: <dtype: 'float32'>\n",
      "2018-11-20 20:35:34,815 - Xworkers_1mill_0 - INFO - Data type: <dtype: 'float32'>\n",
      "2018-11-20 20:35:34,816 - Xworkers_1mill_0 - INFO - architecture: {'rank': 0, 'n_input': 258, 'n_classes': 7, 'train_num_examples': 26769588, 'valid_num_examples': 901868, 'test_num_examples': 0, 'total_num_examples': 26769588, 'n_hidden_1': 200, 'n_hidden_2': 140, 'n_hidden_3': 140}\n",
      "2018-11-20 20:35:34,816 - Xworkers_1mill_0 - INFO - architecture: {'rank': 0, 'n_input': 258, 'n_classes': 7, 'train_num_examples': 26769588, 'valid_num_examples': 901868, 'test_num_examples': 0, 'total_num_examples': 26769588, 'n_hidden_1': 200, 'n_hidden_2': 140, 'n_hidden_3': 140}\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"Num ranks:  {}\".format(hvd.size()))\n",
    "logger.info(\"Num of records: {}\".format(nrecord))\n",
    "logger.info(\"Total batch size: {}\".format(FLAGS.batch_size * hvd.size()))\n",
    "logger.info(\"{}, per device\".format(FLAGS.batch_size))\n",
    "logger.info(\"Data type: {}\".format(DT_FLOAT)) \n",
    "logger.info(\"architecture: {}\".format(architecture)) \n",
    "# time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-20 20:35:34,827 - Xworkers_1mill_0 - INFO - Number of steps per epoch: 3024\n",
      "2018-11-20 20:35:34,827 - Xworkers_1mill_0 - INFO - Number of steps per epoch: 3024\n",
      "2018-11-20 20:35:34,836 - Xworkers_1mill_0 - INFO - Building training graph\n",
      "2018-11-20 20:35:34,836 - Xworkers_1mill_0 - INFO - Building training graph\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/contrib/metrics/python/ops/histogram_ops.py:83: remove_squeezable_dimensions (from tensorflow.contrib.framework.python.framework.tensor_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please switch to remove_squeezable_dimensions from tf.confusion_matrix. Note that the order of the inputs and outputs of labels and predictions have also been switched.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-20 20:35:39,762 - tensorflow - WARNING - From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/contrib/metrics/python/ops/histogram_ops.py:83: remove_squeezable_dimensions (from tensorflow.contrib.framework.python.framework.tensor_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please switch to remove_squeezable_dimensions from tf.confusion_matrix. Note that the order of the inputs and outputs of labels and predictions have also been switched.\n",
      "2018-11-20 20:35:52,587 - Xworkers_1mill_0 - INFO - Graph building completed....\n",
      "2018-11-20 20:35:52,587 - Xworkers_1mill_0 - INFO - Graph building completed....\n",
      "2018-11-20 20:35:52,594 - Xworkers_1mill_0 - INFO - Creating session\n",
      "2018-11-20 20:35:52,594 - Xworkers_1mill_0 - INFO - Creating session\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.FeedForward object at 0x7f58c8051080>\n"
     ]
    }
   ],
   "source": [
    "if FLAGS.eval:\n",
    "    if FLAGS.test_dir is None:\n",
    "        logger.error(\"eval requires data_dir to be specified\")\n",
    "        raise ValueError(\"eval requires data_dir to be specified\")\n",
    "    if hvd.size() > 1:\n",
    "        logger.error(\"Multi-GPU evaluation is not supported\")\n",
    "        raise ValueError(\"Multi-GPU evaluation is not supported\")\n",
    "    evaluator = FeedForward(eval_func, dtype='evaluator')\n",
    "    logger.info(\"Building evaluation graph\")\n",
    "    conf_mtx_op, accuracy_op, better_acc_op, precision_op = evaluator.evaluation_step(FLAGS.test_batch_size)    \n",
    "    print(evaluator)\n",
    "else:    \n",
    "    nstep_per_epoch = nrecord // FLAGS.batch_size # if it is kwnow the total size: (FLAGS.batch_size * hvd.size())\n",
    "    logger.info(\"Number of steps per epoch: %d\" % nstep_per_epoch)\n",
    "    # model_func = lambda features, labels, architecture, FLAGS: loss_func(features, labels, architecture, FLAGS) # inference_vgg(net, images, nlayer)\n",
    "    trainer = FeedForward(loss_func, nstep_per_epoch=nstep_per_epoch)\n",
    "    logger.info(\"Building training graph\")    \n",
    "    train_ops, learning_rate_op, conf_mtx_op, accuracy_op, better_acc_op, precision_op, auc_list_op, auc_mean_op, m_list_op, m_list_mean_op, total_loss_op, lloss_op, auc_pr_op, auc_pr_mean_op, auc_data_op = trainer.training_step(architecture, FLAGS)\n",
    "    logger.info(\"Graph building completed....\")\n",
    "    print(trainer)\n",
    "    global_init, local_init = trainer.init()\n",
    "\n",
    "logger.info(\"Creating session\")\n",
    "config = tf.ConfigProto(allow_soft_placement = True)\n",
    "config.intra_op_parallelism_threads = 1\n",
    "config.inter_op_parallelism_threads = 10\n",
    "config.gpu_options.force_gpu_compatible = True\n",
    "config.gpu_options.visible_device_list = str(hvd.local_rank())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(FLAGS.logdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining summary (writer) and checkpoint (saver) files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-20 20:35:55,606 - Xworkers_1mill_0 - INFO - Initializing variables\n",
      "2018-11-20 20:35:55,606 - Xworkers_1mill_0 - INFO - Initializing variables\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session(config=config)\n",
    "\n",
    "train_writer = None\n",
    "valid_writer = None\n",
    "saver = None\n",
    "summary_ops = None\n",
    "\n",
    "\n",
    "if hvd.rank() == 0 and FLAGS.logdir is not None:\n",
    "    train_writer = tf.summary.FileWriter(os.path.join(FLAGS.logdir), sess.graph)\n",
    "    valid_writer = tf.summary.FileWriter(os.path.join(FLAGS.logdir, 'valid'), graph=None)\n",
    "    summary_ops = tf.summary.merge_all()\n",
    "    last_summary_time = time.time()\n",
    "    saver = tf.train.Saver(keep_checkpoint_every_n_hours=1)\n",
    "    last_save_time = time.time()\n",
    "\n",
    "if not FLAGS.eval:        \n",
    "    logger.info(\"Initializing variables\")    \n",
    "    sess.run([global_init, local_init])\n",
    "\n",
    "restored = False\n",
    "if hvd.rank() == 0 and saver is not None:\n",
    "    ckpt = tf.train.get_checkpoint_state(FLAGS.logdir)\n",
    "    checkpoint_file = os.path.join(FLAGS.logdir, \"checkpoint\")\n",
    "    if ckpt and ckpt.model_checkpoint_path:\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "        restored = True\n",
    "        logger.info(\"Restored session from checkpoint {}\".format(ckpt.model_checkpoint_path))\n",
    "    else:\n",
    "        if not os.path.exists(FLAGS.logdir):\n",
    "            os.mkdir(FLAGS.logdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running evaluation from a checkpoint file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_better_acc(conf_mtx, axis=1):\n",
    "    cfsum = conf_mtx.sum(axis=axis, keepdims=True)\n",
    "    conf_mtx1 = np.divide(conf_mtx, cfsum, out=np.zeros_like(conf_mtx, dtype=np.float32), where=(cfsum!=0), dtype=np.float32)    \n",
    "    bett_acc = conf_mtx1.diagonal().mean()\n",
    "    return bett_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/MLMortgage/data/processed/chuncks_random_c1millx2_train/temporalloandynmodifmrstaticitur15mill-16mill-pp-train_0.h5 ...to load\n",
      "class No.:  R\n",
      "/home/ubuntu/MLMortgage/data/processed/chuncks_random_c1millx2_train/temporalloandynmodifmrstaticitur15mill-16mill-pp-train_0.h5  class  R : loaded in RAM, total_rows:  27885\n",
      "class No.:  9\n",
      "/home/ubuntu/MLMortgage/data/processed/chuncks_random_c1millx2_train/temporalloandynmodifmrstaticitur15mill-16mill-pp-train_0.h5  class  9 : loaded in RAM, total_rows:  614496\n",
      "class No.:  0\n",
      "/home/ubuntu/MLMortgage/data/processed/chuncks_random_c1millx2_train/temporalloandynmodifmrstaticitur15mill-16mill-pp-train_0.h5  class  0 : loaded in RAM, total_rows:  445264\n",
      "class No.:  6\n",
      "/home/ubuntu/MLMortgage/data/processed/chuncks_random_c1millx2_train/temporalloandynmodifmrstaticitur15mill-16mill-pp-train_0.h5  class  6 : loaded in RAM, total_rows:  259623\n",
      "class No.:  F\n",
      "/home/ubuntu/MLMortgage/data/processed/chuncks_random_c1millx2_train/temporalloandynmodifmrstaticitur15mill-16mill-pp-train_0.h5  class  F : loaded in RAM, total_rows:  499157\n",
      "class No.:  3\n",
      "/home/ubuntu/MLMortgage/data/processed/chuncks_random_c1millx2_train/temporalloandynmodifmrstaticitur15mill-16mill-pp-train_0.h5  class  3 : loaded in RAM, total_rows:  736172\n",
      "Majority class: C\n",
      "/home/ubuntu/MLMortgage/data/processed/chuncks_random_c1millx2_train/temporalloandynmodifmrstaticitur15mill-16mill-pp-train_0.h5  class C: loaded in RAM, total_rows:  24186991\n",
      "/home/ubuntu/MLMortgage/data/processed/chuncks_random_c1millx2_valid/temporalloandynmodifmrstaticitur15mill-16mill-pp-valid_0.h5 ...to load\n",
      "/home/ubuntu/MLMortgage/data/processed/chuncks_random_c1millx2_valid/temporalloandynmodifmrstaticitur15mill-16mill-pp-valid_0.h5  loaded in RAM\n",
      "class_weights [ 14560  19805   6681  15002 835450   9960    410]\n",
      "class_weights [0.984 0.978 0.993 0.983 0.074 0.989 1.   ]\n"
     ]
    }
   ],
   "source": [
    "if not FLAGS.eval:\n",
    "\n",
    "    if (hvd.rank()==0):\n",
    "        DATA = md.get_h5_data(PRO_DIR, architecture, FLAGS.train_dir, FLAGS.valid_dir, None, train_period=FLAGS.train_period, valid_period=FLAGS.valid_period, test_period=FLAGS.test_period)         \n",
    "    else:\n",
    "        DATA = md.get_h5_data(PRO_DIR, architecture, FLAGS.train_dir, None, None, train_period=FLAGS.train_period, valid_period=FLAGS.valid_period, test_period=FLAGS.test_period) \n",
    "        \n",
    "    #logger.info('Features List: {}'.format(DATA.train.features_list))\n",
    "    #logger.info('Labels List: {}'.format(DATA.train.labels_list))\n",
    "    \n",
    "else:\n",
    "    DATA = md.get_h5_data(PRO_DIR, architecture, None, None, FLAGS.test_dir, train_period=FLAGS.train_period, valid_period=FLAGS.valid_period, test_period=FLAGS.test_period) \n",
    "    #logger.info('Features List: {}'.format(DATA.test.features_list))\n",
    "    #logger.info('Labels List: {}'.format(DATA.test.labels_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24186991\n",
      "26769588\n"
     ]
    }
   ],
   "source": [
    "print(DATA.train._dict['C']['nrows'])\n",
    "print(nrecord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feed_dict(tag, DATA, FLAGS):\n",
    "    \"\"\"Create the feed dictionary for mapping data onto placeholders in the graph.\"\"\"\n",
    "    if tag == 'batch':\n",
    "        if hvd.rank()==0:\n",
    "            batch_size = FLAGS.batch_size // 2\n",
    "        else:\n",
    "            batch_size = FLAGS.batch_size\n",
    "        \n",
    "        features, targets = DATA.train.next_balanced_batch(batch_size) #DATA.train.next_random_batch(batch_size) \n",
    "        #print(features[-10:], targets[-10:])\n",
    "        ##example_weights = np.sum(np.multiply(np.float32(oh_labels),class_weights4), axis=1)\n",
    "        #example_weights = np.sum(np.multiply(np.float32(targets),DATA.train.class_weights), axis=1)\n",
    "        #print('example_weights', example_weights)\n",
    "        example_weights = np.array([1.0], dtype=NP_FLOAT) # [1.0] * architecture['total_num_examples']                    \n",
    "        # logger.info('features shape: {}'.format(features.shape))\n",
    "    elif tag == 'train':\n",
    "        features = DATA.train.orig.features\n",
    "        targets = DATA.train.orig.labels\n",
    "        example_weights = np.ones_like(targets.iloc[:, 1].values)\n",
    "    elif tag == 'valid':\n",
    "        features, targets, example_weights = DATA.validation.next_sequential_batch(FLAGS.valid_batch_size)\n",
    "    else:\n",
    "        features, targets, example_weights = DATA.test.next_sequential_batch(FLAGS.test_batch_size)\n",
    "\n",
    "    # features[:, :7] = targets\n",
    "    if tag == 'batch':\n",
    "        k_prob_input = 0.9  # 0.9  # .85  # .75  # 0.8  # 0.6\n",
    "        k_prob = FLAGS.dropout_keep\n",
    "        t_flag = True\n",
    "    else:\n",
    "        k_prob_input = 1.0\n",
    "        k_prob = 1.0\n",
    "        t_flag = False\n",
    "\n",
    "    # Change the python dictionary to an io-buffer for a better performance.\n",
    "    # See here:\n",
    "    # https://www.tensorflow.org/performance/performance_guide\n",
    "    feed_d = {\n",
    "        'features:0': features,\n",
    "        'targets:0': targets,\n",
    "        'example_weights:0': example_weights,        \n",
    "        'train_flag:0': t_flag,\n",
    "        #'epoch_flag:0': FLAGS.epoch_flag,\n",
    "        #'1_hidden/dropout/keep_proba:0': k_prob_input,\n",
    "        #'2_hidden/dropout/keep_proba:0': k_prob,\n",
    "        # '3_hidden/dropout/keep_proba:0': k_prob,\n",
    "        # '4_hidden/dropout/keep_proba:0': k_prob,\n",
    "        # '5_hidden/dropout/keep_proba:0': k_prob,\n",
    "        '9_softmax_linear/dropout/keep_proba:0': k_prob\n",
    "    }\n",
    "\t\n",
    "\t# for any tag:\n",
    "    if (FLAGS.n_hidden > 0) :\n",
    "        # print ('k_prob_input', k_prob_input, type(k_prob_input))\n",
    "        feed_d['1_hidden/dropout/keep_proba:0'] = k_prob_input\n",
    "        for hid_i in range(2, FLAGS.n_hidden+1):\n",
    "            feed_d['{:1d}_hidden/dropout/keep_proba:0'.format(hid_i)] = k_prob\n",
    "    # print('feed_d', feed_d)\n",
    "    # print('batch shape: ', features.shape)    \n",
    "    return feed_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_evaluation(nstep, sess, enqueue_ops):\n",
    "    logger.info(\"Evaluating Model\")\n",
    "    accuracys = []\n",
    "    better_accs = []\n",
    "    logger.info(\"  Step  Accuracy  Better-Accuracy   Precision   f1score_micro   f1score_macro\")\n",
    "    for step in range(nstep):\n",
    "        try:\n",
    "            feed = create_feed_dict('test', DATA, FLAGS)     \n",
    "            #logger.info('feed dictionary was created')\n",
    "            conf_mtx, accuracy, better_acc, precision = sess.run(enqueue_ops, feed_dict=feed)\n",
    "            #logger.info('operations were ran')\n",
    "            if (math.isnan(better_acc)):                \n",
    "                better_acc = calculate_better_acc(conf_mtx)                    \n",
    "                #print('beter_acc nan')\n",
    "            if (math.isnan(precision)):                \n",
    "                precision = calculate_better_acc(conf_mtx, axis=0)                    \n",
    "                #print('precision nan')\n",
    "            f1score_micro = 2 * precision * better_acc / (precision + better_acc)\n",
    "            f1score_macro = calculate_macro_f1score(np.array(conf_mtx, dtype='float32'))\n",
    "            \n",
    "            #if step == 0 or (step+1) % FLAGS.display_every == 0:\n",
    "            logger.info(\"% 6i %5.1f%% %5.1f%% %5.1f%% %5.5f%%  %5.5f%%\" % (step+1, accuracy*100, better_acc*100, precision*100, f1score_micro, f1score_macro))\n",
    "            accuracys.append(accuracy)\n",
    "            better_accs.append(better_acc)\n",
    "            #print('acc and bett_acc appended')\n",
    "            \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"Keyboard interrupt\")\n",
    "            break\n",
    "        except  Exception  as e:        \n",
    "            raise ValueError('Error running Evaluation: ' + str(e))    \n",
    "\n",
    "    nstep = len(accuracys)\n",
    "    if nstep == 0:\n",
    "        return\n",
    "    accuracys = np.asarray(accuracys) * 100.\n",
    "    better_accs = np.asarray(better_accs) * 100.\n",
    "    acc_mean = np.mean(accuracys)\n",
    "    bettacc_mean = np.mean(better_accs)\n",
    "    if nstep > 2:\n",
    "        acc_uncertainty = np.std(accuracys, ddof=1) / np.sqrt(float(nstep))\n",
    "        bettacc_uncertainty = np.std(better_accs, ddof=1) / np.sqrt(float(nstep))\n",
    "    else:\n",
    "        acc_uncertainty = float('nan')\n",
    "        bettacc_uncertainty = float('nan')\n",
    "    acc_madstd = 1.4826*np.median(np.abs(accuracys - acc_mean))\n",
    "    bettacc_madstd = 1.4826*np.median(np.abs(better_accs - bettacc_mean))\n",
    "    logger.info('-' * 64)\n",
    "    logger.info('Validation Accuracy: %.3f %% +/- %.2f (jitter = %.1f)' % (\n",
    "        acc_mean, acc_uncertainty, acc_madstd))\n",
    "    logger.info('Validation Better Accuracy: %.3f %% +/- %.2f (jitter = %.1f)' % (\n",
    "        bettacc_mean, bettacc_uncertainty, bettacc_madstd))\n",
    "    logger.info('-' * 64)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "nstep = 0\n",
    "if FLAGS.eval:\n",
    "    if not restored:\n",
    "        logger.error(\"No checkpoint found for evaluation\")\n",
    "        raise ValueError(\"No checkpoint found for evaluation\")\n",
    "    else:        \n",
    "        #nstep = nrecord // FLAGS.test_batch_size \n",
    "        nstep = DATA.test.total_num_batch(FLAGS.test_batch_size) \n",
    "        logger.info(\"total steps: {}\".format(nstep))        \n",
    "        enq_ops = [conf_mtx_op, accuracy_op, better_acc_op, precision_op]\n",
    "        logger.info(\"Executing Evaluation\")        \n",
    "        run_evaluation(nstep, sess, enq_ops)   \n",
    "        logger.info('Evaluation was done')\n",
    "        # sys.exit(0) #the following instructiones will not be  executed\n",
    "        quit()\n",
    "else:    \n",
    "    if FLAGS.epoch_num is not None:\n",
    "        if (nrecord <= 0):\n",
    "            logger.error(\"num_epochs requires nrecord to be specified\")\n",
    "            raise ValueError(\"num_epochs requires nrecord to be specified\")\n",
    "        nstep = math.ceil(np.float32(nrecord * FLAGS.epoch_num / FLAGS.batch_size)) # if it is kwnow the total size: (FLAGS.batch_size * hvd.size()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-20 20:52:30,251 - Xworkers_1mill_0 - INFO - METRICS:  Namespace(allow_summaries=True, batch_layer_type='batch', batch_norm=True, batch_size=8850, batch_type='layer', data_dir='/input_data', decay_rate=1, decay_step=3520000, display_every=100, dropout=True, dropout_keep=0.9, epoch_flag=0, epoch_num=15, eval=False, ftp_dir='processed/c1mill', learning_rate=0.1, log_file=None, logdir=PosixPath('/home/ubuntu/summ_15ep_2wrk'), loss_tolerance=0.0001, lr_decay_epochs=30, lr_decay_policy='time', lr_decay_rate=0.1, lr_poly_power=2.0, max_epoch_size=-1, momentum=0.5, n_hidden=3, name='Xworkers_1mill', nstep_burnin=20, rate_min=0.0015, reg_rate=1e-05, s_hidden=[200, 140, 140], save_interval=450, stratified_flag=False, summary_interval=1800, test_batch_size=1200000, test_dir='chuncks_random_c1millx2_test', test_flag=True, test_period=[330, 342], total_examples=-1, train_dir='chuncks_random_c1millx2_train', train_period=[121, 323], valid_batch_size=1200000, valid_dir='chuncks_random_c1millx2_valid', valid_period=[324, 329], weighted_sampling=False, xla=True)\n",
      "\n",
      "2018-11-20 20:52:30,251 - Xworkers_1mill_0 - INFO - METRICS:  Namespace(allow_summaries=True, batch_layer_type='batch', batch_norm=True, batch_size=8850, batch_type='layer', data_dir='/input_data', decay_rate=1, decay_step=3520000, display_every=100, dropout=True, dropout_keep=0.9, epoch_flag=0, epoch_num=15, eval=False, ftp_dir='processed/c1mill', learning_rate=0.1, log_file=None, logdir=PosixPath('/home/ubuntu/summ_15ep_2wrk'), loss_tolerance=0.0001, lr_decay_epochs=30, lr_decay_policy='time', lr_decay_rate=0.1, lr_poly_power=2.0, max_epoch_size=-1, momentum=0.5, n_hidden=3, name='Xworkers_1mill', nstep_burnin=20, rate_min=0.0015, reg_rate=1e-05, s_hidden=[200, 140, 140], save_interval=450, stratified_flag=False, summary_interval=1800, test_batch_size=1200000, test_dir='chuncks_random_c1millx2_test', test_flag=True, test_period=[330, 342], total_examples=-1, train_dir='chuncks_random_c1millx2_train', train_period=[121, 323], valid_batch_size=1200000, valid_dir='chuncks_random_c1millx2_valid', valid_period=[324, 329], weighted_sampling=False, xla=True)\n",
      "\n",
      "2018-11-20 20:52:30,253 - Xworkers_1mill_0 - INFO - Number of total steps: 45373\n",
      "2018-11-20 20:52:30,253 - Xworkers_1mill_0 - INFO - Number of total steps: 45373\n"
     ]
    }
   ],
   "source": [
    "logger.info('METRICS:  %s\\r\\n' % str(FLAGS))\n",
    "logger.info('Number of total steps: %d' % nstep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# broadcast_global_variables from hvd\n",
    "trainer.sync(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-20 20:52:33,149 - Xworkers_1mill_0 - INFO - Writing summaries to /home/ubuntu/summ_15ep_2wrk\n",
      "2018-11-20 20:52:33,149 - Xworkers_1mill_0 - INFO - Writing summaries to /home/ubuntu/summ_15ep_2wrk\n",
      "2018-11-20 20:52:33,152 - Xworkers_1mill_0 - INFO - Training\n",
      "2018-11-20 20:52:33,152 - Xworkers_1mill_0 - INFO - Training\n",
      "2018-11-20 20:52:33,154 - Xworkers_1mill_0 - INFO -   Step; Epoch; time-per-record(sec);  batchtime/worker(sec);  Loss;   Learning Rate; Accuracy; Recall; Precision; F1score\n",
      "2018-11-20 20:52:33,154 - Xworkers_1mill_0 - INFO -   Step; Epoch; time-per-record(sec);  batchtime/worker(sec);  Loss;   Learning Rate; Accuracy; Recall; Precision; F1score\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint written to /home/ubuntu/summ_15ep_2wrk/checkpoint-0\n"
     ]
    }
   ],
   "source": [
    "# Trying to restore for training:\n",
    "if hvd.rank() == 0 and not restored:\n",
    "    if saver is not None:\n",
    "        save_path = saver.save(sess, checkpoint_file, global_step=0)\n",
    "        print(\"Checkpoint written to\", save_path)\n",
    "\n",
    "logger.info(\"Writing summaries to {}\".format(FLAGS.logdir))\n",
    "logger.info(\"Training\")\n",
    "logger.info(\"  Step; Epoch; time-per-record(sec);  batchtime/worker(sec);  Loss;   Learning Rate; Accuracy; Recall; Precision; F1score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_and_update(sess, local_init, feed_dict):\n",
    "    \"\"\"Reset the local variables and update the necessary update ops.\"\"\"\n",
    "    # sess.run(local_init) # this is necesary in each batch??check out the local variables!\n",
    "        \n",
    "    update_names_list = [\n",
    "        'metrics/auc/{:d}/hist_accumulate/update_op'.format(i)\n",
    "        for i in range(7)\n",
    "    ]\n",
    "\n",
    "    update_names_list.extend([\n",
    "        'metrics/m_measure/' + str(i) + str(j) + '/hist_accumulate/update_op'\n",
    "        for i in range(7) for j in range(7) if i != j\n",
    "    ])\n",
    "\n",
    "    sess.run(update_names_list, feed_dict=feed_dict)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_m_mtx(mtx):\n",
    "    \"\"\"Reshape the python list into a np array.\"\"\"\n",
    "    new_mtx = [0]\n",
    "    for i in range(6):\n",
    "        new_mtx.extend(mtx[i * 7:(i + 1) * 7])\n",
    "        new_mtx.append(0)\n",
    "    temp = np.array(new_mtx).reshape(7, 7)\n",
    "\n",
    "    return temp\n",
    "\n",
    "def print_stats(name, conf_mtx, accuracy, better_acc, precision, f1score_micro, f1score_macro, auc_list, auc_mean, m_list, m_list_mean, lloss, auc_pr, auc_pr_mean, loss):\n",
    "    \"\"\"Print to logger the given stats.\"\"\"        \n",
    "                \n",
    "    m_mtx = np.nan_to_num(reshape_m_mtx(m_list)) \n",
    "    auc_list = np.nan_to_num(auc_list)\n",
    "    conf_mtx = np.array(conf_mtx, dtype=int)\n",
    "            \n",
    "    stdout = 'Loss in ' + name +': {:.5f}\\n'.format(loss)        \n",
    "    stdout = stdout + ' Avg Log_Loss in ' + name +': {:.5f}\\n'.format(lloss)\n",
    "    stdout = stdout +  '{:s}:'.format(name) + ' (Silly) Global-ACC={:.5f}, Recall={:.5f},'.format(accuracy, better_acc) + \\\n",
    "        ' Avg M-Measure={:.4f},'.format(m_list_mean) + \\\n",
    "        ' Avg AUC_AOC={:.4f}'.format(auc_mean) + ' Avg AUC_PR={:.4f}\\n'.format(auc_pr_mean)\n",
    "    stdout = stdout + ' Precision= {:.5f} '.format(precision)\n",
    "    stdout = stdout + ' f1score_micro= {:.5f} '.format(f1score_micro)\n",
    "    stdout = stdout + ' f1score_macro= {:.5f}\\n'.format(f1score_macro)\n",
    "    stdout = stdout + (';').join(['Total Confusion Matrix', 'Total M-Measure Matrix', 'Total AUC_AOC', 'Total AUC_PR\\n'])\n",
    "    for conf_row, row, auc, auc_pr in zip(conf_mtx, m_mtx, auc_list, auc_pr):\n",
    "        for conf_value in conf_row:\n",
    "            stdout = stdout + '{}'.format(conf_value) + ';'\n",
    "        stdout = stdout + ';'\n",
    "        for value in row:\n",
    "            stdout = stdout + '{:.4f}'.format(value) + ';'\n",
    "        stdout = stdout + ';{:.4f}'.format(auc) + ' ;{:.4f}'.format(auc_pr) + '\\n'\n",
    "    stdout = stdout + '---------------------------------------------------------------------'\n",
    "              \n",
    "    logger.info('METRICS each %s (secs):  %s\\r\\n' % (FLAGS.summary_interval, stdout))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for validation set:\n",
    "def batching_dataset(sess, epoch, writer, tag, DATA, FLAGS):\n",
    "    if tag =='valid':\n",
    "        batch_num = DATA.validation.total_num_batch(FLAGS.valid_batch_size) \n",
    "        \n",
    "    # metrics = acc_metrics_init(DATA)\n",
    "    sess.run(local_init)\n",
    "    start_time = datetime.now()\n",
    "    acc_conf_mtx=np.zeros((DATA.train.num_classes, DATA.train.num_classes))        \n",
    "    metrics = [0.0] * 7\n",
    "    for batch_i in range(batch_num):\n",
    "        step = epoch * batch_num + batch_i    # total steps for all epochs        \n",
    "        feed = create_feed_dict(tag, DATA, FLAGS)     \n",
    "        reset_and_update(sess, local_init, feed)\n",
    "        summary, conf_mtx, accuracy, better_acc, precision, lloss, loss = sess.run([summary_ops, conf_mtx_op, accuracy_op, better_acc_op, precision_op, lloss_op, total_loss_op], feed_dict=feed)\n",
    "\n",
    "        if (math.isnan(better_acc)):\n",
    "            better_acc = calculate_better_acc(conf_mtx)\n",
    "\n",
    "        if (math.isnan(precision)):\n",
    "            precision = calculate_better_acc(conf_mtx, axis=0)\n",
    "        \n",
    "        f1score_micro = 2 * precision * better_acc / (precision + better_acc)\n",
    "        f1score_macro = calculate_macro_f1score(np.array(conf_mtx, dtype='float32'))\n",
    "        \n",
    "        acc_conf_mtx = np.add(acc_conf_mtx, conf_mtx)\n",
    "        metrics = np.add(metrics, np.array([accuracy, better_acc, precision, f1score_micro, f1score_macro, lloss, loss]))\n",
    "        writer.add_summary(summary, step)\n",
    "        writer.flush()\n",
    "        del feed\n",
    "    \n",
    "    metrics[:] = [x / batch_num for x in metrics]\n",
    "    valid_time = datetime.now() - start_time\n",
    "    logger.info('%s - Number of batches: %d; batch_size: %d; Total Time: %s' %(tag, batch_num,  FLAGS.valid_batch_size, valid_time))\n",
    "    return acc_conf_mtx, valid_time, metrics\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_train: \n",
      " Empty DataFrame\n",
      "Columns: [step, epoch, batch_time, Loss, LogLoss, Accuracy, Recall, Precision, f1score-Micro, f1score-Macro, M-Measure Mean, AUC_AOC Mean, AUC_PR Mean]\n",
      "Index: []\n",
      "df_valid: \n",
      " Empty DataFrame\n",
      "Columns: [step, epoch, batch_time, Loss, LogLoss, Accuracy, Recall, Precision, f1score-Micro, f1score-Macro]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "if hvd.rank() == 0:\n",
    "    if not FLAGS.eval:\n",
    "        dtype = ['step','epoch','batch_time','Loss','LogLoss','Accuracy','Recall','Precision', 'f1score-Micro', 'f1score-Macro', 'M-Measure Mean','AUC_AOC Mean','AUC_PR Mean']        \n",
    "        train_file = os.path.join(FLAGS.logdir, FLAGS.name + \"_train.csv\")\n",
    "        valid_file = os.path.join(FLAGS.logdir, FLAGS.name + \"_valid.csv\")        \n",
    "        \n",
    "        if not Path(train_file).exists():\n",
    "            df_train = pd.DataFrame(columns=dtype)                \n",
    "            df_train.to_csv(train_file, sep=';', index=False)\n",
    "        else:\n",
    "            df_train = pd.read_csv(train_file, sep=';')\n",
    "\n",
    "        if not Path(valid_file).exists():\n",
    "            df_valid = pd.DataFrame(columns=dtype[:10])\n",
    "            df_valid.to_csv(valid_file, sep=';', index=False)            \n",
    "        else:\n",
    "            df_valid = pd.read_csv(valid_file, sep=';')\n",
    "        \n",
    "        print('df_train: \\n', df_train)\n",
    "        print('df_valid: \\n', df_valid)\n",
    "        \n",
    "    else:  # validation set:\n",
    "        dtype = ['NN_name', 'NN_Number','Total Epochs', 'Execute Epochs', 'Total Training Time', 'Loss','LogLoss','Accuracy','Recall', 'Precision', 'f1score-Micro', 'f1score-Macro', 'M-Measure Mean','AUC_AOC Mean','AUC_PR Mean']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_macro_f1score(conf_mtx):\n",
    "    sum_byrows = conf_mtx.sum(axis=1, keepdims=True)\n",
    "    recall = np.diag(np.divide(conf_mtx, sum_byrows, out=np.zeros_like(conf_mtx), where=sum_byrows!=0)) #, where=sum_byrows!=0\n",
    "    #print('recall: ', recall)\n",
    "\n",
    "    sum_bycols = np.sum(conf_mtx, axis=0, keepdims=True)\n",
    "    #print('sum_bycols: ', sum_bycols)\n",
    "    precision = np.diag(np.divide(conf_mtx, sum_bycols, out=np.zeros_like(conf_mtx), where=sum_byrows!=0)) #where=sum_bycols!=0\n",
    "    #print('precision: ', precision)\n",
    "\n",
    "    sum_rec_prec = recall + precision\n",
    "    #print('sum_rec_prec: ', sum_rec_prec)\n",
    "    f1sc_perclass = np.divide((2 * recall * precision), sum_rec_prec, out=np.zeros_like(recall), where=sum_rec_prec!=0)\n",
    "\n",
    "\n",
    "    #print('f1sc_perclass: ', f1sc_perclass)\n",
    "    np.nan_to_num(f1sc_perclass, copy=False)\n",
    "    #if (recall==np.nan() or precision==np.nan()):\n",
    "    micro_f1sc = np.average(f1sc_perclass)\n",
    "    #print('micro_f1sc: ', micro_f1sc)\n",
    "    \n",
    "    return micro_f1sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py:8: RuntimeWarning: invalid value encountered in true_divide\n",
      "2018-11-20 20:53:06,595 - Xworkers_1mill_0 - INFO - METRICS each 1800 (secs):  Loss in ---Training in Summary---: 2.44492\n",
      " Avg Log_Loss in ---Training in Summary---: 0.49669\n",
      "---Training in Summary---: (Silly) Global-ACC=0.13537, Recall=0.13540, Avg M-Measure=0.4890, Avg AUC_AOC=0.4891 Avg AUC_PR=0.0000\n",
      " Precision= 0.10597  f1score_micro= 0.11889  f1score_macro= 0.07120\n",
      "Total Confusion Matrix;Total M-Measure Matrix;Total AUC_AOC;Total AUC_PR\n",
      "0;369;33;217;1;0;12;;0.0000;0.5065;0.5651;0.4627;0.4741;0.4084;0.4191;;0.4705 ;0.0000\n",
      "0;464;27;135;1;0;5;;0.5965;0.0000;0.5050;0.4698;0.5947;0.4530;0.4820;;0.5168 ;0.0000\n",
      "0;462;38;117;7;0;8;;0.4811;0.5292;0.0000;0.4675;0.5051;0.5028;0.4857;;0.4951 ;0.0000\n",
      "1;499;46;70;5;0;11;;0.3026;0.3789;0.4233;0.0000;0.2500;0.5364;0.5024;;0.3989 ;0.0000\n",
      "0;313;22;291;3;0;4;;0.4840;0.4103;0.3998;0.4910;0.0000;0.5493;0.5132;;0.4742 ;0.0000\n",
      "0;472;42;87;2;0;29;;0.4635;0.4668;0.4496;0.4868;0.4161;0.0000;0.5153;;0.4690 ;0.0000\n",
      "0;481;53;67;7;0;24;;0.6352;0.6366;0.5775;0.5728;0.6986;0.4719;0.0000;;0.5990 ;0.0000\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "2018-11-20 20:53:06,595 - Xworkers_1mill_0 - INFO - METRICS each 1800 (secs):  Loss in ---Training in Summary---: 2.44492\n",
      " Avg Log_Loss in ---Training in Summary---: 0.49669\n",
      "---Training in Summary---: (Silly) Global-ACC=0.13537, Recall=0.13540, Avg M-Measure=0.4890, Avg AUC_AOC=0.4891 Avg AUC_PR=0.0000\n",
      " Precision= 0.10597  f1score_micro= 0.11889  f1score_macro= 0.07120\n",
      "Total Confusion Matrix;Total M-Measure Matrix;Total AUC_AOC;Total AUC_PR\n",
      "0;369;33;217;1;0;12;;0.0000;0.5065;0.5651;0.4627;0.4741;0.4084;0.4191;;0.4705 ;0.0000\n",
      "0;464;27;135;1;0;5;;0.5965;0.0000;0.5050;0.4698;0.5947;0.4530;0.4820;;0.5168 ;0.0000\n",
      "0;462;38;117;7;0;8;;0.4811;0.5292;0.0000;0.4675;0.5051;0.5028;0.4857;;0.4951 ;0.0000\n",
      "1;499;46;70;5;0;11;;0.3026;0.3789;0.4233;0.0000;0.2500;0.5364;0.5024;;0.3989 ;0.0000\n",
      "0;313;22;291;3;0;4;;0.4840;0.4103;0.3998;0.4910;0.0000;0.5493;0.5132;;0.4742 ;0.0000\n",
      "0;472;42;87;2;0;29;;0.4635;0.4668;0.4496;0.4868;0.4161;0.0000;0.5153;;0.4690 ;0.0000\n",
      "0;481;53;67;7;0;24;;0.6352;0.6366;0.5775;0.5728;0.6986;0.4719;0.0000;;0.5990 ;0.0000\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "2018-11-20 20:53:06,599 - Xworkers_1mill_0 - INFO -      1;     1;  1205.4;   7.342; 2.44492; 0.10000; 0.13537; 0.13540; 0.10597; 0.11889; 0.07120\n",
      "2018-11-20 20:53:06,599 - Xworkers_1mill_0 - INFO -      1;     1;  1205.4;   7.342; 2.44492; 0.10000; 0.13537; 0.13540; 0.10597; 0.11889; 0.07120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint written to /home/ubuntu/summ_15ep_2wrk/checkpoint-1\n",
      "Checkpoint written to /home/ubuntu/summ_15ep_2wrk/checkpoint-2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-20 20:53:19,125 - Xworkers_1mill_0 - INFO -    100;     1; 149917.0;   0.059; 0.96562; 0.10000; 0.59638; 0.59634; 0.61847; 0.60720; 0.58073\n",
      "2018-11-20 20:53:19,125 - Xworkers_1mill_0 - INFO -    100;     1; 149917.0;   0.059; 0.96562; 0.10000; 0.59638; 0.59634; 0.61847; 0.60720; 0.58073\n",
      "2018-11-20 20:53:25,019 - Xworkers_1mill_0 - INFO -    200;     1; 153531.4;   0.058; 0.86141; 0.09999; 0.64316; 0.64316; 0.64871; 0.64592; 0.64120\n",
      "2018-11-20 20:53:25,019 - Xworkers_1mill_0 - INFO -    200;     1; 153531.4;   0.058; 0.86141; 0.09999; 0.64316; 0.64316; 0.64871; 0.64592; 0.64120\n",
      "2018-11-20 20:53:30,890 - Xworkers_1mill_0 - INFO -    300;     1; 149140.5;   0.059; 0.84413; 0.09999; 0.64904; 0.64903; 0.65633; 0.65266; 0.64601\n",
      "2018-11-20 20:53:30,890 - Xworkers_1mill_0 - INFO -    300;     1; 149140.5;   0.059; 0.84413; 0.09999; 0.64904; 0.64903; 0.65633; 0.65266; 0.64601\n",
      "2018-11-20 20:53:36,856 - Xworkers_1mill_0 - INFO -    400;     1; 143280.7;   0.062; 0.79484; 0.09999; 0.66441; 0.66442; 0.66902; 0.66671; 0.66151\n",
      "2018-11-20 20:53:36,856 - Xworkers_1mill_0 - INFO -    400;     1; 143280.7;   0.062; 0.79484; 0.09999; 0.66441; 0.66442; 0.66902; 0.66671; 0.66151\n",
      "2018-11-20 20:53:42,763 - Xworkers_1mill_0 - INFO -    500;     1; 153339.2;   0.058; 0.82637; 0.09999; 0.64768; 0.64767; 0.65578; 0.65170; 0.64525\n",
      "2018-11-20 20:53:42,763 - Xworkers_1mill_0 - INFO -    500;     1; 153339.2;   0.058; 0.82637; 0.09999; 0.64768; 0.64767; 0.65578; 0.65170; 0.64525\n",
      "2018-11-20 20:53:48,678 - Xworkers_1mill_0 - INFO -    600;     1; 154568.4;   0.057; 0.80107; 0.09998; 0.66621; 0.66622; 0.67044; 0.66832; 0.66200\n",
      "2018-11-20 20:53:48,678 - Xworkers_1mill_0 - INFO -    600;     1; 154568.4;   0.057; 0.80107; 0.09998; 0.66621; 0.66622; 0.67044; 0.66832; 0.66200\n",
      "2018-11-20 20:53:54,587 - Xworkers_1mill_0 - INFO -    700;     1; 141961.7;   0.062; 0.79229; 0.09998; 0.65401; 0.65401; 0.65690; 0.65545; 0.65131\n",
      "2018-11-20 20:53:54,587 - Xworkers_1mill_0 - INFO -    700;     1; 141961.7;   0.062; 0.79229; 0.09998; 0.65401; 0.65401; 0.65690; 0.65545; 0.65131\n",
      "2018-11-20 20:54:00,565 - Xworkers_1mill_0 - INFO -    800;     1; 151018.5;   0.059; 0.79210; 0.09998; 0.65763; 0.65763; 0.66585; 0.66171; 0.65536\n",
      "2018-11-20 20:54:00,565 - Xworkers_1mill_0 - INFO -    800;     1; 151018.5;   0.059; 0.79210; 0.09998; 0.65763; 0.65763; 0.66585; 0.66171; 0.65536\n",
      "2018-11-20 20:54:06,513 - Xworkers_1mill_0 - INFO -    900;     1; 146755.5;   0.060; 0.77305; 0.09997; 0.66780; 0.66780; 0.67377; 0.67077; 0.66607\n",
      "2018-11-20 20:54:06,513 - Xworkers_1mill_0 - INFO -    900;     1; 146755.5;   0.060; 0.77305; 0.09997; 0.66780; 0.66780; 0.67377; 0.67077; 0.66607\n",
      "2018-11-20 20:54:12,447 - Xworkers_1mill_0 - INFO -   1000;     1; 150876.7;   0.059; 0.78725; 0.09997; 0.66599; 0.66598; 0.68045; 0.67314; 0.66164\n",
      "2018-11-20 20:54:12,447 - Xworkers_1mill_0 - INFO -   1000;     1; 150876.7;   0.059; 0.78725; 0.09997; 0.66599; 0.66598; 0.68045; 0.67314; 0.66164\n",
      "2018-11-20 20:54:18,364 - Xworkers_1mill_0 - INFO -   1100;     1; 145736.2;   0.061; 0.79473; 0.09997; 0.65763; 0.65764; 0.65939; 0.65851; 0.65507\n",
      "2018-11-20 20:54:18,364 - Xworkers_1mill_0 - INFO -   1100;     1; 145736.2;   0.061; 0.79473; 0.09997; 0.65763; 0.65764; 0.65939; 0.65851; 0.65507\n",
      "2018-11-20 20:54:24,259 - Xworkers_1mill_0 - INFO -   1200;     1; 153977.2;   0.057; 0.75316; 0.09997; 0.68113; 0.68114; 0.68711; 0.68411; 0.67807\n",
      "2018-11-20 20:54:24,259 - Xworkers_1mill_0 - INFO -   1200;     1; 153977.2;   0.057; 0.75316; 0.09997; 0.68113; 0.68114; 0.68711; 0.68411; 0.67807\n",
      "2018-11-20 20:54:30,174 - Xworkers_1mill_0 - INFO -   1300;     1; 145629.3;   0.061; 0.74646; 0.09996; 0.67458; 0.67458; 0.67784; 0.67621; 0.67195\n",
      "2018-11-20 20:54:30,174 - Xworkers_1mill_0 - INFO -   1300;     1; 145629.3;   0.061; 0.74646; 0.09996; 0.67458; 0.67458; 0.67784; 0.67621; 0.67195\n",
      "2018-11-20 20:54:36,117 - Xworkers_1mill_0 - INFO -   1400;     1; 153894.8;   0.058; 0.80270; 0.09996; 0.65672; 0.65673; 0.65924; 0.65799; 0.65135\n",
      "2018-11-20 20:54:36,117 - Xworkers_1mill_0 - INFO -   1400;     1; 153894.8;   0.058; 0.80270; 0.09996; 0.65672; 0.65673; 0.65924; 0.65799; 0.65135\n",
      "2018-11-20 20:54:42,021 - Xworkers_1mill_0 - INFO -   1500;     1; 148774.7;   0.059; 0.76417; 0.09996; 0.66870; 0.66871; 0.67026; 0.66948; 0.66728\n",
      "2018-11-20 20:54:42,021 - Xworkers_1mill_0 - INFO -   1500;     1; 148774.7;   0.059; 0.76417; 0.09996; 0.66870; 0.66871; 0.67026; 0.66948; 0.66728\n",
      "2018-11-20 20:54:47,948 - Xworkers_1mill_0 - INFO -   1600;     1; 152172.5;   0.058; 0.76060; 0.09995; 0.68158; 0.68157; 0.68974; 0.68563; 0.67873\n",
      "2018-11-20 20:54:47,948 - Xworkers_1mill_0 - INFO -   1600;     1; 152172.5;   0.058; 0.76060; 0.09995; 0.68158; 0.68157; 0.68974; 0.68563; 0.67873\n",
      "2018-11-20 20:54:53,946 - Xworkers_1mill_0 - INFO -   1700;     1; 148011.8;   0.060; 0.76689; 0.09995; 0.67186; 0.67186; 0.67368; 0.67277; 0.66944\n",
      "2018-11-20 20:54:53,946 - Xworkers_1mill_0 - INFO -   1700;     1; 148011.8;   0.060; 0.76689; 0.09995; 0.67186; 0.67186; 0.67368; 0.67277; 0.66944\n",
      "2018-11-20 20:55:00,001 - Xworkers_1mill_0 - INFO -   1800;     1; 147531.8;   0.060; 0.76227; 0.09995; 0.67254; 0.67254; 0.68404; 0.67824; 0.66780\n",
      "2018-11-20 20:55:00,001 - Xworkers_1mill_0 - INFO -   1800;     1; 147531.8;   0.060; 0.76227; 0.09995; 0.67254; 0.67254; 0.68404; 0.67824; 0.66780\n",
      "2018-11-20 20:55:05,994 - Xworkers_1mill_0 - INFO -   1900;     1; 150876.7;   0.059; 0.74649; 0.09995; 0.67435; 0.67436; 0.67912; 0.67673; 0.67085\n",
      "2018-11-20 20:55:05,994 - Xworkers_1mill_0 - INFO -   1900;     1; 150876.7;   0.059; 0.74649; 0.09995; 0.67435; 0.67436; 0.67912; 0.67673; 0.67085\n",
      "2018-11-20 20:55:11,943 - Xworkers_1mill_0 - INFO -   2000;     1; 146561.3;   0.060; 0.74096; 0.09994; 0.68701; 0.68700; 0.69122; 0.68911; 0.68418\n",
      "2018-11-20 20:55:11,943 - Xworkers_1mill_0 - INFO -   2000;     1; 146561.3;   0.060; 0.74096; 0.09994; 0.68701; 0.68700; 0.69122; 0.68911; 0.68418\n",
      "2018-11-20 20:55:17,870 - Xworkers_1mill_0 - INFO -   2100;     1; 148442.7;   0.060; 0.76104; 0.09994; 0.67186; 0.67187; 0.67902; 0.67543; 0.66953\n",
      "2018-11-20 20:55:17,870 - Xworkers_1mill_0 - INFO -   2100;     1; 148442.7;   0.060; 0.76104; 0.09994; 0.67186; 0.67187; 0.67902; 0.67543; 0.66953\n",
      "2018-11-20 20:55:23,805 - Xworkers_1mill_0 - INFO -   2200;     1; 149307.3;   0.059; 0.76569; 0.09994; 0.67684; 0.67685; 0.67704; 0.67694; 0.67354\n",
      "2018-11-20 20:55:23,805 - Xworkers_1mill_0 - INFO -   2200;     1; 149307.3;   0.059; 0.76569; 0.09994; 0.67684; 0.67685; 0.67704; 0.67694; 0.67354\n",
      "2018-11-20 20:55:29,780 - Xworkers_1mill_0 - INFO -   2300;     1; 145855.3;   0.061; 0.74721; 0.09993; 0.68136; 0.68137; 0.68525; 0.68330; 0.67991\n",
      "2018-11-20 20:55:29,780 - Xworkers_1mill_0 - INFO -   2300;     1; 145855.3;   0.061; 0.74721; 0.09993; 0.68136; 0.68137; 0.68525; 0.68330; 0.67991\n",
      "2018-11-20 20:55:35,737 - Xworkers_1mill_0 - INFO -   2400;     1; 152774.4;   0.058; 0.75288; 0.09993; 0.68226; 0.68226; 0.68458; 0.68342; 0.67793\n",
      "2018-11-20 20:55:35,737 - Xworkers_1mill_0 - INFO -   2400;     1; 152774.4;   0.058; 0.75288; 0.09993; 0.68226; 0.68226; 0.68458; 0.68342; 0.67793\n",
      "2018-11-20 20:55:41,774 - Xworkers_1mill_0 - INFO -   2500;     1; 142662.3;   0.062; 0.74749; 0.09993; 0.66960; 0.66961; 0.67207; 0.67084; 0.66767\n",
      "2018-11-20 20:55:41,774 - Xworkers_1mill_0 - INFO -   2500;     1; 142662.3;   0.062; 0.74749; 0.09993; 0.66960; 0.66961; 0.67207; 0.67084; 0.66767\n",
      "2018-11-20 20:55:47,767 - Xworkers_1mill_0 - INFO -   2600;     1; 145418.8;   0.061; 0.77212; 0.09993; 0.66734; 0.66735; 0.67782; 0.67254; 0.66418\n",
      "2018-11-20 20:55:47,767 - Xworkers_1mill_0 - INFO -   2600;     1; 145418.8;   0.061; 0.77212; 0.09993; 0.66734; 0.66735; 0.67782; 0.67254; 0.66418\n",
      "2018-11-20 20:55:53,708 - Xworkers_1mill_0 - INFO -   2700;     1; 151347.9;   0.058; 0.74797; 0.09992; 0.68000; 0.68000; 0.68447; 0.68223; 0.67701\n",
      "2018-11-20 20:55:53,708 - Xworkers_1mill_0 - INFO -   2700;     1; 151347.9;   0.058; 0.74797; 0.09992; 0.68000; 0.68000; 0.68447; 0.68223; 0.67701\n",
      "2018-11-20 20:55:59,687 - Xworkers_1mill_0 - INFO -   2800;     1; 150499.3;   0.059; 0.74089; 0.09992; 0.68542; 0.68542; 0.69039; 0.68790; 0.68308\n",
      "2018-11-20 20:55:59,687 - Xworkers_1mill_0 - INFO -   2800;     1; 150499.3;   0.059; 0.74089; 0.09992; 0.68542; 0.68542; 0.69039; 0.68790; 0.68308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-20 20:56:05,613 - Xworkers_1mill_0 - INFO -   2900;     1; 148932.3;   0.059; 0.75952; 0.09992; 0.68136; 0.68136; 0.68265; 0.68201; 0.67776\n",
      "2018-11-20 20:56:05,613 - Xworkers_1mill_0 - INFO -   2900;     1; 148932.3;   0.059; 0.75952; 0.09992; 0.68136; 0.68136; 0.68265; 0.68201; 0.67776\n",
      "2018-11-20 20:56:11,521 - Xworkers_1mill_0 - INFO -   3000;     1; 148902.4;   0.059; 0.74374; 0.09991; 0.68045; 0.68046; 0.68539; 0.68292; 0.67422\n",
      "2018-11-20 20:56:11,521 - Xworkers_1mill_0 - INFO -   3000;     1; 148902.4;   0.059; 0.74374; 0.09991; 0.68045; 0.68046; 0.68539; 0.68292; 0.67422\n",
      "2018-11-20 20:57:02,465 - Xworkers_1mill_0 - INFO - valid - Number of batches: 1; batch_size: 1200000; Total Time: 0:00:49.296721\n",
      "2018-11-20 20:57:02,465 - Xworkers_1mill_0 - INFO - valid - Number of batches: 1; batch_size: 1200000; Total Time: 0:00:49.296721\n",
      "2018-11-20 20:57:02,467 - Xworkers_1mill_0 - INFO - ---Validation--- Training Step: 3024; Training Epoch: 1; \n",
      " Confusion Matrix:\n",
      " [[ 10855    310    149    194   2871    134     47]\n",
      " [  1274  12358   4630    165   1362     15      1]\n",
      " [     0   1167   5143    368      0      2      1]\n",
      " [     0      0   1691  12162      0    835    314]\n",
      " [382975  53300   3673    825 394500    145     32]\n",
      " [     0      2      5   1266      0   7300   1387]\n",
      " [     0      0      0     33      1    103    273]]\n",
      "2018-11-20 20:57:02,467 - Xworkers_1mill_0 - INFO - ---Validation--- Training Step: 3024; Training Epoch: 1; \n",
      " Confusion Matrix:\n",
      " [[ 10855    310    149    194   2871    134     47]\n",
      " [  1274  12358   4630    165   1362     15      1]\n",
      " [     0   1167   5143    368      0      2      1]\n",
      " [     0      0   1691  12162      0    835    314]\n",
      " [382975  53300   3673    825 394500    145     32]\n",
      " [     0      2      5   1266      0   7300   1387]\n",
      " [     0      0      0     33      1    103    273]]\n",
      "2018-11-20 20:57:02,474 - Xworkers_1mill_0 - INFO - (Training Step, Training Epoch, loss, logloss, accuracy, recall, precision, f1score_micro, f1score_macro) in Validation:   3024;     1; 0.83722; 0.22338; 0.49075; 0.68871; 0.47652; 0.56329; 0.46658\n",
      "2018-11-20 20:57:02,474 - Xworkers_1mill_0 - INFO - (Training Step, Training Epoch, loss, logloss, accuracy, recall, precision, f1score_micro, f1score_macro) in Validation:   3024;     1; 0.83722; 0.22338; 0.49075; 0.68871; 0.47652; 0.56329; 0.46658\n",
      "2018-11-20 20:57:06,952 - Xworkers_1mill_0 - INFO -   3100;     2; 145054.5;   0.061; 0.76726; 0.09991; 0.66870; 0.66870; 0.67546; 0.67206; 0.66440\n",
      "2018-11-20 20:57:06,952 - Xworkers_1mill_0 - INFO -   3100;     2; 145054.5;   0.061; 0.76726; 0.09991; 0.66870; 0.66870; 0.67546; 0.67206; 0.66440\n",
      "2018-11-20 20:57:13,141 - Xworkers_1mill_0 - INFO -   3200;     2; 145183.8;   0.061; 0.73560; 0.09991; 0.68271; 0.68271; 0.68719; 0.68494; 0.67969\n",
      "2018-11-20 20:57:13,141 - Xworkers_1mill_0 - INFO -   3200;     2; 145183.8;   0.061; 0.73560; 0.09991; 0.68271; 0.68271; 0.68719; 0.68494; 0.67969\n",
      "2018-11-20 20:57:19,285 - Xworkers_1mill_0 - INFO -   3300;     2; 147053.7;   0.060; 0.74823; 0.09991; 0.68701; 0.68702; 0.68802; 0.68752; 0.68342\n",
      "2018-11-20 20:57:19,285 - Xworkers_1mill_0 - INFO -   3300;     2; 147053.7;   0.060; 0.74823; 0.09991; 0.68701; 0.68702; 0.68802; 0.68752; 0.68342\n",
      "2018-11-20 20:57:25,420 - Xworkers_1mill_0 - INFO -   3400;     2; 147857.9;   0.060; 0.72509; 0.09990; 0.68723; 0.68724; 0.69106; 0.68914; 0.68466\n",
      "2018-11-20 20:57:25,420 - Xworkers_1mill_0 - INFO -   3400;     2; 147857.9;   0.060; 0.72509; 0.09990; 0.68723; 0.68724; 0.69106; 0.68914; 0.68466\n",
      "2018-11-20 20:57:31,614 - Xworkers_1mill_0 - INFO -   3500;     2; 139612.4;   0.063; 0.74498; 0.09990; 0.67977; 0.67978; 0.68099; 0.68038; 0.67686\n",
      "2018-11-20 20:57:31,614 - Xworkers_1mill_0 - INFO -   3500;     2; 139612.4;   0.063; 0.74498; 0.09990; 0.67977; 0.67978; 0.68099; 0.68038; 0.67686\n",
      "2018-11-20 20:57:37,787 - Xworkers_1mill_0 - INFO -   3600;     2; 146648.2;   0.060; 0.74411; 0.09990; 0.68475; 0.68475; 0.68739; 0.68607; 0.68150\n",
      "2018-11-20 20:57:37,787 - Xworkers_1mill_0 - INFO -   3600;     2; 146648.2;   0.060; 0.74411; 0.09990; 0.68475; 0.68475; 0.68739; 0.68607; 0.68150\n",
      "2018-11-20 20:57:43,988 - Xworkers_1mill_0 - INFO -   3700;     2; 141692.4;   0.062; 0.76312; 0.09990; 0.67141; 0.67141; 0.67334; 0.67237; 0.66860\n",
      "2018-11-20 20:57:43,988 - Xworkers_1mill_0 - INFO -   3700;     2; 141692.4;   0.062; 0.76312; 0.09990; 0.67141; 0.67141; 0.67334; 0.67237; 0.66860\n",
      "2018-11-20 20:57:50,139 - Xworkers_1mill_0 - INFO -   3800;     2; 142286.1;   0.062; 0.76545; 0.09989; 0.67277; 0.67277; 0.67640; 0.67458; 0.66925\n",
      "2018-11-20 20:57:50,139 - Xworkers_1mill_0 - INFO -   3800;     2; 142286.1;   0.062; 0.76545; 0.09989; 0.67277; 0.67277; 0.67640; 0.67458; 0.66925\n",
      "2018-11-20 20:57:56,278 - Xworkers_1mill_0 - INFO -   3900;     2; 146048.1;   0.061; 0.73021; 0.09989; 0.69356; 0.69356; 0.69686; 0.69521; 0.69027\n",
      "2018-11-20 20:57:56,278 - Xworkers_1mill_0 - INFO -   3900;     2; 146048.1;   0.061; 0.73021; 0.09989; 0.69356; 0.69356; 0.69686; 0.69521; 0.69027\n",
      "2018-11-20 20:58:02,349 - Xworkers_1mill_0 - INFO -   4000;     2; 143048.3;   0.062; 0.73630; 0.09989; 0.68791; 0.68791; 0.69294; 0.69042; 0.68620\n",
      "2018-11-20 20:58:02,349 - Xworkers_1mill_0 - INFO -   4000;     2; 143048.3;   0.062; 0.73630; 0.09989; 0.68791; 0.68791; 0.69294; 0.69042; 0.68620\n",
      "2018-11-20 20:58:08,486 - Xworkers_1mill_0 - INFO -   4100;     2; 146037.8;   0.061; 0.72369; 0.09988; 0.68768; 0.68769; 0.68988; 0.68878; 0.68319\n",
      "2018-11-20 20:58:08,486 - Xworkers_1mill_0 - INFO -   4100;     2; 146037.8;   0.061; 0.72369; 0.09988; 0.68768; 0.68769; 0.68988; 0.68878; 0.68319\n",
      "2018-11-20 20:58:14,603 - Xworkers_1mill_0 - INFO -   4200;     2; 145017.1;   0.061; 0.75265; 0.09988; 0.68746; 0.68746; 0.68798; 0.68772; 0.68511\n",
      "2018-11-20 20:58:14,603 - Xworkers_1mill_0 - INFO -   4200;     2; 145017.1;   0.061; 0.75265; 0.09988; 0.68746; 0.68746; 0.68798; 0.68772; 0.68511\n",
      "2018-11-20 20:58:20,749 - Xworkers_1mill_0 - INFO -   4300;     2; 138249.6;   0.064; 0.73846; 0.09988; 0.68836; 0.68836; 0.68936; 0.68886; 0.68532\n",
      "2018-11-20 20:58:20,749 - Xworkers_1mill_0 - INFO -   4300;     2; 138249.6;   0.064; 0.73846; 0.09988; 0.68836; 0.68836; 0.68936; 0.68886; 0.68532\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint written to /home/ubuntu/summ_15ep_2wrk/checkpoint-4379\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-20 20:58:27,947 - Xworkers_1mill_0 - INFO -   4400;     2; 144659.4;   0.061; 0.75364; 0.09988; 0.67819; 0.67819; 0.68602; 0.68208; 0.67526\n",
      "2018-11-20 20:58:27,947 - Xworkers_1mill_0 - INFO -   4400;     2; 144659.4;   0.061; 0.75364; 0.09988; 0.67819; 0.67819; 0.68602; 0.68208; 0.67526\n",
      "2018-11-20 20:58:34,076 - Xworkers_1mill_0 - INFO -   4500;     2; 145850.7;   0.061; 0.72025; 0.09987; 0.68881; 0.68882; 0.69118; 0.69000; 0.68538\n",
      "2018-11-20 20:58:34,076 - Xworkers_1mill_0 - INFO -   4500;     2; 145850.7;   0.061; 0.72025; 0.09987; 0.68881; 0.68882; 0.69118; 0.69000; 0.68538\n",
      "2018-11-20 20:58:40,216 - Xworkers_1mill_0 - INFO -   4600;     2; 144411.2;   0.061; 0.73415; 0.09987; 0.67887; 0.67886; 0.68202; 0.68044; 0.67524\n",
      "2018-11-20 20:58:40,216 - Xworkers_1mill_0 - INFO -   4600;     2; 144411.2;   0.061; 0.73415; 0.09987; 0.67887; 0.67886; 0.68202; 0.68044; 0.67524\n",
      "2018-11-20 20:58:46,400 - Xworkers_1mill_0 - INFO -   4700;     2; 146135.5;   0.061; 0.73366; 0.09987; 0.68045; 0.68045; 0.68244; 0.68145; 0.67647\n",
      "2018-11-20 20:58:46,400 - Xworkers_1mill_0 - INFO -   4700;     2; 146135.5;   0.061; 0.73366; 0.09987; 0.68045; 0.68045; 0.68244; 0.68145; 0.67647\n",
      "2018-11-20 20:58:52,550 - Xworkers_1mill_0 - INFO -   4800;     2; 145113.5;   0.061; 0.74156; 0.09986; 0.69040; 0.69041; 0.69080; 0.69060; 0.68817\n",
      "2018-11-20 20:58:52,550 - Xworkers_1mill_0 - INFO -   4800;     2; 145113.5;   0.061; 0.74156; 0.09986; 0.69040; 0.69041; 0.69080; 0.69060; 0.68817\n",
      "2018-11-20 20:58:58,704 - Xworkers_1mill_0 - INFO -   4900;     2; 142541.2;   0.062; 0.74392; 0.09986; 0.68045; 0.68046; 0.68098; 0.68072; 0.67635\n",
      "2018-11-20 20:58:58,704 - Xworkers_1mill_0 - INFO -   4900;     2; 142541.2;   0.062; 0.74392; 0.09986; 0.68045; 0.68046; 0.68098; 0.68072; 0.67635\n",
      "2018-11-20 20:59:04,875 - Xworkers_1mill_0 - INFO -   5000;     2; 140832.3;   0.063; 0.74218; 0.09986; 0.68045; 0.68046; 0.68373; 0.68209; 0.67819\n",
      "2018-11-20 20:59:04,875 - Xworkers_1mill_0 - INFO -   5000;     2; 140832.3;   0.063; 0.74218; 0.09986; 0.68045; 0.68046; 0.68373; 0.68209; 0.67819\n",
      "2018-11-20 20:59:11,034 - Xworkers_1mill_0 - INFO -   5100;     2; 138272.7;   0.064; 0.73513; 0.09986; 0.68249; 0.68250; 0.68353; 0.68301; 0.67868\n",
      "2018-11-20 20:59:11,034 - Xworkers_1mill_0 - INFO -   5100;     2; 138272.7;   0.064; 0.73513; 0.09986; 0.68249; 0.68250; 0.68353; 0.68301; 0.67868\n",
      "2018-11-20 20:59:17,162 - Xworkers_1mill_0 - INFO -   5200;     2; 137551.3;   0.064; 0.72643; 0.09985; 0.68746; 0.68745; 0.69560; 0.69150; 0.68609\n",
      "2018-11-20 20:59:17,162 - Xworkers_1mill_0 - INFO -   5200;     2; 137551.3;   0.064; 0.72643; 0.09985; 0.68746; 0.68745; 0.69560; 0.69150; 0.68609\n",
      "2018-11-20 20:59:23,303 - Xworkers_1mill_0 - INFO -   5300;     2; 151790.4;   0.058; 0.73058; 0.09985; 0.68520; 0.68520; 0.68842; 0.68681; 0.68140\n",
      "2018-11-20 20:59:23,303 - Xworkers_1mill_0 - INFO -   5300;     2; 151790.4;   0.058; 0.73058; 0.09985; 0.68520; 0.68520; 0.68842; 0.68681; 0.68140\n",
      "2018-11-20 20:59:29,402 - Xworkers_1mill_0 - INFO -   5400;     2; 149713.2;   0.059; 0.72169; 0.09985; 0.68542; 0.68543; 0.68787; 0.68665; 0.68123\n",
      "2018-11-20 20:59:29,402 - Xworkers_1mill_0 - INFO -   5400;     2; 149713.2;   0.059; 0.72169; 0.09985; 0.68542; 0.68543; 0.68787; 0.68665; 0.68123\n",
      "2018-11-20 20:59:35,560 - Xworkers_1mill_0 - INFO -   5500;     2; 144751.9;   0.061; 0.75296; 0.09984; 0.67706; 0.67707; 0.67953; 0.67830; 0.67114\n",
      "2018-11-20 20:59:35,560 - Xworkers_1mill_0 - INFO -   5500;     2; 144751.9;   0.061; 0.75296; 0.09984; 0.67706; 0.67707; 0.67953; 0.67830; 0.67114\n",
      "2018-11-20 20:59:41,700 - Xworkers_1mill_0 - INFO -   5600;     2; 140626.9;   0.063; 0.73599; 0.09984; 0.68362; 0.68361; 0.69165; 0.68761; 0.68037\n",
      "2018-11-20 20:59:41,700 - Xworkers_1mill_0 - INFO -   5600;     2; 140626.9;   0.063; 0.73599; 0.09984; 0.68362; 0.68361; 0.69165; 0.68761; 0.68037\n",
      "2018-11-20 20:59:47,813 - Xworkers_1mill_0 - INFO -   5700;     2; 141294.4;   0.063; 0.73079; 0.09984; 0.68271; 0.68272; 0.68221; 0.68246; 0.67967\n",
      "2018-11-20 20:59:47,813 - Xworkers_1mill_0 - INFO -   5700;     2; 141294.4;   0.063; 0.73079; 0.09984; 0.68271; 0.68272; 0.68221; 0.68246; 0.67967\n",
      "2018-11-20 20:59:53,932 - Xworkers_1mill_0 - INFO -   5800;     2; 149495.5;   0.059; 0.73046; 0.09984; 0.68994; 0.68994; 0.69218; 0.69106; 0.68749\n",
      "2018-11-20 20:59:53,932 - Xworkers_1mill_0 - INFO -   5800;     2; 149495.5;   0.059; 0.73046; 0.09984; 0.68994; 0.68994; 0.69218; 0.69106; 0.68749\n",
      "2018-11-20 21:00:00,057 - Xworkers_1mill_0 - INFO -   5900;     2; 143652.8;   0.062; 0.70597; 0.09983; 0.69921; 0.69921; 0.70187; 0.70054; 0.69553\n",
      "2018-11-20 21:00:00,057 - Xworkers_1mill_0 - INFO -   5900;     2; 143652.8;   0.062; 0.70597; 0.09983; 0.69921; 0.69921; 0.70187; 0.70054; 0.69553\n",
      "2018-11-20 21:00:06,228 - Xworkers_1mill_0 - INFO -   6000;     2; 148753.3;   0.059; 0.73807; 0.09983; 0.67232; 0.67233; 0.67418; 0.67325; 0.66824\n",
      "2018-11-20 21:00:06,228 - Xworkers_1mill_0 - INFO -   6000;     2; 148753.3;   0.059; 0.73807; 0.09983; 0.67232; 0.67233; 0.67418; 0.67325; 0.66824\n",
      "2018-11-20 21:00:51,807 - Xworkers_1mill_0 - INFO - valid - Number of batches: 1; batch_size: 1200000; Total Time: 0:00:42.625012\n",
      "2018-11-20 21:00:51,807 - Xworkers_1mill_0 - INFO - valid - Number of batches: 1; batch_size: 1200000; Total Time: 0:00:42.625012\n",
      "2018-11-20 21:00:51,809 - Xworkers_1mill_0 - INFO - ---Validation--- Training Step: 6048; Training Epoch: 2; \n",
      " Confusion Matrix:\n",
      " [[  9924    313    142    195   3804    138     44]\n",
      " [   905  12418   4540    175   1751     15      1]\n",
      " [     0   1167   5141    370      0      2      1]\n",
      " [     0      0   1688  12185      0    867    262]\n",
      " [297772  50737   3469    844 482453    150     25]\n",
      " [     0      2      4   1275      0   7569   1110]\n",
      " [     0      0      0     33      1    110    266]]\n",
      "2018-11-20 21:00:51,809 - Xworkers_1mill_0 - INFO - ---Validation--- Training Step: 6048; Training Epoch: 2; \n",
      " Confusion Matrix:\n",
      " [[  9924    313    142    195   3804    138     44]\n",
      " [   905  12418   4540    175   1751     15      1]\n",
      " [     0   1167   5141    370      0      2      1]\n",
      " [     0      0   1688  12185      0    867    262]\n",
      " [297772  50737   3469    844 482453    150     25]\n",
      " [     0      2      4   1275      0   7569   1110]\n",
      " [     0      0      0     33      1    110    266]]\n",
      "2018-11-20 21:00:51,815 - Xworkers_1mill_0 - INFO - (Training Step, Training Epoch, loss, logloss, accuracy, recall, precision, f1score_micro, f1score_macro) in Validation:   6048;     2; 0.76158; 0.20405; 0.58762; 0.69665; 0.48214; 0.56988; 0.48932\n",
      "2018-11-20 21:00:51,815 - Xworkers_1mill_0 - INFO - (Training Step, Training Epoch, loss, logloss, accuracy, recall, precision, f1score_micro, f1score_macro) in Validation:   6048;     2; 0.76158; 0.20405; 0.58762; 0.69665; 0.48214; 0.56988; 0.48932\n",
      "2018-11-20 21:00:55,020 - Xworkers_1mill_0 - INFO -   6100;     3; 142494.7;   0.062; 0.73021; 0.09983; 0.69695; 0.69694; 0.70001; 0.69847; 0.69444\n",
      "2018-11-20 21:00:55,020 - Xworkers_1mill_0 - INFO -   6100;     3; 142494.7;   0.062; 0.73021; 0.09983; 0.69695; 0.69694; 0.70001; 0.69847; 0.69444\n",
      "2018-11-20 21:01:01,194 - Xworkers_1mill_0 - INFO -   6200;     3; 146828.6;   0.060; 0.71351; 0.09982; 0.68881; 0.68882; 0.68973; 0.68927; 0.68507\n",
      "2018-11-20 21:01:01,194 - Xworkers_1mill_0 - INFO -   6200;     3; 146828.6;   0.060; 0.71351; 0.09982; 0.68881; 0.68882; 0.68973; 0.68927; 0.68507\n",
      "2018-11-20 21:01:07,333 - Xworkers_1mill_0 - INFO -   6300;     3; 144682.5;   0.061; 0.75730; 0.09982; 0.67548; 0.67548; 0.67837; 0.67692; 0.67267\n",
      "2018-11-20 21:01:07,333 - Xworkers_1mill_0 - INFO -   6300;     3; 144682.5;   0.061; 0.75730; 0.09982; 0.67548; 0.67548; 0.67837; 0.67692; 0.67267\n",
      "2018-11-20 21:01:13,484 - Xworkers_1mill_0 - INFO -   6400;     3; 143955.3;   0.061; 0.71670; 0.09982; 0.69921; 0.69921; 0.70334; 0.70127; 0.69651\n",
      "2018-11-20 21:01:13,484 - Xworkers_1mill_0 - INFO -   6400;     3; 143955.3;   0.061; 0.71670; 0.09982; 0.69921; 0.69921; 0.70334; 0.70127; 0.69651\n",
      "2018-11-20 21:01:19,654 - Xworkers_1mill_0 - INFO -   6500;     3; 145116.3;   0.061; 0.72396; 0.09982; 0.69198; 0.69197; 0.69371; 0.69284; 0.68985\n",
      "2018-11-20 21:01:19,654 - Xworkers_1mill_0 - INFO -   6500;     3; 145116.3;   0.061; 0.72396; 0.09982; 0.69198; 0.69197; 0.69371; 0.69284; 0.68985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-20 21:01:25,806 - Xworkers_1mill_0 - INFO -   6600;     3; 147645.6;   0.060; 0.73449; 0.09981; 0.67819; 0.67820; 0.67875; 0.67847; 0.67583\n",
      "2018-11-20 21:01:25,806 - Xworkers_1mill_0 - INFO -   6600;     3; 147645.6;   0.060; 0.73449; 0.09981; 0.67819; 0.67820; 0.67875; 0.67847; 0.67583\n",
      "2018-11-20 21:01:31,890 - Xworkers_1mill_0 - INFO -   6700;     3; 147585.8;   0.060; 0.72529; 0.09981; 0.69831; 0.69831; 0.69838; 0.69835; 0.69623\n",
      "2018-11-20 21:01:31,890 - Xworkers_1mill_0 - INFO -   6700;     3; 147585.8;   0.060; 0.72529; 0.09981; 0.69831; 0.69831; 0.69838; 0.69835; 0.69623\n",
      "2018-11-20 21:01:38,028 - Xworkers_1mill_0 - INFO -   6800;     3; 142289.9;   0.062; 0.72040; 0.09981; 0.68339; 0.68341; 0.68314; 0.68327; 0.68122\n",
      "2018-11-20 21:01:38,028 - Xworkers_1mill_0 - INFO -   6800;     3; 142289.9;   0.062; 0.72040; 0.09981; 0.68339; 0.68341; 0.68314; 0.68327; 0.68122\n",
      "2018-11-20 21:01:44,183 - Xworkers_1mill_0 - INFO -   6900;     3; 144488.7;   0.061; 0.71580; 0.09980; 0.69379; 0.69379; 0.69381; 0.69380; 0.69148\n",
      "2018-11-20 21:01:44,183 - Xworkers_1mill_0 - INFO -   6900;     3; 144488.7;   0.061; 0.71580; 0.09980; 0.69379; 0.69379; 0.69381; 0.69380; 0.69148\n",
      "2018-11-20 21:01:50,361 - Xworkers_1mill_0 - INFO -   7000;     3; 142625.0;   0.062; 0.71795; 0.09980; 0.70147; 0.70147; 0.70236; 0.70191; 0.69865\n",
      "2018-11-20 21:01:50,361 - Xworkers_1mill_0 - INFO -   7000;     3; 142625.0;   0.062; 0.71795; 0.09980; 0.70147; 0.70147; 0.70236; 0.70191; 0.69865\n",
      "2018-11-20 21:01:56,508 - Xworkers_1mill_0 - INFO -   7100;     3; 148580.6;   0.060; 0.72992; 0.09980; 0.68881; 0.68882; 0.68953; 0.68917; 0.68678\n",
      "2018-11-20 21:01:56,508 - Xworkers_1mill_0 - INFO -   7100;     3; 148580.6;   0.060; 0.72992; 0.09980; 0.68881; 0.68882; 0.68953; 0.68917; 0.68678\n",
      "2018-11-20 21:02:02,641 - Xworkers_1mill_0 - INFO -   7200;     3; 150506.0;   0.059; 0.71420; 0.09980; 0.69763; 0.69763; 0.70647; 0.70202; 0.69385\n",
      "2018-11-20 21:02:02,641 - Xworkers_1mill_0 - INFO -   7200;     3; 150506.0;   0.059; 0.71420; 0.09980; 0.69763; 0.69763; 0.70647; 0.70202; 0.69385\n",
      "2018-11-20 21:02:08,754 - Xworkers_1mill_0 - INFO -   7300;     3; 147370.1;   0.060; 0.72106; 0.09979; 0.69921; 0.69922; 0.70065; 0.69994; 0.69558\n",
      "2018-11-20 21:02:08,754 - Xworkers_1mill_0 - INFO -   7300;     3; 147370.1;   0.060; 0.72106; 0.09979; 0.69921; 0.69922; 0.70065; 0.69994; 0.69558\n",
      "2018-11-20 21:02:14,879 - Xworkers_1mill_0 - INFO -   7400;     3; 143031.2;   0.062; 0.72463; 0.09979; 0.68701; 0.68701; 0.68859; 0.68780; 0.68606\n",
      "2018-11-20 21:02:14,879 - Xworkers_1mill_0 - INFO -   7400;     3; 143031.2;   0.062; 0.72463; 0.09979; 0.68701; 0.68701; 0.68859; 0.68780; 0.68606\n",
      "2018-11-20 21:02:21,014 - Xworkers_1mill_0 - INFO -   7500;     3; 141201.9;   0.063; 0.72483; 0.09979; 0.69582; 0.69581; 0.69682; 0.69632; 0.69265\n",
      "2018-11-20 21:02:21,014 - Xworkers_1mill_0 - INFO -   7500;     3; 141201.9;   0.063; 0.72483; 0.09979; 0.69582; 0.69581; 0.69682; 0.69632; 0.69265\n",
      "2018-11-20 21:02:27,191 - Xworkers_1mill_0 - INFO -   7600;     3; 146360.2;   0.060; 0.74052; 0.09978; 0.68068; 0.68067; 0.68394; 0.68230; 0.67730\n",
      "2018-11-20 21:02:27,191 - Xworkers_1mill_0 - INFO -   7600;     3; 146360.2;   0.060; 0.74052; 0.09978; 0.68068; 0.68067; 0.68394; 0.68230; 0.67730\n",
      "2018-11-20 21:02:33,353 - Xworkers_1mill_0 - INFO -   7700;     3; 150214.2;   0.059; 0.71517; 0.09978; 0.68994; 0.68995; 0.69169; 0.69081; 0.68709\n",
      "2018-11-20 21:02:33,353 - Xworkers_1mill_0 - INFO -   7700;     3; 150214.2;   0.059; 0.71517; 0.09978; 0.68994; 0.68995; 0.69169; 0.69081; 0.68709\n",
      "2018-11-20 21:02:39,479 - Xworkers_1mill_0 - INFO -   7800;     3; 143202.8;   0.062; 0.71538; 0.09978; 0.70056; 0.70057; 0.70336; 0.70196; 0.69849\n",
      "2018-11-20 21:02:39,479 - Xworkers_1mill_0 - INFO -   7800;     3; 143202.8;   0.062; 0.71538; 0.09978; 0.70056; 0.70057; 0.70336; 0.70196; 0.69849\n",
      "2018-11-20 21:02:45,621 - Xworkers_1mill_0 - INFO -   7900;     3; 147211.8;   0.060; 0.73775; 0.09978; 0.68565; 0.68565; 0.69064; 0.68814; 0.68295\n",
      "2018-11-20 21:02:45,621 - Xworkers_1mill_0 - INFO -   7900;     3; 147211.8;   0.060; 0.73775; 0.09978; 0.68565; 0.68565; 0.69064; 0.68814; 0.68295\n",
      "2018-11-20 21:02:51,751 - Xworkers_1mill_0 - INFO -   8000;     3; 147131.8;   0.060; 0.70518; 0.09977; 0.70599; 0.70599; 0.70828; 0.70713; 0.70336\n",
      "2018-11-20 21:02:51,751 - Xworkers_1mill_0 - INFO -   8000;     3; 147131.8;   0.060; 0.70518; 0.09977; 0.70599; 0.70599; 0.70828; 0.70713; 0.70336\n",
      "2018-11-20 21:02:57,928 - Xworkers_1mill_0 - INFO -   8100;     3; 143418.0;   0.062; 0.71915; 0.09977; 0.68339; 0.68340; 0.68457; 0.68398; 0.68173\n",
      "2018-11-20 21:02:57,928 - Xworkers_1mill_0 - INFO -   8100;     3; 143418.0;   0.062; 0.71915; 0.09977; 0.68339; 0.68340; 0.68457; 0.68398; 0.68173\n",
      "2018-11-20 21:03:04,175 - Xworkers_1mill_0 - INFO -   8200;     3; 113427.5;   0.078; 0.72497; 0.09977; 0.68520; 0.68521; 0.68756; 0.68638; 0.68281\n",
      "2018-11-20 21:03:04,175 - Xworkers_1mill_0 - INFO -   8200;     3; 113427.5;   0.078; 0.72497; 0.09977; 0.68520; 0.68521; 0.68756; 0.68638; 0.68281\n",
      "2018-11-20 21:03:10,351 - Xworkers_1mill_0 - INFO -   8300;     3; 142907.3;   0.062; 0.71892; 0.09976; 0.68836; 0.68835; 0.69309; 0.69071; 0.68677\n",
      "2018-11-20 21:03:10,351 - Xworkers_1mill_0 - INFO -   8300;     3; 142907.3;   0.062; 0.71892; 0.09976; 0.68836; 0.68835; 0.69309; 0.69071; 0.68677\n",
      "2018-11-20 21:03:16,500 - Xworkers_1mill_0 - INFO -   8400;     3; 139530.0;   0.063; 0.73067; 0.09976; 0.68768; 0.68769; 0.68936; 0.68852; 0.68420\n",
      "2018-11-20 21:03:16,500 - Xworkers_1mill_0 - INFO -   8400;     3; 139530.0;   0.063; 0.73067; 0.09976; 0.68768; 0.68769; 0.68936; 0.68852; 0.68420\n",
      "2018-11-20 21:03:22,620 - Xworkers_1mill_0 - INFO -   8500;     3; 148708.0;   0.060; 0.72048; 0.09976; 0.69311; 0.69310; 0.69828; 0.69568; 0.69014\n",
      "2018-11-20 21:03:22,620 - Xworkers_1mill_0 - INFO -   8500;     3; 148708.0;   0.060; 0.72048; 0.09976; 0.69311; 0.69310; 0.69828; 0.69568; 0.69014\n",
      "2018-11-20 21:03:28,756 - Xworkers_1mill_0 - INFO -   8600;     3; 145294.7;   0.061; 0.74757; 0.09976; 0.67932; 0.67933; 0.67973; 0.67953; 0.67777\n",
      "2018-11-20 21:03:28,756 - Xworkers_1mill_0 - INFO -   8600;     3; 145294.7;   0.061; 0.74757; 0.09976; 0.67932; 0.67933; 0.67973; 0.67953; 0.67777\n",
      "2018-11-20 21:03:34,902 - Xworkers_1mill_0 - INFO -   8700;     3; 147759.7;   0.060; 0.73556; 0.09975; 0.68723; 0.68724; 0.68758; 0.68741; 0.68425\n",
      "2018-11-20 21:03:34,902 - Xworkers_1mill_0 - INFO -   8700;     3; 147759.7;   0.060; 0.73556; 0.09975; 0.68723; 0.68724; 0.68758; 0.68741; 0.68425\n",
      "2018-11-20 21:03:41,071 - Xworkers_1mill_0 - INFO -   8800;     3; 145447.2;   0.061; 0.71348; 0.09975; 0.69853; 0.69854; 0.70079; 0.69966; 0.69645\n",
      "2018-11-20 21:03:41,071 - Xworkers_1mill_0 - INFO -   8800;     3; 145447.2;   0.061; 0.71348; 0.09975; 0.69853; 0.69854; 0.70079; 0.69966; 0.69645\n",
      "2018-11-20 21:03:47,173 - Xworkers_1mill_0 - INFO -   8900;     3; 150004.8;   0.059; 0.72853; 0.09975; 0.69017; 0.69016; 0.69359; 0.69187; 0.68736\n",
      "2018-11-20 21:03:47,173 - Xworkers_1mill_0 - INFO -   8900;     3; 150004.8;   0.059; 0.72853; 0.09975; 0.69017; 0.69016; 0.69359; 0.69187; 0.68736\n",
      "2018-11-20 21:03:53,290 - Xworkers_1mill_0 - INFO -   9000;     3; 151477.6;   0.058; 0.72443; 0.09974; 0.69017; 0.69017; 0.69090; 0.69054; 0.68729\n",
      "2018-11-20 21:03:53,290 - Xworkers_1mill_0 - INFO -   9000;     3; 151477.6;   0.058; 0.72443; 0.09974; 0.69017; 0.69017; 0.69090; 0.69054; 0.68729\n",
      "2018-11-20 21:04:41,172 - Xworkers_1mill_0 - INFO - valid - Number of batches: 1; batch_size: 1200000; Total Time: 0:00:43.479329\n",
      "2018-11-20 21:04:41,172 - Xworkers_1mill_0 - INFO - valid - Number of batches: 1; batch_size: 1200000; Total Time: 0:00:43.479329\n",
      "2018-11-20 21:04:41,174 - Xworkers_1mill_0 - INFO - ---Validation--- Training Step: 9072; Training Epoch: 3; \n",
      " Confusion Matrix:\n",
      " [[ 10118    322    152    200   3592    147     29]\n",
      " [   916  11841   5205    169   1658     16      0]\n",
      " [     0    983   5322    373      0      3      0]\n",
      " [     1      0   1667  12251      0    908    175]\n",
      " [311509  52866   3975    840 466085    156     19]\n",
      " [     0      2      3   1289      0   7974    692]\n",
      " [     0      0      0     33      1    135    241]]\n",
      "2018-11-20 21:04:41,174 - Xworkers_1mill_0 - INFO - ---Validation--- Training Step: 9072; Training Epoch: 3; \n",
      " Confusion Matrix:\n",
      " [[ 10118    322    152    200   3592    147     29]\n",
      " [   916  11841   5205    169   1658     16      0]\n",
      " [     0    983   5322    373      0      3      0]\n",
      " [     1      0   1667  12251      0    908    175]\n",
      " [311509  52866   3975    840 466085    156     19]\n",
      " [     0      2      3   1289      0   7974    692]\n",
      " [     0      0      0     33      1    135    241]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-20 21:04:41,179 - Xworkers_1mill_0 - INFO - (Training Step, Training Epoch, loss, logloss, accuracy, recall, precision, f1score_micro, f1score_macro) in Validation:   9072;     3; 0.78078; 0.20932; 0.56974; 0.69319; 0.48519; 0.57083; 0.49409\n",
      "2018-11-20 21:04:41,179 - Xworkers_1mill_0 - INFO - (Training Step, Training Epoch, loss, logloss, accuracy, recall, precision, f1score_micro, f1score_macro) in Validation:   9072;     3; 0.78078; 0.20932; 0.56974; 0.69319; 0.48519; 0.57083; 0.49409\n",
      "2018-11-20 21:04:42,903 - Xworkers_1mill_0 - INFO -   9100;     4; 145463.8;   0.061; 0.71847; 0.09974; 0.68927; 0.68927; 0.70052; 0.69485; 0.68617\n",
      "2018-11-20 21:04:42,903 - Xworkers_1mill_0 - INFO -   9100;     4; 145463.8;   0.061; 0.71847; 0.09974; 0.68927; 0.68927; 0.70052; 0.69485; 0.68617\n",
      "2018-11-20 21:04:49,126 - Xworkers_1mill_0 - INFO -   9200;     4; 147405.8;   0.060; 0.72611; 0.09974; 0.68859; 0.68859; 0.69025; 0.68942; 0.68477\n",
      "2018-11-20 21:04:49,126 - Xworkers_1mill_0 - INFO -   9200;     4; 147405.8;   0.060; 0.72611; 0.09974; 0.68859; 0.68859; 0.69025; 0.68942; 0.68477\n",
      "2018-11-20 21:04:55,293 - Xworkers_1mill_0 - INFO -   9300;     4; 147856.8;   0.060; 0.71864; 0.09974; 0.68678; 0.68679; 0.69076; 0.68877; 0.68504\n",
      "2018-11-20 21:04:55,293 - Xworkers_1mill_0 - INFO -   9300;     4; 147856.8;   0.060; 0.71864; 0.09974; 0.68678; 0.68679; 0.69076; 0.68877; 0.68504\n",
      "2018-11-20 21:05:01,456 - Xworkers_1mill_0 - INFO -   9400;     4; 146075.1;   0.061; 0.71848; 0.09973; 0.68927; 0.68927; 0.69031; 0.68979; 0.68567\n",
      "2018-11-20 21:05:01,456 - Xworkers_1mill_0 - INFO -   9400;     4; 146075.1;   0.061; 0.71848; 0.09973; 0.68927; 0.68927; 0.69031; 0.68979; 0.68567\n",
      "2018-11-20 21:05:07,530 - Xworkers_1mill_0 - INFO -   9500;     4; 144781.2;   0.061; 0.71717; 0.09973; 0.69130; 0.69130; 0.69398; 0.69264; 0.68871\n",
      "2018-11-20 21:05:07,530 - Xworkers_1mill_0 - INFO -   9500;     4; 144781.2;   0.061; 0.71717; 0.09973; 0.69130; 0.69130; 0.69398; 0.69264; 0.68871\n",
      "2018-11-20 21:05:13,663 - Xworkers_1mill_0 - INFO -   9600;     4; 147406.4;   0.060; 0.69703; 0.09973; 0.70056; 0.70056; 0.70375; 0.70215; 0.69809\n",
      "2018-11-20 21:05:13,663 - Xworkers_1mill_0 - INFO -   9600;     4; 147406.4;   0.060; 0.69703; 0.09973; 0.70056; 0.70056; 0.70375; 0.70215; 0.69809\n",
      "2018-11-20 21:05:19,760 - Xworkers_1mill_0 - INFO -   9700;     4; 149662.5;   0.059; 0.70242; 0.09973; 0.70056; 0.70057; 0.70104; 0.70081; 0.69719\n",
      "2018-11-20 21:05:19,760 - Xworkers_1mill_0 - INFO -   9700;     4; 149662.5;   0.059; 0.70242; 0.09973; 0.70056; 0.70057; 0.70104; 0.70081; 0.69719\n",
      "2018-11-20 21:05:25,897 - Xworkers_1mill_0 - INFO -   9800;     4; 145161.1;   0.061; 0.72398; 0.09972; 0.69627; 0.69627; 0.69932; 0.69779; 0.69334\n",
      "2018-11-20 21:05:25,897 - Xworkers_1mill_0 - INFO -   9800;     4; 145161.1;   0.061; 0.72398; 0.09972; 0.69627; 0.69627; 0.69932; 0.69779; 0.69334\n",
      "2018-11-20 21:05:32,000 - Xworkers_1mill_0 - INFO -   9900;     4; 151422.6;   0.058; 0.72015; 0.09972; 0.68362; 0.68362; 0.68662; 0.68511; 0.67933\n",
      "2018-11-20 21:05:32,000 - Xworkers_1mill_0 - INFO -   9900;     4; 151422.6;   0.058; 0.72015; 0.09972; 0.68362; 0.68362; 0.68662; 0.68511; 0.67933\n",
      "2018-11-20 21:05:38,091 - Xworkers_1mill_0 - INFO -  10000;     4; 142362.5;   0.062; 0.71254; 0.09972; 0.69446; 0.69447; 0.69685; 0.69566; 0.69128\n",
      "2018-11-20 21:05:38,091 - Xworkers_1mill_0 - INFO -  10000;     4; 142362.5;   0.062; 0.71254; 0.09972; 0.69446; 0.69447; 0.69685; 0.69566; 0.69128\n",
      "2018-11-20 21:05:44,201 - Xworkers_1mill_0 - INFO -  10100;     4; 144103.4;   0.061; 0.70613; 0.09971; 0.69401; 0.69401; 0.69552; 0.69476; 0.69125\n",
      "2018-11-20 21:05:44,201 - Xworkers_1mill_0 - INFO -  10100;     4; 144103.4;   0.061; 0.70613; 0.09971; 0.69401; 0.69401; 0.69552; 0.69476; 0.69125\n",
      "2018-11-20 21:05:50,301 - Xworkers_1mill_0 - INFO -  10200;     4; 144182.9;   0.061; 0.72745; 0.09971; 0.69853; 0.69854; 0.70083; 0.69968; 0.69567\n",
      "2018-11-20 21:05:50,301 - Xworkers_1mill_0 - INFO -  10200;     4; 144182.9;   0.061; 0.72745; 0.09971; 0.69853; 0.69854; 0.70083; 0.69968; 0.69567\n",
      "2018-11-20 21:05:56,039 - Xworkers_1mill_0 - INFO - METRICS each 1800 (secs):  Loss in ---Training in Summary---: 0.71243\n",
      " Avg Log_Loss in ---Training in Summary---: 0.18195\n",
      "---Training in Summary---: (Silly) Global-ACC=0.69356, Recall=0.69357, Avg M-Measure=0.9507, Avg AUC_AOC=0.9504 Avg AUC_PR=0.7423\n",
      " Precision= 0.69579  f1score_micro= 0.69468  f1score_macro= 0.69093\n",
      "Total Confusion Matrix;Total M-Measure Matrix;Total AUC_AOC;Total AUC_PR\n",
      "415;23;9;9;160;10;6;;0.0000;0.9349;0.9861;0.9836;0.7847;0.9828;0.9888;;0.9432 ;0.7660\n",
      "35;302;202;7;83;2;1;;0.9239;0.0000;0.7371;0.9833;0.8541;0.9956;0.9976;;0.9149 ;0.6243\n",
      "1;71;512;48;0;0;0;;0.9904;0.8173;0.0000;0.9232;0.9955;0.9921;0.9986;;0.9524 ;0.6839\n",
      "0;0;83;493;0;35;21;;0.9899;0.9875;0.9351;0.0000;0.9988;0.9132;0.9691;;0.9655 ;0.7930\n",
      "135;76;5;0;417;0;0;;0.8033;0.8969;0.9954;0.9983;0.0000;0.9984;0.9982;;0.9480 ;0.7106\n",
      "0;1;9;88;0;450;84;;0.9877;0.9965;0.9911;0.9160;0.9990;0.0000;0.8376;;0.9545 ;0.7490\n",
      "0;1;0;26;0;125;480;;0.9944;0.9984;0.9983;0.9752;0.9989;0.8839;0.0000;;0.9745 ;0.8695\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "2018-11-20 21:05:56,039 - Xworkers_1mill_0 - INFO - METRICS each 1800 (secs):  Loss in ---Training in Summary---: 0.71243\n",
      " Avg Log_Loss in ---Training in Summary---: 0.18195\n",
      "---Training in Summary---: (Silly) Global-ACC=0.69356, Recall=0.69357, Avg M-Measure=0.9507, Avg AUC_AOC=0.9504 Avg AUC_PR=0.7423\n",
      " Precision= 0.69579  f1score_micro= 0.69468  f1score_macro= 0.69093\n",
      "Total Confusion Matrix;Total M-Measure Matrix;Total AUC_AOC;Total AUC_PR\n",
      "415;23;9;9;160;10;6;;0.0000;0.9349;0.9861;0.9836;0.7847;0.9828;0.9888;;0.9432 ;0.7660\n",
      "35;302;202;7;83;2;1;;0.9239;0.0000;0.7371;0.9833;0.8541;0.9956;0.9976;;0.9149 ;0.6243\n",
      "1;71;512;48;0;0;0;;0.9904;0.8173;0.0000;0.9232;0.9955;0.9921;0.9986;;0.9524 ;0.6839\n",
      "0;0;83;493;0;35;21;;0.9899;0.9875;0.9351;0.0000;0.9988;0.9132;0.9691;;0.9655 ;0.7930\n",
      "135;76;5;0;417;0;0;;0.8033;0.8969;0.9954;0.9983;0.0000;0.9984;0.9982;;0.9480 ;0.7106\n",
      "0;1;9;88;0;450;84;;0.9877;0.9965;0.9911;0.9160;0.9990;0.0000;0.8376;;0.9545 ;0.7490\n",
      "0;1;0;26;0;125;480;;0.9944;0.9984;0.9983;0.9752;0.9989;0.8839;0.0000;;0.9745 ;0.8695\n",
      "---------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint written to /home/ubuntu/summ_15ep_2wrk/checkpoint-10287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-20 21:05:57,869 - Xworkers_1mill_0 - INFO -  10300;     4; 148117.5;   0.060; 0.73507; 0.09971; 0.69311; 0.69311; 0.69525; 0.69418; 0.69143\n",
      "2018-11-20 21:05:57,869 - Xworkers_1mill_0 - INFO -  10300;     4; 148117.5;   0.060; 0.73507; 0.09971; 0.69311; 0.69311; 0.69525; 0.69418; 0.69143\n",
      "2018-11-20 21:06:03,955 - Xworkers_1mill_0 - INFO -  10400;     4; 148700.8;   0.060; 0.73810; 0.09971; 0.68203; 0.68204; 0.68314; 0.68259; 0.67722\n",
      "2018-11-20 21:06:03,955 - Xworkers_1mill_0 - INFO -  10400;     4; 148700.8;   0.060; 0.73810; 0.09971; 0.68203; 0.68204; 0.68314; 0.68259; 0.67722\n",
      "2018-11-20 21:06:10,055 - Xworkers_1mill_0 - INFO -  10500;     4; 143935.7;   0.061; 0.73925; 0.09970; 0.68271; 0.68272; 0.68598; 0.68435; 0.67988\n",
      "2018-11-20 21:06:10,055 - Xworkers_1mill_0 - INFO -  10500;     4; 143935.7;   0.061; 0.73925; 0.09970; 0.68271; 0.68272; 0.68598; 0.68435; 0.67988\n",
      "2018-11-20 21:06:16,226 - Xworkers_1mill_0 - INFO -  10600;     4; 144140.9;   0.061; 0.73482; 0.09970; 0.68068; 0.68068; 0.68309; 0.68188; 0.67876\n",
      "2018-11-20 21:06:16,226 - Xworkers_1mill_0 - INFO -  10600;     4; 144140.9;   0.061; 0.73482; 0.09970; 0.68068; 0.68068; 0.68309; 0.68188; 0.67876\n",
      "2018-11-20 21:06:22,341 - Xworkers_1mill_0 - INFO -  10700;     4; 148324.7;   0.060; 0.71336; 0.09970; 0.69085; 0.69085; 0.69329; 0.69207; 0.68821\n",
      "2018-11-20 21:06:22,341 - Xworkers_1mill_0 - INFO -  10700;     4; 148324.7;   0.060; 0.71336; 0.09970; 0.69085; 0.69085; 0.69329; 0.69207; 0.68821\n",
      "2018-11-20 21:06:28,412 - Xworkers_1mill_0 - INFO -  10800;     4; 151101.5;   0.059; 0.72346; 0.09969; 0.69311; 0.69311; 0.69372; 0.69341; 0.68988\n",
      "2018-11-20 21:06:28,412 - Xworkers_1mill_0 - INFO -  10800;     4; 151101.5;   0.059; 0.72346; 0.09969; 0.69311; 0.69311; 0.69372; 0.69341; 0.68988\n",
      "2018-11-20 21:06:34,498 - Xworkers_1mill_0 - INFO -  10900;     4; 142303.0;   0.062; 0.71380; 0.09969; 0.69333; 0.69334; 0.69550; 0.69442; 0.69100\n",
      "2018-11-20 21:06:34,498 - Xworkers_1mill_0 - INFO -  10900;     4; 142303.0;   0.062; 0.71380; 0.09969; 0.69333; 0.69334; 0.69550; 0.69442; 0.69100\n",
      "2018-11-20 21:06:40,610 - Xworkers_1mill_0 - INFO -  11000;     4; 143974.3;   0.061; 0.72670; 0.09969; 0.68678; 0.68679; 0.68753; 0.68716; 0.68446\n",
      "2018-11-20 21:06:40,610 - Xworkers_1mill_0 - INFO -  11000;     4; 143974.3;   0.061; 0.72670; 0.09969; 0.68678; 0.68679; 0.68753; 0.68716; 0.68446\n",
      "2018-11-20 21:06:46,747 - Xworkers_1mill_0 - INFO -  11100;     4; 144598.5;   0.061; 0.71697; 0.09969; 0.69243; 0.69245; 0.69353; 0.69299; 0.69100\n",
      "2018-11-20 21:06:46,747 - Xworkers_1mill_0 - INFO -  11100;     4; 144598.5;   0.061; 0.71697; 0.09969; 0.69243; 0.69245; 0.69353; 0.69299; 0.69100\n",
      "2018-11-20 21:06:52,885 - Xworkers_1mill_0 - INFO -  11200;     4; 144286.0;   0.061; 0.73184; 0.09968; 0.67729; 0.67728; 0.67979; 0.67854; 0.67284\n",
      "2018-11-20 21:06:52,885 - Xworkers_1mill_0 - INFO -  11200;     4; 144286.0;   0.061; 0.73184; 0.09968; 0.67729; 0.67728; 0.67979; 0.67854; 0.67284\n",
      "2018-11-20 21:06:58,962 - Xworkers_1mill_0 - INFO -  11300;     4; 144772.8;   0.061; 0.73193; 0.09968; 0.69356; 0.69357; 0.69576; 0.69466; 0.69172\n",
      "2018-11-20 21:06:58,962 - Xworkers_1mill_0 - INFO -  11300;     4; 144772.8;   0.061; 0.73193; 0.09968; 0.69356; 0.69357; 0.69576; 0.69466; 0.69172\n",
      "2018-11-20 21:07:05,105 - Xworkers_1mill_0 - INFO -  11400;     4; 150521.2;   0.059; 0.69703; 0.09968; 0.70734; 0.70734; 0.70853; 0.70793; 0.70500\n",
      "2018-11-20 21:07:05,105 - Xworkers_1mill_0 - INFO -  11400;     4; 150521.2;   0.059; 0.69703; 0.09968; 0.70734; 0.70734; 0.70853; 0.70793; 0.70500\n",
      "2018-11-20 21:07:11,236 - Xworkers_1mill_0 - INFO -  11500;     4; 146111.9;   0.061; 0.71899; 0.09967; 0.70260; 0.70260; 0.70332; 0.70296; 0.70052\n",
      "2018-11-20 21:07:11,236 - Xworkers_1mill_0 - INFO -  11500;     4; 146111.9;   0.061; 0.71899; 0.09967; 0.70260; 0.70260; 0.70332; 0.70296; 0.70052\n",
      "2018-11-20 21:07:17,394 - Xworkers_1mill_0 - INFO -  11600;     4; 145319.7;   0.061; 0.71344; 0.09967; 0.68339; 0.68340; 0.68372; 0.68356; 0.68162\n",
      "2018-11-20 21:07:17,394 - Xworkers_1mill_0 - INFO -  11600;     4; 145319.7;   0.061; 0.71344; 0.09967; 0.68339; 0.68340; 0.68372; 0.68356; 0.68162\n",
      "2018-11-20 21:07:23,564 - Xworkers_1mill_0 - INFO -  11700;     4; 143112.2;   0.062; 0.71892; 0.09967; 0.68881; 0.68883; 0.69002; 0.68942; 0.68528\n",
      "2018-11-20 21:07:23,564 - Xworkers_1mill_0 - INFO -  11700;     4; 143112.2;   0.062; 0.71892; 0.09967; 0.68881; 0.68883; 0.69002; 0.68942; 0.68528\n",
      "2018-11-20 21:07:29,721 - Xworkers_1mill_0 - INFO -  11800;     4; 148358.5;   0.060; 0.70697; 0.09967; 0.69311; 0.69311; 0.69375; 0.69343; 0.69038\n",
      "2018-11-20 21:07:29,721 - Xworkers_1mill_0 - INFO -  11800;     4; 148358.5;   0.060; 0.70697; 0.09967; 0.69311; 0.69311; 0.69375; 0.69343; 0.69038\n",
      "2018-11-20 21:07:35,844 - Xworkers_1mill_0 - INFO -  11900;     4; 146643.6;   0.060; 0.71106; 0.09966; 0.69898; 0.69899; 0.69999; 0.69949; 0.69689\n",
      "2018-11-20 21:07:35,844 - Xworkers_1mill_0 - INFO -  11900;     4; 146643.6;   0.060; 0.71106; 0.09966; 0.69898; 0.69899; 0.69999; 0.69949; 0.69689\n",
      "2018-11-20 21:07:41,935 - Xworkers_1mill_0 - INFO -  12000;     4; 144804.9;   0.061; 0.71511; 0.09966; 0.70011; 0.70011; 0.70206; 0.70108; 0.69800\n",
      "2018-11-20 21:07:41,935 - Xworkers_1mill_0 - INFO -  12000;     4; 144804.9;   0.061; 0.71511; 0.09966; 0.70011; 0.70011; 0.70206; 0.70108; 0.69800\n",
      "2018-11-20 21:08:31,124 - Xworkers_1mill_0 - INFO - valid - Number of batches: 1; batch_size: 1200000; Total Time: 0:00:43.268047\n",
      "2018-11-20 21:08:31,124 - Xworkers_1mill_0 - INFO - valid - Number of batches: 1; batch_size: 1200000; Total Time: 0:00:43.268047\n",
      "2018-11-20 21:08:31,126 - Xworkers_1mill_0 - INFO - ---Validation--- Training Step: 12096; Training Epoch: 4; \n",
      " Confusion Matrix:\n",
      " [[  9874    336    150    197   3822    141     40]\n",
      " [   859  12402   4701    174   1653     16      0]\n",
      " [     0   1104   5194    380      0      2      1]\n",
      " [     0      0   1663  12236      0    848    255]\n",
      " [287603  56193   3622    846 487011    148     27]\n",
      " [     0      2      5   1277      0   7428   1248]\n",
      " [     0      0      0     32      1     99    278]]\n",
      "2018-11-20 21:08:31,126 - Xworkers_1mill_0 - INFO - ---Validation--- Training Step: 12096; Training Epoch: 4; \n",
      " Confusion Matrix:\n",
      " [[  9874    336    150    197   3822    141     40]\n",
      " [   859  12402   4701    174   1653     16      0]\n",
      " [     0   1104   5194    380      0      2      1]\n",
      " [     0      0   1663  12236      0    848    255]\n",
      " [287603  56193   3622    846 487011    148     27]\n",
      " [     0      2      5   1277      0   7428   1248]\n",
      " [     0      0      0     32      1     99    278]]\n",
      "2018-11-20 21:08:31,134 - Xworkers_1mill_0 - INFO - (Training Step, Training Epoch, loss, logloss, accuracy, recall, precision, f1score_micro, f1score_macro) in Validation:  12096;     4; 0.77565; 0.20787; 0.59257; 0.70060; 0.47882; 0.56886; 0.48563\n",
      "2018-11-20 21:08:31,134 - Xworkers_1mill_0 - INFO - (Training Step, Training Epoch, loss, logloss, accuracy, recall, precision, f1score_micro, f1score_macro) in Validation:  12096;     4; 0.77565; 0.20787; 0.59257; 0.70060; 0.47882; 0.56886; 0.48563\n",
      "2018-11-20 21:08:31,378 - Xworkers_1mill_0 - INFO -  12100;     4; 145914.4;   0.061; 0.72051; 0.09966; 0.69220; 0.69220; 0.69652; 0.69435; 0.68871\n",
      "2018-11-20 21:08:31,378 - Xworkers_1mill_0 - INFO -  12100;     4; 145914.4;   0.061; 0.72051; 0.09966; 0.69220; 0.69220; 0.69652; 0.69435; 0.68871\n",
      "2018-11-20 21:08:37,513 - Xworkers_1mill_0 - INFO -  12200;     5; 145567.6;   0.061; 0.70397; 0.09965; 0.69718; 0.69718; 0.70045; 0.69881; 0.69504\n",
      "2018-11-20 21:08:37,513 - Xworkers_1mill_0 - INFO -  12200;     5; 145567.6;   0.061; 0.70397; 0.09965; 0.69718; 0.69718; 0.70045; 0.69881; 0.69504\n",
      "2018-11-20 21:08:43,726 - Xworkers_1mill_0 - INFO -  12300;     5; 139836.5;   0.063; 0.70404; 0.09965; 0.70011; 0.70012; 0.70604; 0.70307; 0.69668\n",
      "2018-11-20 21:08:43,726 - Xworkers_1mill_0 - INFO -  12300;     5; 139836.5;   0.063; 0.70404; 0.09965; 0.70011; 0.70012; 0.70604; 0.70307; 0.69668\n",
      "2018-11-20 21:08:49,844 - Xworkers_1mill_0 - INFO -  12400;     5; 146002.7;   0.061; 0.72957; 0.09965; 0.68723; 0.68723; 0.68966; 0.68844; 0.68626\n",
      "2018-11-20 21:08:49,844 - Xworkers_1mill_0 - INFO -  12400;     5; 146002.7;   0.061; 0.72957; 0.09965; 0.68723; 0.68723; 0.68966; 0.68844; 0.68626\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-20 21:08:55,914 - Xworkers_1mill_0 - INFO -  12500;     5; 147243.9;   0.060; 0.69812; 0.09965; 0.69808; 0.69807; 0.70230; 0.70018; 0.69594\n",
      "2018-11-20 21:08:55,914 - Xworkers_1mill_0 - INFO -  12500;     5; 147243.9;   0.060; 0.69812; 0.09965; 0.69808; 0.69807; 0.70230; 0.70018; 0.69594\n",
      "2018-11-20 21:09:01,991 - Xworkers_1mill_0 - INFO -  12600;     5; 150013.9;   0.059; 0.72907; 0.09964; 0.68565; 0.68566; 0.68745; 0.68655; 0.68074\n",
      "2018-11-20 21:09:01,991 - Xworkers_1mill_0 - INFO -  12600;     5; 150013.9;   0.059; 0.72907; 0.09964; 0.68565; 0.68566; 0.68745; 0.68655; 0.68074\n",
      "2018-11-20 21:09:08,113 - Xworkers_1mill_0 - INFO -  12700;     5; 144812.9;   0.061; 0.69863; 0.09964; 0.69695; 0.69695; 0.69779; 0.69737; 0.69321\n",
      "2018-11-20 21:09:08,113 - Xworkers_1mill_0 - INFO -  12700;     5; 144812.9;   0.061; 0.69863; 0.09964; 0.69695; 0.69695; 0.69779; 0.69737; 0.69321\n",
      "2018-11-20 21:09:14,193 - Xworkers_1mill_0 - INFO -  12800;     5; 146388.5;   0.060; 0.73004; 0.09964; 0.68520; 0.68520; 0.68656; 0.68588; 0.68344\n",
      "2018-11-20 21:09:14,193 - Xworkers_1mill_0 - INFO -  12800;     5; 146388.5;   0.060; 0.73004; 0.09964; 0.68520; 0.68520; 0.68656; 0.68588; 0.68344\n",
      "2018-11-20 21:09:20,301 - Xworkers_1mill_0 - INFO -  12900;     5; 148490.8;   0.060; 0.72748; 0.09963; 0.69220; 0.69220; 0.69776; 0.69497; 0.68643\n",
      "2018-11-20 21:09:20,301 - Xworkers_1mill_0 - INFO -  12900;     5; 148490.8;   0.060; 0.72748; 0.09963; 0.69220; 0.69220; 0.69776; 0.69497; 0.68643\n",
      "2018-11-20 21:09:26,432 - Xworkers_1mill_0 - INFO -  13000;     5; 131257.4;   0.067; 0.71204; 0.09963; 0.69153; 0.69153; 0.69459; 0.69305; 0.68922\n",
      "2018-11-20 21:09:26,432 - Xworkers_1mill_0 - INFO -  13000;     5; 131257.4;   0.067; 0.71204; 0.09963; 0.69153; 0.69153; 0.69459; 0.69305; 0.68922\n",
      "2018-11-20 21:09:32,576 - Xworkers_1mill_0 - INFO -  13100;     5; 142904.5;   0.062; 0.71568; 0.09963; 0.69921; 0.69921; 0.70181; 0.70051; 0.69532\n",
      "2018-11-20 21:09:32,576 - Xworkers_1mill_0 - INFO -  13100;     5; 142904.5;   0.062; 0.71568; 0.09963; 0.69921; 0.69921; 0.70181; 0.70051; 0.69532\n",
      "2018-11-20 21:09:38,746 - Xworkers_1mill_0 - INFO -  13200;     5; 138727.5;   0.064; 0.71015; 0.09963; 0.68655; 0.68656; 0.69002; 0.68828; 0.68404\n",
      "2018-11-20 21:09:38,746 - Xworkers_1mill_0 - INFO -  13200;     5; 138727.5;   0.064; 0.71015; 0.09963; 0.68655; 0.68656; 0.69002; 0.68828; 0.68404\n",
      "2018-11-20 21:09:44,884 - Xworkers_1mill_0 - INFO -  13300;     5; 147378.9;   0.060; 0.70940; 0.09962; 0.70350; 0.70351; 0.70659; 0.70505; 0.70205\n",
      "2018-11-20 21:09:44,884 - Xworkers_1mill_0 - INFO -  13300;     5; 147378.9;   0.060; 0.70940; 0.09962; 0.70350; 0.70351; 0.70659; 0.70505; 0.70205\n",
      "2018-11-20 21:09:51,007 - Xworkers_1mill_0 - INFO -  13400;     5; 141409.0;   0.063; 0.72685; 0.09962; 0.69311; 0.69311; 0.69352; 0.69332; 0.69106\n",
      "2018-11-20 21:09:51,007 - Xworkers_1mill_0 - INFO -  13400;     5; 141409.0;   0.063; 0.72685; 0.09962; 0.69311; 0.69311; 0.69352; 0.69332; 0.69106\n",
      "2018-11-20 21:09:57,215 - Xworkers_1mill_0 - INFO -  13500;     5; 148320.0;   0.060; 0.68961; 0.09962; 0.70576; 0.70577; 0.70848; 0.70712; 0.70286\n",
      "2018-11-20 21:09:57,215 - Xworkers_1mill_0 - INFO -  13500;     5; 148320.0;   0.060; 0.68961; 0.09962; 0.70576; 0.70577; 0.70848; 0.70712; 0.70286\n",
      "2018-11-20 21:10:03,340 - Xworkers_1mill_0 - INFO -  13600;     5; 149892.7;   0.059; 0.71529; 0.09962; 0.69695; 0.69696; 0.69809; 0.69752; 0.69509\n",
      "2018-11-20 21:10:03,340 - Xworkers_1mill_0 - INFO -  13600;     5; 149892.7;   0.059; 0.71529; 0.09962; 0.69695; 0.69696; 0.69809; 0.69752; 0.69509\n",
      "2018-11-20 21:10:09,445 - Xworkers_1mill_0 - INFO -  13700;     5; 145834.1;   0.061; 0.70835; 0.09961; 0.69785; 0.69786; 0.70024; 0.69904; 0.69511\n",
      "2018-11-20 21:10:09,445 - Xworkers_1mill_0 - INFO -  13700;     5; 145834.1;   0.061; 0.70835; 0.09961; 0.69785; 0.69786; 0.70024; 0.69904; 0.69511\n",
      "2018-11-20 21:10:15,550 - Xworkers_1mill_0 - INFO -  13800;     5; 145904.6;   0.061; 0.71861; 0.09961; 0.69175; 0.69175; 0.69660; 0.69417; 0.69029\n",
      "2018-11-20 21:10:15,550 - Xworkers_1mill_0 - INFO -  13800;     5; 145904.6;   0.061; 0.71861; 0.09961; 0.69175; 0.69175; 0.69660; 0.69417; 0.69029\n",
      "2018-11-20 21:10:21,654 - Xworkers_1mill_0 - INFO -  13900;     5; 142639.3;   0.062; 0.70866; 0.09961; 0.69040; 0.69040; 0.69457; 0.69248; 0.68620\n",
      "2018-11-20 21:10:21,654 - Xworkers_1mill_0 - INFO -  13900;     5; 142639.3;   0.062; 0.70866; 0.09961; 0.69040; 0.69040; 0.69457; 0.69248; 0.68620\n",
      "2018-11-20 21:10:27,756 - Xworkers_1mill_0 - INFO -  14000;     5; 144921.4;   0.061; 0.72306; 0.09960; 0.69107; 0.69107; 0.69337; 0.69222; 0.68804\n",
      "2018-11-20 21:10:27,756 - Xworkers_1mill_0 - INFO -  14000;     5; 144921.4;   0.061; 0.72306; 0.09960; 0.69107; 0.69107; 0.69337; 0.69222; 0.68804\n",
      "2018-11-20 21:10:33,854 - Xworkers_1mill_0 - INFO -  14100;     5; 141885.8;   0.062; 0.72553; 0.09960; 0.69469; 0.69470; 0.69772; 0.69620; 0.69132\n",
      "2018-11-20 21:10:33,854 - Xworkers_1mill_0 - INFO -  14100;     5; 141885.8;   0.062; 0.72553; 0.09960; 0.69469; 0.69470; 0.69772; 0.69620; 0.69132\n",
      "2018-11-20 21:10:39,965 - Xworkers_1mill_0 - INFO -  14200;     5; 141676.7;   0.062; 0.70842; 0.09960; 0.69266; 0.69265; 0.69887; 0.69575; 0.69079\n",
      "2018-11-20 21:10:39,965 - Xworkers_1mill_0 - INFO -  14200;     5; 141676.7;   0.062; 0.70842; 0.09960; 0.69266; 0.69265; 0.69887; 0.69575; 0.69079\n",
      "2018-11-20 21:10:46,114 - Xworkers_1mill_0 - INFO -  14300;     5; 145479.2;   0.061; 0.71070; 0.09960; 0.69808; 0.69808; 0.69961; 0.69884; 0.69672\n",
      "2018-11-20 21:10:46,114 - Xworkers_1mill_0 - INFO -  14300;     5; 145479.2;   0.061; 0.71070; 0.09960; 0.69808; 0.69808; 0.69961; 0.69884; 0.69672\n",
      "2018-11-20 21:10:52,291 - Xworkers_1mill_0 - INFO -  14400;     5; 145311.2;   0.061; 0.71545; 0.09959; 0.70350; 0.70351; 0.70406; 0.70378; 0.70082\n",
      "2018-11-20 21:10:52,291 - Xworkers_1mill_0 - INFO -  14400;     5; 145311.2;   0.061; 0.71545; 0.09959; 0.70350; 0.70351; 0.70406; 0.70378; 0.70082\n",
      "2018-11-20 21:10:58,456 - Xworkers_1mill_0 - INFO -  14500;     5; 143789.1;   0.062; 0.71784; 0.09959; 0.68881; 0.68882; 0.69081; 0.68981; 0.68732\n",
      "2018-11-20 21:10:58,456 - Xworkers_1mill_0 - INFO -  14500;     5; 143789.1;   0.062; 0.71784; 0.09959; 0.68881; 0.68882; 0.69081; 0.68981; 0.68732\n",
      "2018-11-20 21:11:04,715 - Xworkers_1mill_0 - INFO -  14600;     5; 149747.0;   0.059; 0.69714; 0.09959; 0.70734; 0.70735; 0.70916; 0.70825; 0.70536\n",
      "2018-11-20 21:11:04,715 - Xworkers_1mill_0 - INFO -  14600;     5; 149747.0;   0.059; 0.69714; 0.09959; 0.70734; 0.70735; 0.70916; 0.70825; 0.70536\n",
      "2018-11-20 21:11:10,924 - Xworkers_1mill_0 - INFO -  14700;     5; 142447.1;   0.062; 0.70102; 0.09958; 0.69627; 0.69628; 0.69920; 0.69773; 0.69427\n",
      "2018-11-20 21:11:10,924 - Xworkers_1mill_0 - INFO -  14700;     5; 142447.1;   0.062; 0.70102; 0.09958; 0.69627; 0.69628; 0.69920; 0.69773; 0.69427\n",
      "2018-11-20 21:11:17,140 - Xworkers_1mill_0 - INFO -  14800;     5; 138991.4;   0.064; 0.71893; 0.09958; 0.69582; 0.69582; 0.69821; 0.69701; 0.69173\n",
      "2018-11-20 21:11:17,140 - Xworkers_1mill_0 - INFO -  14800;     5; 138991.4;   0.064; 0.71893; 0.09958; 0.69582; 0.69582; 0.69821; 0.69701; 0.69173\n",
      "2018-11-20 21:11:23,391 - Xworkers_1mill_0 - INFO -  14900;     5; 138637.8;   0.064; 0.69632; 0.09958; 0.70237; 0.70237; 0.70627; 0.70431; 0.70041\n",
      "2018-11-20 21:11:23,391 - Xworkers_1mill_0 - INFO -  14900;     5; 138637.8;   0.064; 0.69632; 0.09958; 0.70237; 0.70237; 0.70627; 0.70431; 0.70041\n",
      "2018-11-20 21:11:29,622 - Xworkers_1mill_0 - INFO -  15000;     5; 145269.6;   0.061; 0.68695; 0.09958; 0.70644; 0.70644; 0.70800; 0.70722; 0.70441\n",
      "2018-11-20 21:11:29,622 - Xworkers_1mill_0 - INFO -  15000;     5; 145269.6;   0.061; 0.68695; 0.09958; 0.70644; 0.70644; 0.70800; 0.70722; 0.70441\n",
      "2018-11-20 21:11:35,795 - Xworkers_1mill_0 - INFO -  15100;     5; 139233.8;   0.064; 0.72027; 0.09957; 0.68814; 0.68813; 0.69491; 0.69150; 0.68717\n",
      "2018-11-20 21:11:35,795 - Xworkers_1mill_0 - INFO -  15100;     5; 139233.8;   0.064; 0.72027; 0.09957; 0.68814; 0.68813; 0.69491; 0.69150; 0.68717\n",
      "2018-11-20 21:12:20,033 - Xworkers_1mill_0 - INFO - valid - Number of batches: 1; batch_size: 1200000; Total Time: 0:00:43.007246\n",
      "2018-11-20 21:12:20,033 - Xworkers_1mill_0 - INFO - valid - Number of batches: 1; batch_size: 1200000; Total Time: 0:00:43.007246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-20 21:12:20,035 - Xworkers_1mill_0 - INFO - ---Validation--- Training Step: 15120; Training Epoch: 5; \n",
      " Confusion Matrix:\n",
      " [[  8607    316    152    195   5106    137     47]\n",
      " [   566  11996   5030    178   2019     16      0]\n",
      " [     0    991   5305    382      0      2      1]\n",
      " [     0      0   1648  12230      0    859    265]\n",
      " [196553  51194   3845    854 582829    147     28]\n",
      " [     0      2      5   1273      0   7463   1217]\n",
      " [     0      0      0     32      1    102    275]]\n",
      "2018-11-20 21:12:20,035 - Xworkers_1mill_0 - INFO - ---Validation--- Training Step: 15120; Training Epoch: 5; \n",
      " Confusion Matrix:\n",
      " [[  8607    316    152    195   5106    137     47]\n",
      " [   566  11996   5030    178   2019     16      0]\n",
      " [     0    991   5305    382      0      2      1]\n",
      " [     0      0   1648  12230      0    859    265]\n",
      " [196553  51194   3845    854 582829    147     28]\n",
      " [     0      2      5   1273      0   7463   1217]\n",
      " [     0      0      0     32      1    102    275]]\n",
      "2018-11-20 21:12:20,042 - Xworkers_1mill_0 - INFO - (Training Step, Training Epoch, loss, logloss, accuracy, recall, precision, f1score_micro, f1score_macro) in Validation:  15120;     5; 0.69899; 0.18701; 0.69711; 0.70339; 0.48007; 0.57066; 0.50057\n",
      "2018-11-20 21:12:20,042 - Xworkers_1mill_0 - INFO - (Training Step, Training Epoch, loss, logloss, accuracy, recall, precision, f1score_micro, f1score_macro) in Validation:  15120;     5; 0.69899; 0.18701; 0.69711; 0.70339; 0.48007; 0.57066; 0.50057\n",
      "2018-11-20 21:12:25,037 - Xworkers_1mill_0 - INFO -  15200;     6; 137795.4;   0.064; 0.72795; 0.09957; 0.69266; 0.69265; 0.69347; 0.69306; 0.69035\n",
      "2018-11-20 21:12:25,037 - Xworkers_1mill_0 - INFO -  15200;     6; 137795.4;   0.064; 0.72795; 0.09957; 0.69266; 0.69265; 0.69347; 0.69306; 0.69035\n",
      "2018-11-20 21:12:31,251 - Xworkers_1mill_0 - INFO -  15300;     6; 147605.1;   0.060; 0.69244; 0.09957; 0.69740; 0.69741; 0.70121; 0.69930; 0.69539\n",
      "2018-11-20 21:12:31,251 - Xworkers_1mill_0 - INFO -  15300;     6; 147605.1;   0.060; 0.69244; 0.09957; 0.69740; 0.69741; 0.70121; 0.69930; 0.69539\n",
      "2018-11-20 21:12:37,469 - Xworkers_1mill_0 - INFO -  15400;     6; 142716.6;   0.062; 0.70656; 0.09956; 0.68927; 0.68927; 0.68951; 0.68939; 0.68681\n",
      "2018-11-20 21:12:37,469 - Xworkers_1mill_0 - INFO -  15400;     6; 142716.6;   0.062; 0.70656; 0.09956; 0.68927; 0.68927; 0.68951; 0.68939; 0.68681\n",
      "2018-11-20 21:12:43,685 - Xworkers_1mill_0 - INFO -  15500;     6; 152332.4;   0.058; 0.70455; 0.09956; 0.70147; 0.70148; 0.70538; 0.70342; 0.69933\n",
      "2018-11-20 21:12:43,685 - Xworkers_1mill_0 - INFO -  15500;     6; 152332.4;   0.058; 0.70455; 0.09956; 0.70147; 0.70148; 0.70538; 0.70342; 0.69933\n",
      "2018-11-20 21:12:49,797 - Xworkers_1mill_0 - INFO -  15600;     6; 147949.9;   0.060; 0.69469; 0.09956; 0.69876; 0.69876; 0.70181; 0.70028; 0.69453\n",
      "2018-11-20 21:12:49,797 - Xworkers_1mill_0 - INFO -  15600;     6; 147949.9;   0.060; 0.69469; 0.09956; 0.69876; 0.69876; 0.70181; 0.70028; 0.69453\n",
      "2018-11-20 21:12:55,926 - Xworkers_1mill_0 - INFO -  15700;     6; 139077.8;   0.064; 0.69892; 0.09956; 0.69853; 0.69853; 0.69981; 0.69917; 0.69456\n",
      "2018-11-20 21:12:55,926 - Xworkers_1mill_0 - INFO -  15700;     6; 139077.8;   0.064; 0.69892; 0.09956; 0.69853; 0.69853; 0.69981; 0.69917; 0.69456\n",
      "2018-11-20 21:13:02,066 - Xworkers_1mill_0 - INFO -  15800;     6; 136548.4;   0.065; 0.69830; 0.09955; 0.70395; 0.70395; 0.70534; 0.70465; 0.69979\n",
      "2018-11-20 21:13:02,066 - Xworkers_1mill_0 - INFO -  15800;     6; 136548.4;   0.065; 0.69830; 0.09955; 0.70395; 0.70395; 0.70534; 0.70465; 0.69979\n",
      "2018-11-20 21:13:08,155 - Xworkers_1mill_0 - INFO -  15900;     6; 146340.0;   0.060; 0.68459; 0.09955; 0.69989; 0.69990; 0.70090; 0.70040; 0.69747\n",
      "2018-11-20 21:13:08,155 - Xworkers_1mill_0 - INFO -  15900;     6; 146340.0;   0.060; 0.68459; 0.09955; 0.69989; 0.69990; 0.70090; 0.70040; 0.69747\n",
      "2018-11-20 21:13:14,271 - Xworkers_1mill_0 - INFO -  16000;     6; 136863.1;   0.065; 0.71003; 0.09955; 0.69763; 0.69764; 0.70034; 0.69899; 0.69494\n",
      "2018-11-20 21:13:14,271 - Xworkers_1mill_0 - INFO -  16000;     6; 136863.1;   0.065; 0.71003; 0.09955; 0.69763; 0.69764; 0.70034; 0.69899; 0.69494\n",
      "2018-11-20 21:13:20,408 - Xworkers_1mill_0 - INFO -  16100;     6; 152446.9;   0.058; 0.70652; 0.09954; 0.70169; 0.70170; 0.70309; 0.70240; 0.70002\n",
      "2018-11-20 21:13:20,408 - Xworkers_1mill_0 - INFO -  16100;     6; 152446.9;   0.058; 0.70652; 0.09954; 0.70169; 0.70170; 0.70309; 0.70240; 0.70002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint written to /home/ubuntu/summ_15ep_2wrk/checkpoint-16185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-20 21:13:27,619 - Xworkers_1mill_0 - INFO -  16200;     6; 142857.8;   0.062; 0.70834; 0.09954; 0.69650; 0.69651; 0.69814; 0.69732; 0.69304\n",
      "2018-11-20 21:13:27,619 - Xworkers_1mill_0 - INFO -  16200;     6; 142857.8;   0.062; 0.70834; 0.09954; 0.69650; 0.69651; 0.69814; 0.69732; 0.69304\n",
      "2018-11-20 21:13:33,815 - Xworkers_1mill_0 - INFO -  16300;     6; 151603.8;   0.058; 0.70759; 0.09954; 0.69356; 0.69356; 0.69598; 0.69476; 0.69100\n",
      "2018-11-20 21:13:33,815 - Xworkers_1mill_0 - INFO -  16300;     6; 151603.8;   0.058; 0.70759; 0.09954; 0.69356; 0.69356; 0.69598; 0.69476; 0.69100\n",
      "2018-11-20 21:13:39,931 - Xworkers_1mill_0 - INFO -  16400;     6; 141137.5;   0.063; 0.70905; 0.09954; 0.69424; 0.69424; 0.69779; 0.69601; 0.69197\n",
      "2018-11-20 21:13:39,931 - Xworkers_1mill_0 - INFO -  16400;     6; 141137.5;   0.063; 0.70905; 0.09954; 0.69424; 0.69424; 0.69779; 0.69601; 0.69197\n",
      "2018-11-20 21:13:46,010 - Xworkers_1mill_0 - INFO -  16500;     6; 146601.9;   0.060; 0.70258; 0.09953; 0.70260; 0.70260; 0.70750; 0.70504; 0.70104\n",
      "2018-11-20 21:13:46,010 - Xworkers_1mill_0 - INFO -  16500;     6; 146601.9;   0.060; 0.70258; 0.09953; 0.70260; 0.70260; 0.70750; 0.70504; 0.70104\n",
      "2018-11-20 21:13:52,116 - Xworkers_1mill_0 - INFO -  16600;     6; 150938.0;   0.059; 0.69496; 0.09953; 0.70147; 0.70147; 0.70251; 0.70199; 0.69998\n",
      "2018-11-20 21:13:52,116 - Xworkers_1mill_0 - INFO -  16600;     6; 150938.0;   0.059; 0.69496; 0.09953; 0.70147; 0.70147; 0.70251; 0.70199; 0.69998\n",
      "2018-11-20 21:13:58,282 - Xworkers_1mill_0 - INFO -  16700;     6; 146755.5;   0.060; 0.70684; 0.09953; 0.69356; 0.69356; 0.69614; 0.69485; 0.69195\n",
      "2018-11-20 21:13:58,282 - Xworkers_1mill_0 - INFO -  16700;     6; 146755.5;   0.060; 0.70684; 0.09953; 0.69356; 0.69356; 0.69614; 0.69485; 0.69195\n",
      "2018-11-20 21:14:04,407 - Xworkers_1mill_0 - INFO -  16800;     6; 148398.8;   0.060; 0.70868; 0.09953; 0.68949; 0.68949; 0.69045; 0.68997; 0.68644\n",
      "2018-11-20 21:14:04,407 - Xworkers_1mill_0 - INFO -  16800;     6; 148398.8;   0.060; 0.70868; 0.09953; 0.68949; 0.68949; 0.69045; 0.68997; 0.68644\n",
      "2018-11-20 21:14:10,520 - Xworkers_1mill_0 - INFO -  16900;     6; 132275.7;   0.067; 0.68990; 0.09952; 0.69785; 0.69788; 0.69677; 0.69733; 0.69552\n",
      "2018-11-20 21:14:10,520 - Xworkers_1mill_0 - INFO -  16900;     6; 132275.7;   0.067; 0.68990; 0.09952; 0.69785; 0.69788; 0.69677; 0.69733; 0.69552\n",
      "2018-11-20 21:14:16,643 - Xworkers_1mill_0 - INFO -  17000;     6; 143425.8;   0.062; 0.71237; 0.09952; 0.69831; 0.69830; 0.70017; 0.69923; 0.69614\n",
      "2018-11-20 21:14:16,643 - Xworkers_1mill_0 - INFO -  17000;     6; 143425.8;   0.062; 0.71237; 0.09952; 0.69831; 0.69830; 0.70017; 0.69923; 0.69614\n",
      "2018-11-20 21:14:22,758 - Xworkers_1mill_0 - INFO -  17100;     6; 141051.2;   0.063; 0.69734; 0.09952; 0.69695; 0.69696; 0.69995; 0.69845; 0.69532\n",
      "2018-11-20 21:14:22,758 - Xworkers_1mill_0 - INFO -  17100;     6; 141051.2;   0.063; 0.69734; 0.09952; 0.69695; 0.69696; 0.69995; 0.69845; 0.69532\n",
      "2018-11-20 21:14:28,914 - Xworkers_1mill_0 - INFO -  17200;     6; 142003.6;   0.062; 0.70645; 0.09951; 0.70124; 0.70125; 0.70619; 0.70371; 0.69898\n",
      "2018-11-20 21:14:28,914 - Xworkers_1mill_0 - INFO -  17200;     6; 142003.6;   0.062; 0.70645; 0.09951; 0.70124; 0.70125; 0.70619; 0.70371; 0.69898\n",
      "2018-11-20 21:14:34,989 - Xworkers_1mill_0 - INFO -  17300;     6; 144648.1;   0.061; 0.71140; 0.09951; 0.68814; 0.68814; 0.68792; 0.68803; 0.68484\n",
      "2018-11-20 21:14:34,989 - Xworkers_1mill_0 - INFO -  17300;     6; 144648.1;   0.061; 0.71140; 0.09951; 0.68814; 0.68814; 0.68792; 0.68803; 0.68484\n",
      "2018-11-20 21:14:41,111 - Xworkers_1mill_0 - INFO -  17400;     6; 147414.0;   0.060; 0.71000; 0.09951; 0.70056; 0.70057; 0.70182; 0.70119; 0.69743\n",
      "2018-11-20 21:14:41,111 - Xworkers_1mill_0 - INFO -  17400;     6; 147414.0;   0.060; 0.71000; 0.09951; 0.70056; 0.70057; 0.70182; 0.70119; 0.69743\n",
      "2018-11-20 21:14:47,283 - Xworkers_1mill_0 - INFO -  17500;     6; 146893.1;   0.060; 0.68234; 0.09951; 0.71503; 0.71503; 0.71556; 0.71530; 0.71296\n",
      "2018-11-20 21:14:47,283 - Xworkers_1mill_0 - INFO -  17500;     6; 146893.1;   0.060; 0.68234; 0.09951; 0.71503; 0.71503; 0.71556; 0.71530; 0.71296\n",
      "2018-11-20 21:14:53,446 - Xworkers_1mill_0 - INFO -  17600;     6; 146644.7;   0.060; 0.70846; 0.09950; 0.69695; 0.69695; 0.69780; 0.69738; 0.69484\n",
      "2018-11-20 21:14:53,446 - Xworkers_1mill_0 - INFO -  17600;     6; 146644.7;   0.060; 0.70846; 0.09950; 0.69695; 0.69695; 0.69780; 0.69738; 0.69484\n",
      "2018-11-20 21:14:59,714 - Xworkers_1mill_0 - INFO -  17700;     6; 139132.0;   0.064; 0.72296; 0.09950; 0.69446; 0.69446; 0.69546; 0.69496; 0.69266\n",
      "2018-11-20 21:14:59,714 - Xworkers_1mill_0 - INFO -  17700;     6; 139132.0;   0.064; 0.72296; 0.09950; 0.69446; 0.69446; 0.69546; 0.69496; 0.69266\n",
      "2018-11-20 21:15:06,029 - Xworkers_1mill_0 - INFO -  17800;     6; 144507.3;   0.061; 0.70177; 0.09950; 0.69740; 0.69741; 0.69855; 0.69798; 0.69568\n",
      "2018-11-20 21:15:06,029 - Xworkers_1mill_0 - INFO -  17800;     6; 144507.3;   0.061; 0.70177; 0.09950; 0.69740; 0.69741; 0.69855; 0.69798; 0.69568\n",
      "2018-11-20 21:15:12,292 - Xworkers_1mill_0 - INFO -  17900;     6; 142763.3;   0.062; 0.70342; 0.09949; 0.69424; 0.69425; 0.69425; 0.69425; 0.69132\n",
      "2018-11-20 21:15:12,292 - Xworkers_1mill_0 - INFO -  17900;     6; 142763.3;   0.062; 0.70342; 0.09949; 0.69424; 0.69425; 0.69425; 0.69425; 0.69132\n",
      "2018-11-20 21:15:18,547 - Xworkers_1mill_0 - INFO -  18000;     6; 141707.6;   0.062; 0.71845; 0.09949; 0.69333; 0.69334; 0.69530; 0.69432; 0.69054\n",
      "2018-11-20 21:15:18,547 - Xworkers_1mill_0 - INFO -  18000;     6; 141707.6;   0.062; 0.71845; 0.09949; 0.69333; 0.69334; 0.69530; 0.69432; 0.69054\n",
      "2018-11-20 21:15:24,777 - Xworkers_1mill_0 - INFO -  18100;     6; 136474.6;   0.065; 0.70310; 0.09949; 0.69808; 0.69809; 0.70068; 0.69938; 0.69712\n",
      "2018-11-20 21:15:24,777 - Xworkers_1mill_0 - INFO -  18100;     6; 136474.6;   0.065; 0.70310; 0.09949; 0.69808; 0.69809; 0.70068; 0.69938; 0.69712\n",
      "2018-11-20 21:16:12,183 - Xworkers_1mill_0 - INFO - valid - Number of batches: 1; batch_size: 1200000; Total Time: 0:00:44.670375\n",
      "2018-11-20 21:16:12,183 - Xworkers_1mill_0 - INFO - valid - Number of batches: 1; batch_size: 1200000; Total Time: 0:00:44.670375\n",
      "2018-11-20 21:16:12,185 - Xworkers_1mill_0 - INFO - ---Validation--- Training Step: 18144; Training Epoch: 6; \n",
      " Confusion Matrix:\n",
      " [[  7410    377    143    200   6252    146     32]\n",
      " [   358  12417   4739    162   2113     16      0]\n",
      " [     0   1083   5227    368      0      2      1]\n",
      " [     0      0   1728  12180      0    898    196]\n",
      " [121971  57448   3545    829 651481    149     27]\n",
      " [     0      2      9   1276      0   7793    880]\n",
      " [     0      0      0     32      1    117    260]]\n",
      "2018-11-20 21:16:12,185 - Xworkers_1mill_0 - INFO - ---Validation--- Training Step: 18144; Training Epoch: 6; \n",
      " Confusion Matrix:\n",
      " [[  7410    377    143    200   6252    146     32]\n",
      " [   358  12417   4739    162   2113     16      0]\n",
      " [     0   1083   5227    368      0      2      1]\n",
      " [     0      0   1728  12180      0    898    196]\n",
      " [121971  57448   3545    829 651481    149     27]\n",
      " [     0      2      9   1276      0   7793    880]\n",
      " [     0      0      0     32      1    117    260]]\n",
      "2018-11-20 21:16:12,193 - Xworkers_1mill_0 - INFO - (Training Step, Training Epoch, loss, logloss, accuracy, recall, precision, f1score_micro, f1score_macro) in Validation:  18144;     6; 0.64823; 0.17286; 0.77258; 0.70379; 0.48689; 0.57559; 0.51938\n",
      "2018-11-20 21:16:12,193 - Xworkers_1mill_0 - INFO - (Training Step, Training Epoch, loss, logloss, accuracy, recall, precision, f1score_micro, f1score_macro) in Validation:  18144;     6; 0.64823; 0.17286; 0.77258; 0.70379; 0.48689; 0.57559; 0.51938\n",
      "2018-11-20 21:16:15,639 - Xworkers_1mill_0 - INFO -  18200;     7; 145129.9;   0.061; 0.69702; 0.09949; 0.69582; 0.69583; 0.69569; 0.69576; 0.69290\n",
      "2018-11-20 21:16:15,639 - Xworkers_1mill_0 - INFO -  18200;     7; 145129.9;   0.061; 0.69702; 0.09949; 0.69582; 0.69583; 0.69569; 0.69576; 0.69290\n",
      "2018-11-20 21:16:21,799 - Xworkers_1mill_0 - INFO -  18300;     7; 142984.9;   0.062; 0.70250; 0.09948; 0.70486; 0.70487; 0.70781; 0.70633; 0.70189\n",
      "2018-11-20 21:16:21,799 - Xworkers_1mill_0 - INFO -  18300;     7; 142984.9;   0.062; 0.70250; 0.09948; 0.70486; 0.70487; 0.70781; 0.70633; 0.70189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-20 21:16:27,993 - Xworkers_1mill_0 - INFO -  18400;     7; 144126.9;   0.061; 0.68804; 0.09948; 0.70802; 0.70802; 0.70985; 0.70894; 0.70673\n",
      "2018-11-20 21:16:27,993 - Xworkers_1mill_0 - INFO -  18400;     7; 144126.9;   0.061; 0.68804; 0.09948; 0.70802; 0.70802; 0.70985; 0.70894; 0.70673\n",
      "2018-11-20 21:16:34,205 - Xworkers_1mill_0 - INFO -  18500;     7; 137706.4;   0.064; 0.68929; 0.09948; 0.70328; 0.70328; 0.70588; 0.70458; 0.69992\n",
      "2018-11-20 21:16:34,205 - Xworkers_1mill_0 - INFO -  18500;     7; 137706.4;   0.064; 0.68929; 0.09948; 0.70328; 0.70328; 0.70588; 0.70458; 0.69992\n",
      "2018-11-20 21:16:40,366 - Xworkers_1mill_0 - INFO -  18600;     7; 146803.0;   0.060; 0.70418; 0.09947; 0.69446; 0.69447; 0.69403; 0.69425; 0.69235\n",
      "2018-11-20 21:16:40,366 - Xworkers_1mill_0 - INFO -  18600;     7; 146803.0;   0.060; 0.70418; 0.09947; 0.69446; 0.69447; 0.69403; 0.69425; 0.69235\n",
      "2018-11-20 21:16:46,512 - Xworkers_1mill_0 - INFO -  18700;     7; 140910.3;   0.063; 0.70778; 0.09947; 0.68972; 0.68972; 0.69299; 0.69135; 0.68664\n",
      "2018-11-20 21:16:46,512 - Xworkers_1mill_0 - INFO -  18700;     7; 140910.3;   0.063; 0.70778; 0.09947; 0.68972; 0.68972; 0.69299; 0.69135; 0.68664\n",
      "2018-11-20 21:16:52,775 - Xworkers_1mill_0 - INFO -  18800;     7; 137003.0;   0.065; 0.69126; 0.09947; 0.69966; 0.69966; 0.70041; 0.70004; 0.69815\n",
      "2018-11-20 21:16:52,775 - Xworkers_1mill_0 - INFO -  18800;     7; 137003.0;   0.065; 0.69126; 0.09947; 0.69966; 0.69966; 0.70041; 0.70004; 0.69815\n",
      "2018-11-20 21:16:58,969 - Xworkers_1mill_0 - INFO -  18900;     7; 145894.9;   0.061; 0.70620; 0.09947; 0.69333; 0.69334; 0.69601; 0.69467; 0.69149\n",
      "2018-11-20 21:16:58,969 - Xworkers_1mill_0 - INFO -  18900;     7; 145894.9;   0.061; 0.70620; 0.09947; 0.69333; 0.69334; 0.69601; 0.69467; 0.69149\n",
      "2018-11-20 21:17:05,158 - Xworkers_1mill_0 - INFO -  19000;     7; 144145.9;   0.061; 0.69548; 0.09946; 0.71729; 0.71729; 0.71823; 0.71776; 0.71561\n",
      "2018-11-20 21:17:05,158 - Xworkers_1mill_0 - INFO -  19000;     7; 144145.9;   0.061; 0.69548; 0.09946; 0.71729; 0.71729; 0.71823; 0.71776; 0.71561\n",
      "2018-11-20 21:17:11,313 - Xworkers_1mill_0 - INFO -  19100;     7; 144594.0;   0.061; 0.69288; 0.09946; 0.70079; 0.70079; 0.70184; 0.70132; 0.69845\n",
      "2018-11-20 21:17:11,313 - Xworkers_1mill_0 - INFO -  19100;     7; 144594.0;   0.061; 0.69288; 0.09946; 0.70079; 0.70079; 0.70184; 0.70132; 0.69845\n",
      "2018-11-20 21:17:17,420 - Xworkers_1mill_0 - INFO -  19200;     7; 146169.5;   0.061; 0.72021; 0.09946; 0.68881; 0.68881; 0.69062; 0.68971; 0.68645\n",
      "2018-11-20 21:17:17,420 - Xworkers_1mill_0 - INFO -  19200;     7; 146169.5;   0.061; 0.72021; 0.09946; 0.68881; 0.68881; 0.69062; 0.68971; 0.68645\n",
      "2018-11-20 21:17:23,548 - Xworkers_1mill_0 - INFO -  19300;     7; 111732.6;   0.079; 0.72260; 0.09945; 0.69085; 0.69085; 0.69037; 0.69061; 0.68813\n",
      "2018-11-20 21:17:23,548 - Xworkers_1mill_0 - INFO -  19300;     7; 111732.6;   0.079; 0.72260; 0.09945; 0.69085; 0.69085; 0.69037; 0.69061; 0.68813\n",
      "2018-11-20 21:17:29,677 - Xworkers_1mill_0 - INFO -  19400;     7; 147381.8;   0.060; 0.69606; 0.09945; 0.70328; 0.70328; 0.70422; 0.70375; 0.70054\n",
      "2018-11-20 21:17:29,677 - Xworkers_1mill_0 - INFO -  19400;     7; 147381.8;   0.060; 0.69606; 0.09945; 0.70328; 0.70328; 0.70422; 0.70375; 0.70054\n",
      "2018-11-20 21:17:35,860 - Xworkers_1mill_0 - INFO -  19500;     7; 125783.1;   0.070; 0.70738; 0.09945; 0.69311; 0.69311; 0.69307; 0.69309; 0.69079\n",
      "2018-11-20 21:17:35,860 - Xworkers_1mill_0 - INFO -  19500;     7; 125783.1;   0.070; 0.70738; 0.09945; 0.69311; 0.69311; 0.69307; 0.69309; 0.69079\n",
      "2018-11-20 21:17:41,992 - Xworkers_1mill_0 - INFO -  19600;     7; 148787.8;   0.059; 0.70516; 0.09945; 0.69401; 0.69402; 0.69551; 0.69476; 0.69114\n",
      "2018-11-20 21:17:41,992 - Xworkers_1mill_0 - INFO -  19600;     7; 148787.8;   0.059; 0.70516; 0.09945; 0.69401; 0.69402; 0.69551; 0.69476; 0.69114\n",
      "2018-11-20 21:17:48,124 - Xworkers_1mill_0 - INFO -  19700;     7; 145114.0;   0.061; 0.70883; 0.09944; 0.70011; 0.70012; 0.70050; 0.70031; 0.69775\n",
      "2018-11-20 21:17:48,124 - Xworkers_1mill_0 - INFO -  19700;     7; 145114.0;   0.061; 0.70883; 0.09944; 0.70011; 0.70012; 0.70050; 0.70031; 0.69775\n",
      "2018-11-20 21:17:54,280 - Xworkers_1mill_0 - INFO -  19800;     7; 143357.6;   0.062; 0.68710; 0.09944; 0.70938; 0.70938; 0.71097; 0.71018; 0.70766\n",
      "2018-11-20 21:17:54,280 - Xworkers_1mill_0 - INFO -  19800;     7; 143357.6;   0.062; 0.68710; 0.09944; 0.70938; 0.70938; 0.71097; 0.71018; 0.70766\n",
      "2018-11-20 21:18:00,427 - Xworkers_1mill_0 - INFO -  19900;     7; 143016.3;   0.062; 0.70334; 0.09944; 0.70282; 0.70283; 0.70439; 0.70361; 0.69933\n",
      "2018-11-20 21:18:00,427 - Xworkers_1mill_0 - INFO -  19900;     7; 143016.3;   0.062; 0.70334; 0.09944; 0.70282; 0.70283; 0.70439; 0.70361; 0.69933\n",
      "2018-11-20 21:18:06,618 - Xworkers_1mill_0 - INFO -  20000;     7; 144090.5;   0.061; 0.68701; 0.09944; 0.70644; 0.70643; 0.70965; 0.70804; 0.70418\n",
      "2018-11-20 21:18:06,618 - Xworkers_1mill_0 - INFO -  20000;     7; 144090.5;   0.061; 0.68701; 0.09944; 0.70644; 0.70643; 0.70965; 0.70804; 0.70418\n",
      "2018-11-20 21:18:12,843 - Xworkers_1mill_0 - INFO -  20100;     7; 143420.8;   0.062; 0.70468; 0.09943; 0.70282; 0.70283; 0.70602; 0.70442; 0.70155\n",
      "2018-11-20 21:18:12,843 - Xworkers_1mill_0 - INFO -  20100;     7; 143420.8;   0.062; 0.70468; 0.09943; 0.70282; 0.70283; 0.70602; 0.70442; 0.70155\n",
      "2018-11-20 21:18:18,977 - Xworkers_1mill_0 - INFO -  20200;     7; 146396.6;   0.060; 0.71856; 0.09943; 0.69379; 0.69378; 0.69614; 0.69496; 0.69012\n",
      "2018-11-20 21:18:18,977 - Xworkers_1mill_0 - INFO -  20200;     7; 146396.6;   0.060; 0.71856; 0.09943; 0.69379; 0.69378; 0.69614; 0.69496; 0.69012\n",
      "2018-11-20 21:18:25,064 - Xworkers_1mill_0 - INFO -  20300;     7; 147756.7;   0.060; 0.70776; 0.09943; 0.69175; 0.69175; 0.69278; 0.69227; 0.68789\n",
      "2018-11-20 21:18:25,064 - Xworkers_1mill_0 - INFO -  20300;     7; 147756.7;   0.060; 0.70776; 0.09943; 0.69175; 0.69175; 0.69278; 0.69227; 0.68789\n",
      "2018-11-20 21:18:31,304 - Xworkers_1mill_0 - INFO -  20400;     7; 141353.0;   0.063; 0.69826; 0.09942; 0.71254; 0.71254; 0.71485; 0.71370; 0.70989\n",
      "2018-11-20 21:18:31,304 - Xworkers_1mill_0 - INFO -  20400;     7; 141353.0;   0.063; 0.69826; 0.09942; 0.71254; 0.71254; 0.71485; 0.71370; 0.70989\n",
      "2018-11-20 21:18:37,497 - Xworkers_1mill_0 - INFO -  20500;     7; 143901.2;   0.062; 0.67714; 0.09942; 0.71345; 0.71345; 0.71689; 0.71516; 0.71014\n",
      "2018-11-20 21:18:37,497 - Xworkers_1mill_0 - INFO -  20500;     7; 143901.2;   0.062; 0.67714; 0.09942; 0.71345; 0.71345; 0.71689; 0.71516; 0.71014\n",
      "2018-11-20 21:18:43,664 - Xworkers_1mill_0 - INFO -  20600;     7; 145712.2;   0.061; 0.70098; 0.09942; 0.70147; 0.70146; 0.70453; 0.70299; 0.69753\n",
      "2018-11-20 21:18:43,664 - Xworkers_1mill_0 - INFO -  20600;     7; 145712.2;   0.061; 0.70098; 0.09942; 0.70147; 0.70146; 0.70453; 0.70299; 0.69753\n",
      "2018-11-20 21:18:49,872 - Xworkers_1mill_0 - INFO -  20700;     7; 147692.6;   0.060; 0.68493; 0.09942; 0.70870; 0.70870; 0.71206; 0.71038; 0.70604\n",
      "2018-11-20 21:18:49,872 - Xworkers_1mill_0 - INFO -  20700;     7; 147692.6;   0.060; 0.68493; 0.09942; 0.70870; 0.70870; 0.71206; 0.71038; 0.70604\n",
      "2018-11-20 21:18:56,098 - Xworkers_1mill_0 - INFO -  20800;     7; 142390.3;   0.062; 0.70848; 0.09941; 0.69831; 0.69832; 0.70097; 0.69964; 0.69772\n",
      "2018-11-20 21:18:56,098 - Xworkers_1mill_0 - INFO -  20800;     7; 142390.3;   0.062; 0.70848; 0.09941; 0.69831; 0.69832; 0.70097; 0.69964; 0.69772\n",
      "2018-11-20 21:19:02,386 - Xworkers_1mill_0 - INFO -  20900;     7; 139322.6;   0.064; 0.70476; 0.09941; 0.70192; 0.70192; 0.70257; 0.70225; 0.69929\n",
      "2018-11-20 21:19:02,386 - Xworkers_1mill_0 - INFO -  20900;     7; 139322.6;   0.064; 0.70476; 0.09941; 0.70192; 0.70192; 0.70257; 0.70225; 0.69929\n",
      "2018-11-20 21:19:08,640 - Xworkers_1mill_0 - INFO -  21000;     7; 146640.1;   0.060; 0.70706; 0.09941; 0.70395; 0.70395; 0.70558; 0.70476; 0.70031\n",
      "2018-11-20 21:19:08,640 - Xworkers_1mill_0 - INFO -  21000;     7; 146640.1;   0.060; 0.70706; 0.09941; 0.70395; 0.70395; 0.70558; 0.70476; 0.70031\n",
      "2018-11-20 21:19:14,909 - Xworkers_1mill_0 - INFO -  21100;     7; 142146.6;   0.062; 0.68537; 0.09940; 0.71096; 0.71096; 0.71381; 0.71238; 0.70959\n",
      "2018-11-20 21:19:14,909 - Xworkers_1mill_0 - INFO -  21100;     7; 142146.6;   0.062; 0.68537; 0.09940; 0.71096; 0.71096; 0.71381; 0.71238; 0.70959\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-20 21:20:04,444 - Xworkers_1mill_0 - INFO - valid - Number of batches: 1; batch_size: 1200000; Total Time: 0:00:45.297328\n",
      "2018-11-20 21:20:04,444 - Xworkers_1mill_0 - INFO - valid - Number of batches: 1; batch_size: 1200000; Total Time: 0:00:45.297328\n",
      "2018-11-20 21:20:04,446 - Xworkers_1mill_0 - INFO - ---Validation--- Training Step: 21168; Training Epoch: 7; \n",
      " Confusion Matrix:\n",
      " [[  8450    330    136    202   5267    149     26]\n",
      " [   523  12632   4399    162   2073     16      0]\n",
      " [     0   1205   5110    363      0      2      1]\n",
      " [     1      0   1731  12208      0    928    134]\n",
      " [187224  50614   3234    840 593365    158     15]\n",
      " [     0      2      7   1287      0   7990    674]\n",
      " [     0      0      0     33      1    133    243]]\n",
      "2018-11-20 21:20:04,446 - Xworkers_1mill_0 - INFO - ---Validation--- Training Step: 21168; Training Epoch: 7; \n",
      " Confusion Matrix:\n",
      " [[  8450    330    136    202   5267    149     26]\n",
      " [   523  12632   4399    162   2073     16      0]\n",
      " [     0   1205   5110    363      0      2      1]\n",
      " [     1      0   1731  12208      0    928    134]\n",
      " [187224  50614   3234    840 593365    158     15]\n",
      " [     0      2      7   1287      0   7990    674]\n",
      " [     0      0      0     33      1    133    243]]\n",
      "2018-11-20 21:20:04,451 - Xworkers_1mill_0 - INFO - (Training Step, Training Epoch, loss, logloss, accuracy, recall, precision, f1score_micro, f1score_macro) in Validation:  21168;     7; 0.68085; 0.18238; 0.70964; 0.70027; 0.49410; 0.57939; 0.52087\n",
      "2018-11-20 21:20:04,451 - Xworkers_1mill_0 - INFO - (Training Step, Training Epoch, loss, logloss, accuracy, recall, precision, f1score_micro, f1score_macro) in Validation:  21168;     7; 0.68085; 0.18238; 0.70964; 0.70027; 0.49410; 0.57939; 0.52087\n",
      "2018-11-20 21:20:06,438 - Xworkers_1mill_0 - INFO -  21200;     8; 133980.1;   0.066; 0.71143; 0.09940; 0.69740; 0.69741; 0.69828; 0.69785; 0.69475\n",
      "2018-11-20 21:20:06,438 - Xworkers_1mill_0 - INFO -  21200;     8; 133980.1;   0.066; 0.71143; 0.09940; 0.69740; 0.69741; 0.69828; 0.69785; 0.69475\n",
      "2018-11-20 21:20:12,660 - Xworkers_1mill_0 - INFO -  21300;     8; 141432.7;   0.063; 0.68727; 0.09940; 0.70373; 0.70373; 0.70667; 0.70520; 0.70178\n",
      "2018-11-20 21:20:12,660 - Xworkers_1mill_0 - INFO -  21300;     8; 141432.7;   0.063; 0.68727; 0.09940; 0.70373; 0.70373; 0.70667; 0.70520; 0.70178\n",
      "2018-11-20 21:20:18,858 - Xworkers_1mill_0 - INFO -  21400;     8; 145218.5;   0.061; 0.68854; 0.09940; 0.70554; 0.70554; 0.70652; 0.70603; 0.70247\n",
      "2018-11-20 21:20:18,858 - Xworkers_1mill_0 - INFO -  21400;     8; 145218.5;   0.061; 0.68854; 0.09940; 0.70554; 0.70554; 0.70652; 0.70603; 0.70247\n",
      "2018-11-20 21:20:25,062 - Xworkers_1mill_0 - INFO -  21500;     8; 141755.7;   0.062; 0.71368; 0.09939; 0.70328; 0.70329; 0.70412; 0.70370; 0.70026\n",
      "2018-11-20 21:20:25,062 - Xworkers_1mill_0 - INFO -  21500;     8; 141755.7;   0.062; 0.71368; 0.09939; 0.70328; 0.70329; 0.70412; 0.70370; 0.70026\n",
      "2018-11-20 21:20:31,226 - Xworkers_1mill_0 - INFO -  21600;     8; 141591.9;   0.063; 0.70552; 0.09939; 0.69763; 0.69764; 0.69897; 0.69831; 0.69415\n",
      "2018-11-20 21:20:31,226 - Xworkers_1mill_0 - INFO -  21600;     8; 141591.9;   0.063; 0.70552; 0.09939; 0.69763; 0.69764; 0.69897; 0.69831; 0.69415\n",
      "2018-11-20 21:20:37,392 - Xworkers_1mill_0 - INFO -  21700;     8; 146546.9;   0.060; 0.67836; 0.09939; 0.70644; 0.70645; 0.70738; 0.70692; 0.70410\n",
      "2018-11-20 21:20:37,392 - Xworkers_1mill_0 - INFO -  21700;     8; 146546.9;   0.060; 0.67836; 0.09939; 0.70644; 0.70645; 0.70738; 0.70692; 0.70410\n",
      "2018-11-20 21:20:43,580 - Xworkers_1mill_0 - INFO -  21800;     8; 119981.4;   0.074; 0.69396; 0.09938; 0.70350; 0.70350; 0.70635; 0.70492; 0.70213\n",
      "2018-11-20 21:20:43,580 - Xworkers_1mill_0 - INFO -  21800;     8; 119981.4;   0.074; 0.69396; 0.09938; 0.70350; 0.70350; 0.70635; 0.70492; 0.70213\n",
      "2018-11-20 21:20:49,792 - Xworkers_1mill_0 - INFO -  21900;     8; 146457.3;   0.060; 0.71000; 0.09938; 0.69040; 0.69041; 0.69230; 0.69135; 0.68854\n",
      "2018-11-20 21:20:49,792 - Xworkers_1mill_0 - INFO -  21900;     8; 146457.3;   0.060; 0.71000; 0.09938; 0.69040; 0.69041; 0.69230; 0.69135; 0.68854\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint written to /home/ubuntu/summ_15ep_2wrk/checkpoint-21994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-20 21:20:57,068 - Xworkers_1mill_0 - INFO -  22000;     8; 149243.1;   0.059; 0.71931; 0.09938; 0.69537; 0.69536; 0.70042; 0.69788; 0.69314\n",
      "2018-11-20 21:20:57,068 - Xworkers_1mill_0 - INFO -  22000;     8; 149243.1;   0.059; 0.71931; 0.09938; 0.69537; 0.69536; 0.70042; 0.69788; 0.69314\n",
      "2018-11-20 21:21:03,323 - Xworkers_1mill_0 - INFO -  22100;     8; 140180.3;   0.063; 0.71171; 0.09938; 0.68271; 0.68271; 0.68448; 0.68359; 0.67948\n",
      "2018-11-20 21:21:03,323 - Xworkers_1mill_0 - INFO -  22100;     8; 140180.3;   0.063; 0.71171; 0.09938; 0.68271; 0.68271; 0.68448; 0.68359; 0.67948\n",
      "2018-11-20 21:21:09,505 - Xworkers_1mill_0 - INFO -  22200;     8; 146607.1;   0.060; 0.69998; 0.09937; 0.70508; 0.70508; 0.70818; 0.70663; 0.70428\n",
      "2018-11-20 21:21:09,505 - Xworkers_1mill_0 - INFO -  22200;     8; 146607.1;   0.060; 0.69998; 0.09937; 0.70508; 0.70508; 0.70818; 0.70663; 0.70428\n",
      "2018-11-20 21:21:15,722 - Xworkers_1mill_0 - INFO -  22300;     8; 143640.5;   0.062; 0.69304; 0.09937; 0.70599; 0.70599; 0.70743; 0.70671; 0.70446\n",
      "2018-11-20 21:21:15,722 - Xworkers_1mill_0 - INFO -  22300;     8; 143640.5;   0.062; 0.69304; 0.09937; 0.70599; 0.70599; 0.70743; 0.70671; 0.70446\n",
      "2018-11-20 21:21:21,924 - Xworkers_1mill_0 - INFO -  22400;     8; 139083.5;   0.064; 0.69608; 0.09937; 0.70395; 0.70396; 0.70520; 0.70458; 0.70158\n",
      "2018-11-20 21:21:21,924 - Xworkers_1mill_0 - INFO -  22400;     8; 139083.5;   0.064; 0.69608; 0.09937; 0.70395; 0.70396; 0.70520; 0.70458; 0.70158\n",
      "2018-11-20 21:21:28,088 - Xworkers_1mill_0 - INFO -  22500;     8; 146530.1;   0.060; 0.69143; 0.09936; 0.70486; 0.70486; 0.70721; 0.70603; 0.70165\n",
      "2018-11-20 21:21:28,088 - Xworkers_1mill_0 - INFO -  22500;     8; 146530.1;   0.060; 0.69143; 0.09936; 0.70486; 0.70486; 0.70721; 0.70603; 0.70165\n",
      "2018-11-20 21:21:34,291 - Xworkers_1mill_0 - INFO -  22600;     8; 146120.6;   0.061; 0.70864; 0.09936; 0.69627; 0.69628; 0.69638; 0.69633; 0.69356\n",
      "2018-11-20 21:21:34,291 - Xworkers_1mill_0 - INFO -  22600;     8; 146120.6;   0.061; 0.70864; 0.09936; 0.69627; 0.69628; 0.69638; 0.69633; 0.69356\n",
      "2018-11-20 21:21:40,441 - Xworkers_1mill_0 - INFO -  22700;     8; 145183.3;   0.061; 0.72564; 0.09936; 0.68768; 0.68769; 0.69163; 0.68965; 0.68584\n",
      "2018-11-20 21:21:40,441 - Xworkers_1mill_0 - INFO -  22700;     8; 145183.3;   0.061; 0.72564; 0.09936; 0.68768; 0.68769; 0.69163; 0.68965; 0.68584\n",
      "2018-11-20 21:21:46,641 - Xworkers_1mill_0 - INFO -  22800;     8; 142504.0;   0.062; 0.70684; 0.09936; 0.69763; 0.69763; 0.69744; 0.69753; 0.69422\n",
      "2018-11-20 21:21:46,641 - Xworkers_1mill_0 - INFO -  22800;     8; 142504.0;   0.062; 0.70684; 0.09936; 0.69763; 0.69763; 0.69744; 0.69753; 0.69422\n",
      "2018-11-20 21:21:52,866 - Xworkers_1mill_0 - INFO -  22900;     8; 140476.8;   0.063; 0.69261; 0.09935; 0.69898; 0.69899; 0.69991; 0.69945; 0.69742\n",
      "2018-11-20 21:21:52,866 - Xworkers_1mill_0 - INFO -  22900;     8; 140476.8;   0.063; 0.69261; 0.09935; 0.69898; 0.69899; 0.69991; 0.69945; 0.69742\n",
      "2018-11-20 21:21:59,060 - Xworkers_1mill_0 - INFO -  23000;     8; 146749.7;   0.060; 0.70098; 0.09935; 0.69785; 0.69786; 0.70004; 0.69895; 0.69534\n",
      "2018-11-20 21:21:59,060 - Xworkers_1mill_0 - INFO -  23000;     8; 146749.7;   0.060; 0.70098; 0.09935; 0.69785; 0.69786; 0.70004; 0.69895; 0.69534\n",
      "2018-11-20 21:22:05,231 - Xworkers_1mill_0 - INFO -  23100;     8; 142951.3;   0.062; 0.70585; 0.09935; 0.69944; 0.69943; 0.70292; 0.70117; 0.69710\n",
      "2018-11-20 21:22:05,231 - Xworkers_1mill_0 - INFO -  23100;     8; 142951.3;   0.062; 0.70585; 0.09935; 0.69944; 0.69943; 0.70292; 0.70117; 0.69710\n",
      "2018-11-20 21:22:11,513 - Xworkers_1mill_0 - INFO -  23200;     8; 145933.9;   0.061; 0.71697; 0.09935; 0.69040; 0.69039; 0.69227; 0.69133; 0.68656\n",
      "2018-11-20 21:22:11,513 - Xworkers_1mill_0 - INFO -  23200;     8; 145933.9;   0.061; 0.71697; 0.09935; 0.69040; 0.69039; 0.69227; 0.69133; 0.68656\n",
      "2018-11-20 21:22:17,792 - Xworkers_1mill_0 - INFO -  23300;     8; 139013.2;   0.064; 0.70397; 0.09934; 0.69605; 0.69606; 0.69548; 0.69577; 0.69329\n",
      "2018-11-20 21:22:17,792 - Xworkers_1mill_0 - INFO -  23300;     8; 139013.2;   0.064; 0.70397; 0.09934; 0.69605; 0.69606; 0.69548; 0.69577; 0.69329\n",
      "2018-11-20 21:22:24,034 - Xworkers_1mill_0 - INFO -  23400;     8; 138758.6;   0.064; 0.69401; 0.09934; 0.70373; 0.70373; 0.70547; 0.70460; 0.70225\n",
      "2018-11-20 21:22:24,034 - Xworkers_1mill_0 - INFO -  23400;     8; 138758.6;   0.064; 0.69401; 0.09934; 0.70373; 0.70373; 0.70547; 0.70460; 0.70225\n",
      "2018-11-20 21:22:30,280 - Xworkers_1mill_0 - INFO -  23500;     8; 147806.7;   0.060; 0.70459; 0.09934; 0.69740; 0.69740; 0.69831; 0.69786; 0.69477\n",
      "2018-11-20 21:22:30,280 - Xworkers_1mill_0 - INFO -  23500;     8; 147806.7;   0.060; 0.70459; 0.09934; 0.69740; 0.69740; 0.69831; 0.69786; 0.69477\n",
      "2018-11-20 21:22:36,523 - Xworkers_1mill_0 - INFO -  23600;     8; 147311.1;   0.060; 0.69437; 0.09933; 0.69266; 0.69266; 0.69403; 0.69335; 0.68849\n",
      "2018-11-20 21:22:36,523 - Xworkers_1mill_0 - INFO -  23600;     8; 147311.1;   0.060; 0.69437; 0.09933; 0.69266; 0.69266; 0.69403; 0.69335; 0.68849\n",
      "2018-11-20 21:22:42,740 - Xworkers_1mill_0 - INFO -  23700;     8; 146415.7;   0.060; 0.69549; 0.09933; 0.70893; 0.70892; 0.70950; 0.70921; 0.70715\n",
      "2018-11-20 21:22:42,740 - Xworkers_1mill_0 - INFO -  23700;     8; 146415.7;   0.060; 0.69549; 0.09933; 0.70893; 0.70892; 0.70950; 0.70921; 0.70715\n",
      "2018-11-20 21:22:48,932 - Xworkers_1mill_0 - INFO -  23800;     8; 146785.0;   0.060; 0.67793; 0.09933; 0.71232; 0.71232; 0.71258; 0.71245; 0.71036\n",
      "2018-11-20 21:22:48,932 - Xworkers_1mill_0 - INFO -  23800;     8; 146785.0;   0.060; 0.67793; 0.09933; 0.71232; 0.71232; 0.71258; 0.71245; 0.71036\n",
      "2018-11-20 21:22:55,133 - Xworkers_1mill_0 - INFO -  23900;     8; 146877.4;   0.060; 0.70489; 0.09933; 0.69966; 0.69966; 0.70084; 0.70025; 0.69595\n",
      "2018-11-20 21:22:55,133 - Xworkers_1mill_0 - INFO -  23900;     8; 146877.4;   0.060; 0.70489; 0.09933; 0.69966; 0.69966; 0.70084; 0.70025; 0.69595\n",
      "2018-11-20 21:23:01,349 - Xworkers_1mill_0 - INFO -  24000;     8; 141274.0;   0.063; 0.70894; 0.09932; 0.70011; 0.70012; 0.70126; 0.70069; 0.69781\n",
      "2018-11-20 21:23:01,349 - Xworkers_1mill_0 - INFO -  24000;     8; 141274.0;   0.063; 0.70894; 0.09932; 0.70011; 0.70012; 0.70126; 0.70069; 0.69781\n",
      "2018-11-20 21:23:07,510 - Xworkers_1mill_0 - INFO -  24100;     8; 137770.8;   0.064; 0.67267; 0.09932; 0.70644; 0.70644; 0.70819; 0.70731; 0.70434\n",
      "2018-11-20 21:23:07,510 - Xworkers_1mill_0 - INFO -  24100;     8; 137770.8;   0.064; 0.67267; 0.09932; 0.70644; 0.70644; 0.70819; 0.70731; 0.70434\n",
      "2018-11-20 21:23:56,148 - Xworkers_1mill_0 - INFO - valid - Number of batches: 1; batch_size: 1200000; Total Time: 0:00:42.937470\n",
      "2018-11-20 21:23:56,148 - Xworkers_1mill_0 - INFO - valid - Number of batches: 1; batch_size: 1200000; Total Time: 0:00:42.937470\n",
      "2018-11-20 21:23:56,151 - Xworkers_1mill_0 - INFO - ---Validation--- Training Step: 24192; Training Epoch: 8; \n",
      " Confusion Matrix:\n",
      " [[  7679    350    143    200   6009    148     31]\n",
      " [   404  12232   4838    181   2134     16      0]\n",
      " [     0   1035   5239    404      0      2      1]\n",
      " [     1      0   1571  12339      0    913    178]\n",
      " [133229  53621   3601    876 643948    154     21]\n",
      " [     0      2      4   1279      0   7895    780]\n",
      " [     0      0      0     30      1    127    252]]\n",
      "2018-11-20 21:23:56,151 - Xworkers_1mill_0 - INFO - ---Validation--- Training Step: 24192; Training Epoch: 8; \n",
      " Confusion Matrix:\n",
      " [[  7679    350    143    200   6009    148     31]\n",
      " [   404  12232   4838    181   2134     16      0]\n",
      " [     0   1035   5239    404      0      2      1]\n",
      " [     1      0   1571  12339      0    913    178]\n",
      " [133229  53621   3601    876 643948    154     21]\n",
      " [     0      2      4   1279      0   7895    780]\n",
      " [     0      0      0     30      1    127    252]]\n",
      "2018-11-20 21:23:56,155 - Xworkers_1mill_0 - INFO - (Training Step, Training Epoch, loss, logloss, accuracy, recall, precision, f1score_micro, f1score_macro) in Validation:  24192;     8; 0.64822; 0.17313; 0.76462; 0.70425; 0.48895; 0.57717; 0.52245\n",
      "2018-11-20 21:23:56,155 - Xworkers_1mill_0 - INFO - (Training Step, Training Epoch, loss, logloss, accuracy, recall, precision, f1score_micro, f1score_macro) in Validation:  24192;     8; 0.64822; 0.17313; 0.76462; 0.70425; 0.48895; 0.57717; 0.52245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-20 21:23:56,658 - Xworkers_1mill_0 - INFO -  24200;     9; 135562.0;   0.065; 0.71017; 0.09932; 0.70486; 0.70486; 0.70628; 0.70557; 0.70267\n",
      "2018-11-20 21:23:56,658 - Xworkers_1mill_0 - INFO -  24200;     9; 135562.0;   0.065; 0.71017; 0.09932; 0.70486; 0.70486; 0.70628; 0.70557; 0.70267\n",
      "2018-11-20 21:24:02,944 - Xworkers_1mill_0 - INFO -  24300;     9; 138879.6;   0.064; 0.71296; 0.09931; 0.69537; 0.69537; 0.69740; 0.69638; 0.69260\n",
      "2018-11-20 21:24:02,944 - Xworkers_1mill_0 - INFO -  24300;     9; 138879.6;   0.064; 0.71296; 0.09931; 0.69537; 0.69537; 0.69740; 0.69638; 0.69260\n",
      "2018-11-20 21:24:09,147 - Xworkers_1mill_0 - INFO -  24400;     9; 147663.8;   0.060; 0.70519; 0.09931; 0.70079; 0.70079; 0.70261; 0.70170; 0.69943\n",
      "2018-11-20 21:24:09,147 - Xworkers_1mill_0 - INFO -  24400;     9; 147663.8;   0.060; 0.70519; 0.09931; 0.70079; 0.70079; 0.70261; 0.70170; 0.69943\n",
      "2018-11-20 21:24:15,310 - Xworkers_1mill_0 - INFO -  24500;     9; 141532.5;   0.063; 0.67800; 0.09931; 0.71006; 0.71006; 0.71095; 0.71051; 0.70787\n",
      "2018-11-20 21:24:15,310 - Xworkers_1mill_0 - INFO -  24500;     9; 141532.5;   0.063; 0.67800; 0.09931; 0.71006; 0.71006; 0.71095; 0.71051; 0.70787\n",
      "2018-11-20 21:24:21,475 - Xworkers_1mill_0 - INFO -  24600;     9; 138278.4;   0.064; 0.67875; 0.09931; 0.70486; 0.70486; 0.70655; 0.70570; 0.70341\n",
      "2018-11-20 21:24:21,475 - Xworkers_1mill_0 - INFO -  24600;     9; 138278.4;   0.064; 0.67875; 0.09931; 0.70486; 0.70486; 0.70655; 0.70570; 0.70341\n",
      "2018-11-20 21:24:27,738 - Xworkers_1mill_0 - INFO -  24700;     9; 145191.2;   0.061; 0.69888; 0.09930; 0.69605; 0.69604; 0.70052; 0.69827; 0.69291\n",
      "2018-11-20 21:24:27,738 - Xworkers_1mill_0 - INFO -  24700;     9; 145191.2;   0.061; 0.69888; 0.09930; 0.69605; 0.69604; 0.70052; 0.69827; 0.69291\n",
      "2018-11-20 21:24:33,959 - Xworkers_1mill_0 - INFO -  24800;     9; 149489.5;   0.059; 0.69581; 0.09930; 0.70734; 0.70735; 0.70832; 0.70783; 0.70412\n",
      "2018-11-20 21:24:33,959 - Xworkers_1mill_0 - INFO -  24800;     9; 149489.5;   0.059; 0.69581; 0.09930; 0.70734; 0.70735; 0.70832; 0.70783; 0.70412\n",
      "2018-11-20 21:24:40,210 - Xworkers_1mill_0 - INFO -  24900;     9; 132527.8;   0.067; 0.69504; 0.09930; 0.70079; 0.70079; 0.70167; 0.70123; 0.69897\n",
      "2018-11-20 21:24:40,210 - Xworkers_1mill_0 - INFO -  24900;     9; 132527.8;   0.067; 0.69504; 0.09930; 0.70079; 0.70079; 0.70167; 0.70123; 0.69897\n",
      "2018-11-20 21:24:46,452 - Xworkers_1mill_0 - INFO -  25000;     9; 143470.7;   0.062; 0.71149; 0.09929; 0.69446; 0.69447; 0.69466; 0.69456; 0.69209\n",
      "2018-11-20 21:24:46,452 - Xworkers_1mill_0 - INFO -  25000;     9; 143470.7;   0.062; 0.71149; 0.09929; 0.69446; 0.69447; 0.69466; 0.69456; 0.69209\n",
      "2018-11-20 21:24:52,723 - Xworkers_1mill_0 - INFO -  25100;     9; 141897.2;   0.062; 0.71304; 0.09929; 0.69808; 0.69808; 0.70098; 0.69953; 0.69648\n",
      "2018-11-20 21:24:52,723 - Xworkers_1mill_0 - INFO -  25100;     9; 141897.2;   0.062; 0.71304; 0.09929; 0.69808; 0.69808; 0.70098; 0.69953; 0.69648\n",
      "2018-11-20 21:24:59,018 - Xworkers_1mill_0 - INFO -  25200;     9; 136907.5;   0.065; 0.70209; 0.09929; 0.69379; 0.69379; 0.69566; 0.69472; 0.69053\n",
      "2018-11-20 21:24:59,018 - Xworkers_1mill_0 - INFO -  25200;     9; 136907.5;   0.065; 0.70209; 0.09929; 0.69379; 0.69379; 0.69566; 0.69472; 0.69053\n",
      "2018-11-20 21:25:05,337 - Xworkers_1mill_0 - INFO -  25300;     9; 139997.3;   0.063; 0.72361; 0.09929; 0.69672; 0.69672; 0.70097; 0.69884; 0.69416\n",
      "2018-11-20 21:25:05,337 - Xworkers_1mill_0 - INFO -  25300;     9; 139997.3;   0.063; 0.72361; 0.09929; 0.69672; 0.69672; 0.70097; 0.69884; 0.69416\n",
      "2018-11-20 21:25:11,572 - Xworkers_1mill_0 - INFO -  25400;     9; 150663.6;   0.059; 0.68598; 0.09928; 0.69989; 0.69989; 0.70096; 0.70042; 0.69786\n",
      "2018-11-20 21:25:11,572 - Xworkers_1mill_0 - INFO -  25400;     9; 150663.6;   0.059; 0.68598; 0.09928; 0.69989; 0.69989; 0.70096; 0.70042; 0.69786\n",
      "2018-11-20 21:25:17,773 - Xworkers_1mill_0 - INFO -  25500;     9; 138948.2;   0.064; 0.70192; 0.09928; 0.69876; 0.69876; 0.70024; 0.69950; 0.69708\n",
      "2018-11-20 21:25:17,773 - Xworkers_1mill_0 - INFO -  25500;     9; 138948.2;   0.064; 0.70192; 0.09928; 0.69876; 0.69876; 0.70024; 0.69950; 0.69708\n",
      "2018-11-20 21:25:23,918 - Xworkers_1mill_0 - INFO -  25600;     9; 143475.1;   0.062; 0.68308; 0.09928; 0.71661; 0.71662; 0.71768; 0.71715; 0.71381\n",
      "2018-11-20 21:25:23,918 - Xworkers_1mill_0 - INFO -  25600;     9; 143475.1;   0.062; 0.68308; 0.09928; 0.71661; 0.71662; 0.71768; 0.71715; 0.71381\n",
      "2018-11-20 21:25:30,102 - Xworkers_1mill_0 - INFO -  25700;     9; 144394.9;   0.061; 0.68906; 0.09928; 0.70350; 0.70350; 0.70546; 0.70448; 0.70070\n",
      "2018-11-20 21:25:30,102 - Xworkers_1mill_0 - INFO -  25700;     9; 144394.9;   0.061; 0.68906; 0.09928; 0.70350; 0.70350; 0.70546; 0.70448; 0.70070\n",
      "2018-11-20 21:25:36,316 - Xworkers_1mill_0 - INFO -  25800;     9; 149776.6;   0.059; 0.70454; 0.09927; 0.69808; 0.69808; 0.69861; 0.69835; 0.69610\n",
      "2018-11-20 21:25:36,316 - Xworkers_1mill_0 - INFO -  25800;     9; 149776.6;   0.059; 0.70454; 0.09927; 0.69808; 0.69808; 0.69861; 0.69835; 0.69610\n",
      "2018-11-20 21:25:42,484 - Xworkers_1mill_0 - INFO -  25900;     9; 108362.5;   0.082; 0.68883; 0.09927; 0.70508; 0.70508; 0.70549; 0.70529; 0.70310\n",
      "2018-11-20 21:25:42,484 - Xworkers_1mill_0 - INFO -  25900;     9; 108362.5;   0.082; 0.68883; 0.09927; 0.70508; 0.70508; 0.70549; 0.70529; 0.70310\n",
      "2018-11-20 21:25:48,627 - Xworkers_1mill_0 - INFO -  26000;     9; 146042.4;   0.061; 0.71389; 0.09927; 0.69582; 0.69582; 0.69690; 0.69636; 0.69079\n",
      "2018-11-20 21:25:48,627 - Xworkers_1mill_0 - INFO -  26000;     9; 146042.4;   0.061; 0.71389; 0.09927; 0.69582; 0.69582; 0.69690; 0.69636; 0.69079\n",
      "2018-11-20 21:25:54,759 - Xworkers_1mill_0 - INFO -  26100;     9; 131868.7;   0.067; 0.70482; 0.09926; 0.69311; 0.69312; 0.69449; 0.69380; 0.69197\n",
      "2018-11-20 21:25:54,759 - Xworkers_1mill_0 - INFO -  26100;     9; 131868.7;   0.067; 0.70482; 0.09926; 0.69311; 0.69312; 0.69449; 0.69380; 0.69197\n",
      "2018-11-20 21:26:00,884 - Xworkers_1mill_0 - INFO -  26200;     9; 147983.5;   0.060; 0.69513; 0.09926; 0.70011; 0.70012; 0.70216; 0.70114; 0.69722\n",
      "2018-11-20 21:26:00,884 - Xworkers_1mill_0 - INFO -  26200;     9; 147983.5;   0.060; 0.69513; 0.09926; 0.70011; 0.70012; 0.70216; 0.70114; 0.69722\n",
      "2018-11-20 21:26:07,088 - Xworkers_1mill_0 - INFO -  26300;     9; 147623.9;   0.060; 0.69856; 0.09926; 0.69989; 0.69990; 0.70341; 0.70165; 0.69833\n",
      "2018-11-20 21:26:07,088 - Xworkers_1mill_0 - INFO -  26300;     9; 147623.9;   0.060; 0.69856; 0.09926; 0.69989; 0.69990; 0.70341; 0.70165; 0.69833\n",
      "2018-11-20 21:26:13,250 - Xworkers_1mill_0 - INFO -  26400;     9; 139684.9;   0.063; 0.69471; 0.09926; 0.70282; 0.70283; 0.70313; 0.70298; 0.70031\n",
      "2018-11-20 21:26:13,250 - Xworkers_1mill_0 - INFO -  26400;     9; 139684.9;   0.063; 0.69471; 0.09926; 0.70282; 0.70283; 0.70313; 0.70298; 0.70031\n",
      "2018-11-20 21:26:19,406 - Xworkers_1mill_0 - INFO -  26500;     9; 140043.3;   0.063; 0.69150; 0.09925; 0.70938; 0.70938; 0.71374; 0.71155; 0.70660\n",
      "2018-11-20 21:26:19,406 - Xworkers_1mill_0 - INFO -  26500;     9; 140043.3;   0.063; 0.69150; 0.09925; 0.70938; 0.70938; 0.71374; 0.71155; 0.70660\n",
      "2018-11-20 21:26:25,547 - Xworkers_1mill_0 - INFO -  26600;     9; 146864.0;   0.060; 0.71283; 0.09925; 0.70102; 0.70101; 0.70180; 0.70141; 0.69880\n",
      "2018-11-20 21:26:25,547 - Xworkers_1mill_0 - INFO -  26600;     9; 146864.0;   0.060; 0.71283; 0.09925; 0.70102; 0.70101; 0.70180; 0.70141; 0.69880\n",
      "2018-11-20 21:26:31,731 - Xworkers_1mill_0 - INFO -  26700;     9; 143477.3;   0.062; 0.68854; 0.09925; 0.70915; 0.70915; 0.71221; 0.71068; 0.70561\n",
      "2018-11-20 21:26:31,731 - Xworkers_1mill_0 - INFO -  26700;     9; 143477.3;   0.062; 0.68854; 0.09925; 0.70915; 0.70915; 0.71221; 0.71068; 0.70561\n",
      "2018-11-20 21:26:37,954 - Xworkers_1mill_0 - INFO -  26800;     9; 140464.0;   0.063; 0.68174; 0.09924; 0.70373; 0.70374; 0.70351; 0.70362; 0.70173\n",
      "2018-11-20 21:26:37,954 - Xworkers_1mill_0 - INFO -  26800;     9; 140464.0;   0.063; 0.68174; 0.09924; 0.70373; 0.70374; 0.70351; 0.70362; 0.70173\n",
      "2018-11-20 21:26:44,125 - Xworkers_1mill_0 - INFO -  26900;     9; 144277.6;   0.061; 0.69515; 0.09924; 0.70825; 0.70825; 0.71055; 0.70940; 0.70671\n",
      "2018-11-20 21:26:44,125 - Xworkers_1mill_0 - INFO -  26900;     9; 144277.6;   0.061; 0.69515; 0.09924; 0.70825; 0.70825; 0.71055; 0.70940; 0.70671\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-20 21:26:50,315 - Xworkers_1mill_0 - INFO -  27000;     9; 136957.5;   0.065; 0.67306; 0.09924; 0.71887; 0.71887; 0.72095; 0.71991; 0.71630\n",
      "2018-11-20 21:26:50,315 - Xworkers_1mill_0 - INFO -  27000;     9; 136957.5;   0.065; 0.67306; 0.09924; 0.71887; 0.71887; 0.72095; 0.71991; 0.71630\n",
      "2018-11-20 21:26:56,550 - Xworkers_1mill_0 - INFO -  27100;     9; 145507.7;   0.061; 0.67713; 0.09924; 0.71006; 0.71007; 0.71118; 0.71062; 0.70777\n",
      "2018-11-20 21:26:56,550 - Xworkers_1mill_0 - INFO -  27100;     9; 145507.7;   0.061; 0.67713; 0.09924; 0.71006; 0.71007; 0.71118; 0.71062; 0.70777\n",
      "2018-11-20 21:27:02,780 - Xworkers_1mill_0 - INFO -  27200;     9; 140755.4;   0.063; 0.70146; 0.09923; 0.69514; 0.69515; 0.69595; 0.69555; 0.69262\n",
      "2018-11-20 21:27:02,780 - Xworkers_1mill_0 - INFO -  27200;     9; 140755.4;   0.063; 0.70146; 0.09923; 0.69514; 0.69515; 0.69595; 0.69555; 0.69262\n",
      "2018-11-20 21:27:47,202 - Xworkers_1mill_0 - INFO - valid - Number of batches: 1; batch_size: 1200000; Total Time: 0:00:43.430977\n",
      "2018-11-20 21:27:47,202 - Xworkers_1mill_0 - INFO - valid - Number of batches: 1; batch_size: 1200000; Total Time: 0:00:43.430977\n",
      "2018-11-20 21:27:47,204 - Xworkers_1mill_0 - INFO - ---Validation--- Training Step: 27216; Training Epoch: 9; \n",
      " Confusion Matrix:\n",
      " [[  8405    325    149    202   5304    151     24]\n",
      " [   525  11978   5069    168   2049     16      0]\n",
      " [     0    976   5347    355      0      2      1]\n",
      " [     1      0   1748  12162      0    938    153]\n",
      " [182230  51320   3857    839 597029    158     17]\n",
      " [     0      2      9   1274      0   8044    631]\n",
      " [     0      0      0     32      1    139    238]]\n",
      "2018-11-20 21:27:47,204 - Xworkers_1mill_0 - INFO - ---Validation--- Training Step: 27216; Training Epoch: 9; \n",
      " Confusion Matrix:\n",
      " [[  8405    325    149    202   5304    151     24]\n",
      " [   525  11978   5069    168   2049     16      0]\n",
      " [     0    976   5347    355      0      2      1]\n",
      " [     1      0   1748  12162      0    938    153]\n",
      " [182230  51320   3857    839 597029    158     17]\n",
      " [     0      2      9   1274      0   8044    631]\n",
      " [     0      0      0     32      1    139    238]]\n",
      "2018-11-20 21:27:47,210 - Xworkers_1mill_0 - INFO - (Training Step, Training Epoch, loss, logloss, accuracy, recall, precision, f1score_micro, f1score_macro) in Validation:  27216;     9; 0.69288; 0.18530; 0.71319; 0.69940; 0.49027; 0.57645; 0.51777\n",
      "2018-11-20 21:27:47,210 - Xworkers_1mill_0 - INFO - (Training Step, Training Epoch, loss, logloss, accuracy, recall, precision, f1score_micro, f1score_macro) in Validation:  27216;     9; 0.69288; 0.18530; 0.71319; 0.69940; 0.49027; 0.57645; 0.51777\n",
      "2018-11-20 21:27:52,446 - Xworkers_1mill_0 - INFO -  27300;    10; 138932.6;   0.064; 0.68251; 0.09923; 0.71096; 0.71096; 0.71224; 0.71160; 0.70931\n",
      "2018-11-20 21:27:52,446 - Xworkers_1mill_0 - INFO -  27300;    10; 138932.6;   0.064; 0.68251; 0.09923; 0.71096; 0.71096; 0.71224; 0.71160; 0.70931\n",
      "2018-11-20 21:27:58,742 - Xworkers_1mill_0 - INFO -  27400;    10; 142600.9;   0.062; 0.68771; 0.09923; 0.70373; 0.70373; 0.70562; 0.70467; 0.70152\n",
      "2018-11-20 21:27:58,742 - Xworkers_1mill_0 - INFO -  27400;    10; 142600.9;   0.062; 0.68771; 0.09923; 0.70373; 0.70373; 0.70562; 0.70467; 0.70152\n",
      "2018-11-20 21:28:04,996 - Xworkers_1mill_0 - INFO -  27500;    10; 144231.6;   0.061; 0.69830; 0.09922; 0.70373; 0.70373; 0.70646; 0.70509; 0.70073\n",
      "2018-11-20 21:28:04,996 - Xworkers_1mill_0 - INFO -  27500;    10; 144231.6;   0.061; 0.69830; 0.09922; 0.70373; 0.70373; 0.70646; 0.70509; 0.70073\n",
      "2018-11-20 21:28:11,189 - Xworkers_1mill_0 - INFO -  27600;    10; 147051.4;   0.060; 0.69156; 0.09922; 0.71299; 0.71300; 0.71704; 0.71501; 0.71102\n",
      "2018-11-20 21:28:11,189 - Xworkers_1mill_0 - INFO -  27600;    10; 147051.4;   0.060; 0.69156; 0.09922; 0.71299; 0.71300; 0.71704; 0.71501; 0.71102\n",
      "2018-11-20 21:28:17,404 - Xworkers_1mill_0 - INFO -  27700;    10; 149842.5;   0.059; 0.69891; 0.09922; 0.69672; 0.69672; 0.69932; 0.69801; 0.69347\n",
      "2018-11-20 21:28:17,404 - Xworkers_1mill_0 - INFO -  27700;    10; 149842.5;   0.059; 0.69891; 0.09922; 0.69672; 0.69672; 0.69932; 0.69801; 0.69347\n",
      "2018-11-20 21:28:23,578 - Xworkers_1mill_0 - INFO -  27800;    10; 143062.6;   0.062; 0.70773; 0.09922; 0.69333; 0.69334; 0.69645; 0.69489; 0.69149\n",
      "2018-11-20 21:28:23,578 - Xworkers_1mill_0 - INFO -  27800;    10; 143062.6;   0.062; 0.70773; 0.09922; 0.69333; 0.69334; 0.69645; 0.69489; 0.69149\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint written to /home/ubuntu/summ_15ep_2wrk/checkpoint-27833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-20 21:28:30,832 - Xworkers_1mill_0 - INFO -  27900;    10; 143781.3;   0.062; 0.69757; 0.09921; 0.70260; 0.70260; 0.70408; 0.70334; 0.70144\n",
      "2018-11-20 21:28:30,832 - Xworkers_1mill_0 - INFO -  27900;    10; 143781.3;   0.062; 0.69757; 0.09921; 0.70260; 0.70260; 0.70408; 0.70334; 0.70144\n",
      "2018-11-20 21:28:36,965 - Xworkers_1mill_0 - INFO -  28000;    10; 137917.7;   0.064; 0.70109; 0.09921; 0.69446; 0.69447; 0.69510; 0.69478; 0.69271\n",
      "2018-11-20 21:28:36,965 - Xworkers_1mill_0 - INFO -  28000;    10; 137917.7;   0.064; 0.70109; 0.09921; 0.69446; 0.69447; 0.69510; 0.69478; 0.69271\n",
      "2018-11-20 21:28:43,246 - Xworkers_1mill_0 - INFO -  28100;    10; 141138.6;   0.063; 0.70429; 0.09921; 0.69107; 0.69107; 0.69169; 0.69138; 0.68906\n",
      "2018-11-20 21:28:43,246 - Xworkers_1mill_0 - INFO -  28100;    10; 141138.6;   0.063; 0.70429; 0.09921; 0.69107; 0.69107; 0.69169; 0.69138; 0.68906\n",
      "2018-11-20 21:28:49,526 - Xworkers_1mill_0 - INFO -  28200;    10; 135456.6;   0.065; 0.69073; 0.09921; 0.70282; 0.70283; 0.70596; 0.70439; 0.70051\n",
      "2018-11-20 21:28:49,526 - Xworkers_1mill_0 - INFO -  28200;    10; 135456.6;   0.065; 0.69073; 0.09921; 0.70282; 0.70283; 0.70596; 0.70439; 0.70051\n",
      "2018-11-20 21:28:55,812 - Xworkers_1mill_0 - INFO -  28300;    10; 130575.0;   0.068; 0.69227; 0.09920; 0.70147; 0.70147; 0.70337; 0.70242; 0.69957\n",
      "2018-11-20 21:28:55,812 - Xworkers_1mill_0 - INFO -  28300;    10; 130575.0;   0.068; 0.69227; 0.09920; 0.70147; 0.70147; 0.70337; 0.70242; 0.69957\n",
      "2018-11-20 21:29:02,034 - Xworkers_1mill_0 - INFO -  28400;    10; 135365.3;   0.065; 0.70500; 0.09920; 0.69966; 0.69967; 0.70108; 0.70037; 0.69633\n",
      "2018-11-20 21:29:02,034 - Xworkers_1mill_0 - INFO -  28400;    10; 135365.3;   0.065; 0.70500; 0.09920; 0.69966; 0.69967; 0.70108; 0.70037; 0.69633\n",
      "2018-11-20 21:29:08,276 - Xworkers_1mill_0 - INFO -  28500;    10; 115265.3;   0.077; 0.70251; 0.09920; 0.70011; 0.70011; 0.70254; 0.70132; 0.69852\n",
      "2018-11-20 21:29:08,276 - Xworkers_1mill_0 - INFO -  28500;    10; 115265.3;   0.077; 0.70251; 0.09920; 0.70011; 0.70011; 0.70254; 0.70132; 0.69852\n",
      "2018-11-20 21:29:14,488 - Xworkers_1mill_0 - INFO -  28600;    10; 147746.7;   0.060; 0.69275; 0.09919; 0.70757; 0.70757; 0.71033; 0.70895; 0.70435\n",
      "2018-11-20 21:29:14,488 - Xworkers_1mill_0 - INFO -  28600;    10; 147746.7;   0.060; 0.69275; 0.09919; 0.70757; 0.70757; 0.71033; 0.70895; 0.70435\n",
      "2018-11-20 21:29:20,673 - Xworkers_1mill_0 - INFO -  28700;    10; 150185.7;   0.059; 0.69673; 0.09919; 0.69288; 0.69289; 0.69334; 0.69312; 0.69042\n",
      "2018-11-20 21:29:20,673 - Xworkers_1mill_0 - INFO -  28700;    10; 150185.7;   0.059; 0.69673; 0.09919; 0.69288; 0.69289; 0.69334; 0.69312; 0.69042\n",
      "2018-11-20 21:29:26,857 - Xworkers_1mill_0 - INFO -  28800;    10; 138068.5;   0.064; 0.71856; 0.09919; 0.69288; 0.69288; 0.69502; 0.69395; 0.68995\n",
      "2018-11-20 21:29:26,857 - Xworkers_1mill_0 - INFO -  28800;    10; 138068.5;   0.064; 0.71856; 0.09919; 0.69288; 0.69288; 0.69502; 0.69395; 0.68995\n",
      "2018-11-20 21:29:33,097 - Xworkers_1mill_0 - INFO -  28900;    10; 141879.3;   0.062; 0.69546; 0.09919; 0.70056; 0.70057; 0.70093; 0.70075; 0.69891\n",
      "2018-11-20 21:29:33,097 - Xworkers_1mill_0 - INFO -  28900;    10; 141879.3;   0.062; 0.69546; 0.09919; 0.70056; 0.70057; 0.70093; 0.70075; 0.69891\n",
      "2018-11-20 21:29:39,317 - Xworkers_1mill_0 - INFO -  29000;    10; 144734.4;   0.061; 0.69040; 0.09918; 0.70215; 0.70214; 0.70382; 0.70298; 0.69997\n",
      "2018-11-20 21:29:39,317 - Xworkers_1mill_0 - INFO -  29000;    10; 144734.4;   0.061; 0.69040; 0.09918; 0.70215; 0.70214; 0.70382; 0.70298; 0.69997\n",
      "2018-11-20 21:29:45,508 - Xworkers_1mill_0 - INFO -  29100;    10; 136578.0;   0.065; 0.69735; 0.09918; 0.69966; 0.69967; 0.70018; 0.69993; 0.69805\n",
      "2018-11-20 21:29:45,508 - Xworkers_1mill_0 - INFO -  29100;    10; 136578.0;   0.065; 0.69735; 0.09918; 0.69966; 0.69967; 0.70018; 0.69993; 0.69805\n",
      "2018-11-20 21:29:51,762 - Xworkers_1mill_0 - INFO -  29200;    10; 141910.2;   0.062; 0.69080; 0.09918; 0.69356; 0.69356; 0.69649; 0.69502; 0.69006\n",
      "2018-11-20 21:29:51,762 - Xworkers_1mill_0 - INFO -  29200;    10; 141910.2;   0.062; 0.69080; 0.09918; 0.69356; 0.69356; 0.69649; 0.69502; 0.69006\n",
      "2018-11-20 21:29:57,924 - Xworkers_1mill_0 - INFO -  29300;    10; 139614.5;   0.063; 0.71088; 0.09917; 0.69085; 0.69086; 0.69306; 0.69196; 0.68953\n",
      "2018-11-20 21:29:57,924 - Xworkers_1mill_0 - INFO -  29300;    10; 139614.5;   0.063; 0.71088; 0.09917; 0.69085; 0.69086; 0.69306; 0.69196; 0.68953\n",
      "2018-11-20 21:30:04,179 - Xworkers_1mill_0 - INFO -  29400;    10; 144396.6;   0.061; 0.68817; 0.09917; 0.69492; 0.69492; 0.69505; 0.69499; 0.69246\n",
      "2018-11-20 21:30:04,179 - Xworkers_1mill_0 - INFO -  29400;    10; 144396.6;   0.061; 0.68817; 0.09917; 0.69492; 0.69492; 0.69505; 0.69499; 0.69246\n",
      "2018-11-20 21:30:10,408 - Xworkers_1mill_0 - INFO -  29500;    10; 148089.8;   0.060; 0.70311; 0.09917; 0.69989; 0.69990; 0.70335; 0.70162; 0.69804\n",
      "2018-11-20 21:30:10,408 - Xworkers_1mill_0 - INFO -  29500;    10; 148089.8;   0.060; 0.70311; 0.09917; 0.69989; 0.69990; 0.70335; 0.70162; 0.69804\n",
      "2018-11-20 21:30:16,640 - Xworkers_1mill_0 - INFO -  29600;    10; 144512.4;   0.061; 0.70277; 0.09917; 0.69718; 0.69717; 0.69790; 0.69754; 0.69330\n",
      "2018-11-20 21:30:16,640 - Xworkers_1mill_0 - INFO -  29600;    10; 144512.4;   0.061; 0.70277; 0.09917; 0.69718; 0.69717; 0.69790; 0.69754; 0.69330\n",
      "2018-11-20 21:30:22,802 - Xworkers_1mill_0 - INFO -  29700;    10; 128919.7;   0.069; 0.69194; 0.09916; 0.71073; 0.71074; 0.71089; 0.71081; 0.70702\n",
      "2018-11-20 21:30:22,802 - Xworkers_1mill_0 - INFO -  29700;    10; 128919.7;   0.069; 0.69194; 0.09916; 0.71073; 0.71074; 0.71089; 0.71081; 0.70702\n",
      "2018-11-20 21:30:28,917 - Xworkers_1mill_0 - INFO -  29800;    10; 142111.8;   0.062; 0.68685; 0.09916; 0.70802; 0.70803; 0.70948; 0.70875; 0.70705\n",
      "2018-11-20 21:30:28,917 - Xworkers_1mill_0 - INFO -  29800;    10; 142111.8;   0.062; 0.68685; 0.09916; 0.70802; 0.70803; 0.70948; 0.70875; 0.70705\n",
      "2018-11-20 21:30:35,027 - Xworkers_1mill_0 - INFO -  29900;    10; 151663.9;   0.058; 0.67610; 0.09916; 0.71571; 0.71572; 0.71575; 0.71573; 0.71358\n",
      "2018-11-20 21:30:35,027 - Xworkers_1mill_0 - INFO -  29900;    10; 151663.9;   0.058; 0.67610; 0.09916; 0.71571; 0.71572; 0.71575; 0.71573; 0.71358\n",
      "2018-11-20 21:30:41,195 - Xworkers_1mill_0 - INFO -  30000;    10; 145778.0;   0.061; 0.69906; 0.09915; 0.70260; 0.70260; 0.70551; 0.70405; 0.69931\n",
      "2018-11-20 21:30:41,195 - Xworkers_1mill_0 - INFO -  30000;    10; 145778.0;   0.061; 0.69906; 0.09915; 0.70260; 0.70260; 0.70551; 0.70405; 0.69931\n",
      "2018-11-20 21:30:47,362 - Xworkers_1mill_0 - INFO -  30100;    10; 139538.4;   0.063; 0.71606; 0.09915; 0.69311; 0.69311; 0.69512; 0.69411; 0.69019\n",
      "2018-11-20 21:30:47,362 - Xworkers_1mill_0 - INFO -  30100;    10; 139538.4;   0.063; 0.71606; 0.09915; 0.69311; 0.69311; 0.69512; 0.69411; 0.69019\n",
      "2018-11-20 21:30:53,511 - Xworkers_1mill_0 - INFO -  30200;    10; 147128.3;   0.060; 0.70628; 0.09915; 0.68520; 0.68521; 0.68710; 0.68615; 0.68188\n",
      "2018-11-20 21:30:53,511 - Xworkers_1mill_0 - INFO -  30200;    10; 147128.3;   0.060; 0.70628; 0.09915; 0.68520; 0.68521; 0.68710; 0.68615; 0.68188\n",
      "2018-11-20 21:31:39,402 - Xworkers_1mill_0 - INFO - valid - Number of batches: 1; batch_size: 1200000; Total Time: 0:00:43.434995\n",
      "2018-11-20 21:31:39,402 - Xworkers_1mill_0 - INFO - valid - Number of batches: 1; batch_size: 1200000; Total Time: 0:00:43.434995\n",
      "2018-11-20 21:31:39,404 - Xworkers_1mill_0 - INFO - ---Validation--- Training Step: 30240; Training Epoch: 10; \n",
      " Confusion Matrix:\n",
      " [[  8228    330    142    197   5483    132     48]\n",
      " [   488  12211   4825    171   2094     16      0]\n",
      " [     0   1054   5253    371      0      2      1]\n",
      " [     1      0   1672  12184      0    835    310]\n",
      " [168699  50483   3541    850 611706    143     28]\n",
      " [     0      3      5   1268      0   7232   1452]\n",
      " [     0      0      0     30      1     88    291]]\n",
      "2018-11-20 21:31:39,404 - Xworkers_1mill_0 - INFO - ---Validation--- Training Step: 30240; Training Epoch: 10; \n",
      " Confusion Matrix:\n",
      " [[  8228    330    142    197   5483    132     48]\n",
      " [   488  12211   4825    171   2094     16      0]\n",
      " [     0   1054   5253    371      0      2      1]\n",
      " [     1      0   1672  12184      0    835    310]\n",
      " [168699  50483   3541    850 611706    143     28]\n",
      " [     0      3      5   1268      0   7232   1452]\n",
      " [     0      0      0     30      1     88    291]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-20 21:31:39,409 - Xworkers_1mill_0 - INFO - (Training Step, Training Epoch, loss, logloss, accuracy, recall, precision, f1score_micro, f1score_macro) in Validation:  30240;    10; 0.67489; 0.18047; 0.72860; 0.70688; 0.48087; 0.57237; 0.50257\n",
      "2018-11-20 21:31:39,409 - Xworkers_1mill_0 - INFO - (Training Step, Training Epoch, loss, logloss, accuracy, recall, precision, f1score_micro, f1score_macro) in Validation:  30240;    10; 0.67489; 0.18047; 0.72860; 0.70688; 0.48087; 0.57237; 0.50257\n",
      "2018-11-20 21:31:43,086 - Xworkers_1mill_0 - INFO -  30300;    11; 146833.8;   0.060; 0.69774; 0.09915; 0.70418; 0.70418; 0.70575; 0.70496; 0.70202\n",
      "2018-11-20 21:31:43,086 - Xworkers_1mill_0 - INFO -  30300;    11; 146833.8;   0.060; 0.69774; 0.09915; 0.70418; 0.70418; 0.70575; 0.70496; 0.70202\n",
      "2018-11-20 21:31:49,190 - Xworkers_1mill_0 - INFO -  30400;    11; 137253.7;   0.064; 0.69080; 0.09914; 0.70373; 0.70374; 0.70618; 0.70496; 0.70053\n",
      "2018-11-20 21:31:49,190 - Xworkers_1mill_0 - INFO -  30400;    11; 137253.7;   0.064; 0.69080; 0.09914; 0.70373; 0.70374; 0.70618; 0.70496; 0.70053\n",
      "2018-11-20 21:31:55,254 - Xworkers_1mill_0 - INFO -  30500;    11; 139924.0;   0.063; 0.70324; 0.09914; 0.70328; 0.70328; 0.70526; 0.70427; 0.70187\n",
      "2018-11-20 21:31:55,254 - Xworkers_1mill_0 - INFO -  30500;    11; 139924.0;   0.063; 0.70324; 0.09914; 0.70328; 0.70328; 0.70526; 0.70427; 0.70187\n",
      "2018-11-20 21:32:01,397 - Xworkers_1mill_0 - INFO -  30600;    11; 141347.1;   0.063; 0.69539; 0.09914; 0.69898; 0.69899; 0.69981; 0.69940; 0.69562\n",
      "2018-11-20 21:32:01,397 - Xworkers_1mill_0 - INFO -  30600;    11; 141347.1;   0.063; 0.69539; 0.09914; 0.69898; 0.69899; 0.69981; 0.69940; 0.69562\n",
      "2018-11-20 21:32:07,553 - Xworkers_1mill_0 - INFO -  30700;    11; 147086.9;   0.060; 0.69276; 0.09914; 0.69989; 0.69989; 0.70184; 0.70086; 0.69752\n",
      "2018-11-20 21:32:07,553 - Xworkers_1mill_0 - INFO -  30700;    11; 147086.9;   0.060; 0.69276; 0.09914; 0.69989; 0.69989; 0.70184; 0.70086; 0.69752\n",
      "2018-11-20 21:32:13,649 - Xworkers_1mill_0 - INFO -  30800;    11; 147259.6;   0.060; 0.70856; 0.09913; 0.69627; 0.69628; 0.69644; 0.69636; 0.69234\n",
      "2018-11-20 21:32:13,649 - Xworkers_1mill_0 - INFO -  30800;    11; 147259.6;   0.060; 0.70856; 0.09913; 0.69627; 0.69628; 0.69644; 0.69636; 0.69234\n",
      "2018-11-20 21:32:19,724 - Xworkers_1mill_0 - INFO -  30900;    11; 143876.1;   0.062; 0.68796; 0.09913; 0.69537; 0.69537; 0.69727; 0.69632; 0.69177\n",
      "2018-11-20 21:32:19,724 - Xworkers_1mill_0 - INFO -  30900;    11; 143876.1;   0.062; 0.68796; 0.09913; 0.69537; 0.69537; 0.69727; 0.69632; 0.69177\n",
      "2018-11-20 21:32:25,857 - Xworkers_1mill_0 - INFO -  31000;    11; 150281.7;   0.059; 0.71083; 0.09913; 0.69627; 0.69627; 0.69728; 0.69677; 0.69313\n",
      "2018-11-20 21:32:25,857 - Xworkers_1mill_0 - INFO -  31000;    11; 150281.7;   0.059; 0.71083; 0.09913; 0.69627; 0.69627; 0.69728; 0.69677; 0.69313\n",
      "2018-11-20 21:32:31,946 - Xworkers_1mill_0 - INFO -  31100;    11; 145031.8;   0.061; 0.68998; 0.09912; 0.69853; 0.69854; 0.69939; 0.69896; 0.69613\n",
      "2018-11-20 21:32:31,946 - Xworkers_1mill_0 - INFO -  31100;    11; 145031.8;   0.061; 0.68998; 0.09912; 0.69853; 0.69854; 0.69939; 0.69896; 0.69613\n",
      "2018-11-20 21:32:38,096 - Xworkers_1mill_0 - INFO -  31200;    11; 145195.2;   0.061; 0.69689; 0.09912; 0.70712; 0.70713; 0.70753; 0.70733; 0.70496\n",
      "2018-11-20 21:32:38,096 - Xworkers_1mill_0 - INFO -  31200;    11; 145195.2;   0.061; 0.69689; 0.09912; 0.70712; 0.70713; 0.70753; 0.70733; 0.70496\n",
      "2018-11-20 21:32:44,312 - Xworkers_1mill_0 - INFO -  31300;    11; 145605.3;   0.061; 0.68525; 0.09912; 0.70260; 0.70260; 0.70511; 0.70386; 0.70080\n",
      "2018-11-20 21:32:44,312 - Xworkers_1mill_0 - INFO -  31300;    11; 145605.3;   0.061; 0.68525; 0.09912; 0.70260; 0.70260; 0.70511; 0.70386; 0.70080\n",
      "2018-11-20 21:32:50,502 - Xworkers_1mill_0 - INFO -  31400;    11; 144849.0;   0.061; 0.69310; 0.09912; 0.70621; 0.70622; 0.70860; 0.70741; 0.70275\n",
      "2018-11-20 21:32:50,502 - Xworkers_1mill_0 - INFO -  31400;    11; 144849.0;   0.061; 0.69310; 0.09912; 0.70621; 0.70622; 0.70860; 0.70741; 0.70275\n",
      "2018-11-20 21:32:56,624 - Xworkers_1mill_0 - INFO -  31500;    11; 143022.3;   0.062; 0.71906; 0.09911; 0.69831; 0.69830; 0.70004; 0.69917; 0.69593\n",
      "2018-11-20 21:32:56,624 - Xworkers_1mill_0 - INFO -  31500;    11; 143022.3;   0.062; 0.71906; 0.09911; 0.69831; 0.69830; 0.70004; 0.69917; 0.69593\n",
      "2018-11-20 21:33:02,737 - Xworkers_1mill_0 - INFO -  31600;    11; 146613.4;   0.060; 0.69333; 0.09911; 0.70463; 0.70464; 0.70591; 0.70527; 0.70229\n",
      "2018-11-20 21:33:02,737 - Xworkers_1mill_0 - INFO -  31600;    11; 146613.4;   0.060; 0.69333; 0.09911; 0.70463; 0.70464; 0.70591; 0.70527; 0.70229\n",
      "2018-11-20 21:33:08,951 - Xworkers_1mill_0 - INFO -  31700;    11; 145610.4;   0.061; 0.69181; 0.09911; 0.70079; 0.70080; 0.70073; 0.70077; 0.69856\n",
      "2018-11-20 21:33:08,951 - Xworkers_1mill_0 - INFO -  31700;    11; 145610.4;   0.061; 0.69181; 0.09911; 0.70079; 0.70080; 0.70073; 0.70077; 0.69856\n",
      "2018-11-20 21:33:15,194 - Xworkers_1mill_0 - INFO -  31800;    11; 143000.9;   0.062; 0.70021; 0.09910; 0.70079; 0.70080; 0.70097; 0.70088; 0.69968\n",
      "2018-11-20 21:33:15,194 - Xworkers_1mill_0 - INFO -  31800;    11; 143000.9;   0.062; 0.70021; 0.09910; 0.70079; 0.70080; 0.70097; 0.70088; 0.69968\n",
      "2018-11-20 21:33:21,403 - Xworkers_1mill_0 - INFO -  31900;    11; 135250.8;   0.065; 0.69524; 0.09910; 0.70147; 0.70147; 0.70217; 0.70182; 0.69819\n",
      "2018-11-20 21:33:21,403 - Xworkers_1mill_0 - INFO -  31900;    11; 135250.8;   0.065; 0.69524; 0.09910; 0.70147; 0.70147; 0.70217; 0.70182; 0.69819\n",
      "2018-11-20 21:33:27,480 - Xworkers_1mill_0 - INFO -  32000;    11; 142646.4;   0.062; 0.69272; 0.09910; 0.71209; 0.71210; 0.71358; 0.71284; 0.71002\n",
      "2018-11-20 21:33:27,480 - Xworkers_1mill_0 - INFO -  32000;    11; 142646.4;   0.062; 0.69272; 0.09910; 0.71209; 0.71210; 0.71358; 0.71284; 0.71002\n",
      "2018-11-20 21:33:33,597 - Xworkers_1mill_0 - INFO -  32100;    11; 139935.6;   0.063; 0.68402; 0.09910; 0.70712; 0.70712; 0.70771; 0.70742; 0.70445\n",
      "2018-11-20 21:33:33,597 - Xworkers_1mill_0 - INFO -  32100;    11; 139935.6;   0.063; 0.68402; 0.09910; 0.70712; 0.70712; 0.70771; 0.70742; 0.70445\n",
      "2018-11-20 21:33:39,765 - Xworkers_1mill_0 - INFO -  32200;    11; 143034.5;   0.062; 0.68124; 0.09909; 0.70825; 0.70826; 0.70911; 0.70869; 0.70679\n",
      "2018-11-20 21:33:39,765 - Xworkers_1mill_0 - INFO -  32200;    11; 143034.5;   0.062; 0.68124; 0.09909; 0.70825; 0.70826; 0.70911; 0.70869; 0.70679\n",
      "2018-11-20 21:33:45,931 - Xworkers_1mill_0 - INFO -  32300;    11; 145708.7;   0.061; 0.69431; 0.09909; 0.70960; 0.70961; 0.71029; 0.70995; 0.70687\n",
      "2018-11-20 21:33:45,931 - Xworkers_1mill_0 - INFO -  32300;    11; 145708.7;   0.061; 0.69431; 0.09909; 0.70960; 0.70961; 0.71029; 0.70995; 0.70687\n",
      "2018-11-20 21:33:52,038 - Xworkers_1mill_0 - INFO -  32400;    11; 148359.1;   0.060; 0.68681; 0.09909; 0.70034; 0.70035; 0.70292; 0.70163; 0.69758\n",
      "2018-11-20 21:33:52,038 - Xworkers_1mill_0 - INFO -  32400;    11; 148359.1;   0.060; 0.68681; 0.09909; 0.70034; 0.70035; 0.70292; 0.70163; 0.69758\n",
      "2018-11-20 21:33:58,111 - Xworkers_1mill_0 - INFO -  32500;    11; 136981.3;   0.065; 0.70345; 0.09909; 0.70554; 0.70553; 0.70692; 0.70623; 0.70332\n",
      "2018-11-20 21:33:58,111 - Xworkers_1mill_0 - INFO -  32500;    11; 136981.3;   0.065; 0.70345; 0.09909; 0.70554; 0.70553; 0.70692; 0.70623; 0.70332\n",
      "2018-11-20 21:34:04,182 - Xworkers_1mill_0 - INFO -  32600;    11; 148559.8;   0.060; 0.69832; 0.09908; 0.70124; 0.70125; 0.70252; 0.70188; 0.69831\n",
      "2018-11-20 21:34:04,182 - Xworkers_1mill_0 - INFO -  32600;    11; 148559.8;   0.060; 0.69832; 0.09908; 0.70124; 0.70125; 0.70252; 0.70188; 0.69831\n",
      "2018-11-20 21:34:10,236 - Xworkers_1mill_0 - INFO -  32700;    11; 147986.5;   0.060; 0.67922; 0.09908; 0.70734; 0.70735; 0.70751; 0.70743; 0.70432\n",
      "2018-11-20 21:34:10,236 - Xworkers_1mill_0 - INFO -  32700;    11; 147986.5;   0.060; 0.67922; 0.09908; 0.70734; 0.70735; 0.70751; 0.70743; 0.70432\n",
      "2018-11-20 21:34:16,276 - Xworkers_1mill_0 - INFO -  32800;    11; 148644.2;   0.060; 0.68114; 0.09908; 0.71525; 0.71525; 0.71774; 0.71649; 0.71306\n",
      "2018-11-20 21:34:16,276 - Xworkers_1mill_0 - INFO -  32800;    11; 148644.2;   0.060; 0.68114; 0.09908; 0.71525; 0.71525; 0.71774; 0.71649; 0.71306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-20 21:34:22,384 - Xworkers_1mill_0 - INFO -  32900;    11; 147268.4;   0.060; 0.69244; 0.09907; 0.70192; 0.70193; 0.70354; 0.70273; 0.70046\n",
      "2018-11-20 21:34:22,384 - Xworkers_1mill_0 - INFO -  32900;    11; 147268.4;   0.060; 0.69244; 0.09907; 0.70192; 0.70193; 0.70354; 0.70273; 0.70046\n",
      "2018-11-20 21:34:28,464 - Xworkers_1mill_0 - INFO -  33000;    11; 140132.6;   0.063; 0.68165; 0.09907; 0.70531; 0.70532; 0.70720; 0.70626; 0.70271\n",
      "2018-11-20 21:34:28,464 - Xworkers_1mill_0 - INFO -  33000;    11; 140132.6;   0.063; 0.68165; 0.09907; 0.70531; 0.70532; 0.70720; 0.70626; 0.70271\n",
      "2018-11-20 21:34:34,575 - Xworkers_1mill_0 - INFO -  33100;    11; 149097.4;   0.059; 0.69169; 0.09907; 0.70531; 0.70532; 0.70686; 0.70609; 0.70246\n",
      "2018-11-20 21:34:34,575 - Xworkers_1mill_0 - INFO -  33100;    11; 149097.4;   0.059; 0.69169; 0.09907; 0.70531; 0.70532; 0.70686; 0.70609; 0.70246\n",
      "2018-11-20 21:34:40,702 - Xworkers_1mill_0 - INFO -  33200;    11; 141547.1;   0.063; 0.69287; 0.09907; 0.70102; 0.70102; 0.70192; 0.70147; 0.69931\n",
      "2018-11-20 21:34:40,702 - Xworkers_1mill_0 - INFO -  33200;    11; 141547.1;   0.063; 0.69287; 0.09907; 0.70102; 0.70102; 0.70192; 0.70147; 0.69931\n"
     ]
    }
   ],
   "source": [
    "ops_to_run = [learning_rate_op, train_ops]\n",
    "ops_stats = [conf_mtx_op, accuracy_op, better_acc_op, precision_op, auc_list_op, auc_mean_op, m_list_op, m_list_mean_op, \n",
    "             lloss_op, auc_pr_op, auc_pr_mean_op, total_loss_op]\n",
    "                    \n",
    "oom = False\n",
    "step0 = int(sess.run(trainer.global_step))\n",
    "for step in range(step0, nstep):    \n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        epoch = step*FLAGS.batch_size // nrecord #*hvd.size()\n",
    "        batch_dict= create_feed_dict('batch', DATA, FLAGS)        \n",
    "        \n",
    "        if (hvd.rank() == 0 and summary_ops is not None and\n",
    "            (step == 0 or step+1 == nstep or\n",
    "             time.time() - last_summary_time > FLAGS.summary_interval)):\n",
    "            \n",
    "            if step != 0:\n",
    "                last_summary_time += FLAGS.summary_interval                        \n",
    "                \n",
    "            reset_and_update(sess, local_init, batch_dict)\n",
    "            summary, conf_mtx, accuracy, better_acc, precision, auc_list, auc_mean, m_list, m_list_mean, lloss, auc_pr, auc_pr_mean, loss, lr, _ = sess.run([summary_ops] + ops_stats + ops_to_run, feed_dict=batch_dict)                        \n",
    "            train_writer.add_summary(summary, step)            \n",
    "            train_writer.flush()\n",
    "            \n",
    "            if (math.isnan(better_acc)):\n",
    "                better_acc = calculate_better_acc(conf_mtx)        \n",
    "            if (math.isnan(precision)):\n",
    "                precision = calculate_better_acc(conf_mtx, axis=0)                         \n",
    "            f1score_micro = 2 * precision * better_acc / (precision + better_acc)\n",
    "            f1score_macro = calculate_macro_f1score(np.array(conf_mtx, dtype='float32'))\n",
    "            \n",
    "            elapsed = time.time() - start_time            \n",
    "            #this not necessarily matches with the display at console not even with validation set, due the summary_interval!\n",
    "            print_stats('---Training in Summary---', conf_mtx, accuracy, better_acc, precision, f1score_micro, f1score_macro, auc_list, auc_mean, m_list, m_list_mean, lloss, auc_pr, auc_pr_mean, loss)                         \n",
    "            df_train.loc[len(df_train)] = [step+1, epoch+1, elapsed, loss, lloss, accuracy, better_acc, precision, f1score_micro, f1score_macro, m_list_mean, auc_mean, auc_pr_mean]                                                    \n",
    "        else:\n",
    "            accuracy, conf_mtx, better_acc, precision, loss, lr, _ = sess.run([accuracy_op, conf_mtx_op, better_acc_op, precision_op, total_loss_op] + ops_to_run, feed_dict=batch_dict)\n",
    "            \n",
    "            if (math.isnan(better_acc)):\n",
    "                better_acc = calculate_better_acc(conf_mtx)        \n",
    "            if (math.isnan(precision)):\n",
    "                precision = calculate_better_acc(conf_mtx, axis=0)            \n",
    "            f1score_micro = 2 * precision * better_acc / (precision + better_acc)\n",
    "            f1score_macro = calculate_macro_f1score(np.array(conf_mtx, dtype='float32'))\n",
    "            \n",
    "            elapsed = time.time() - start_time\n",
    "            \n",
    "        if step == 0 or (step+1) % FLAGS.display_every == 0:                    \n",
    "            feature_per_sec = FLAGS.batch_size / elapsed                        \n",
    "            logger.info(\"%6i; %5i; %7.1f; %7.3f; %7.5f; %7.5f; %7.5f; %7.5f; %7.5f; %7.5f; %7.5f\" % (\n",
    "                step+1, epoch+1, feature_per_sec*hvd.size(), elapsed, loss, lr, accuracy, better_acc, precision, f1score_micro, f1score_macro))        \n",
    "\n",
    "        if (hvd.rank() == 0 and  ((step+1) % nstep_per_epoch == 0 or step+1 == nstep)):\n",
    "            #Running validation set:\n",
    "            valid_conf_mtx, valid_time, metrics = batching_dataset(sess, epoch, valid_writer, 'valid', DATA, FLAGS)\n",
    "            #valid_conf_mtx = np.array2string(valid_conf_mtx, formatter={'int_type':lambda x: \"int(%)\" % x})\n",
    "            valid_conf_mtx = np.array(valid_conf_mtx, dtype=int)\n",
    "            logger.info(\"---Validation--- Training Step: %d; Training Epoch: %d; \\n Confusion Matrix:\\n %s\" % (step+1, epoch+1, str(valid_conf_mtx)))            \n",
    "            df_valid.loc[len(df_valid)] = [step+1, epoch+1, valid_time, metrics[6], metrics[5], metrics[0], metrics[1], metrics[2], metrics[3], metrics[4]]            \n",
    "            logger.info(\"(Training Step, Training Epoch, loss, logloss, accuracy, recall, precision, f1score_micro, f1score_macro) in Validation: %6i; %5i; %7.5f; %7.5f; %7.5f; %7.5f; %7.5f; %7.5f; %7.5f\" % (\n",
    "                step+1, epoch+1, metrics[6], metrics[5], metrics[0], metrics[1], metrics[2], metrics[3], metrics[4]))    \n",
    "            sess.run(local_init)\n",
    "        \n",
    "        del batch_dict\n",
    "                    \n",
    "    except KeyboardInterrupt:\n",
    "        if hvd.rank() == 0:\n",
    "            df_train.to_csv(train_file, index=False, mode='a', sep =';', header=False)\n",
    "            df_valid.to_csv(valid_file, index=False, mode='a', sep =';', header=False)\n",
    "        logger.info(\"Keyboard interrupt\")\n",
    "        break\n",
    "    except tf.errors.ResourceExhaustedError:\n",
    "        elapsed = -1.\n",
    "        loss    = 0.\n",
    "        lr      = -1\n",
    "        if hvd.rank() == 0:\n",
    "            df_train.to_csv(train_file, index=False, mode='a', sep =';', header=False)\n",
    "            df_valid.to_csv(valid_file, index=False, mode='a', sep =';', header=False)\n",
    "        oom = True\n",
    "    \n",
    "    if (hvd.rank() == 0 and saver is not None and\n",
    "        (time.time() - last_save_time > FLAGS.save_interval or step+1 == nstep)):\n",
    "        last_save_time += FLAGS.save_interval\n",
    "        save_path = saver.save(sess, checkpoint_file, global_step=trainer.global_step)\n",
    "        print(\"Checkpoint written to\", save_path)\n",
    "    \n",
    "    if oom:\n",
    "        break\n",
    "        \n",
    "\n",
    "if hvd.rank() == 0:                               \n",
    "    df_train.to_csv(train_file, index=False, mode='a', sep =';', header=False)\n",
    "    df_valid.to_csv(valid_file, index=False, mode='a', sep =';', header=False)\n",
    "                               \n",
    "if train_writer is not None:\n",
    "    train_writer.close()\n",
    "\n",
    "if valid_writer is not None:\n",
    "    valid_writer.close()    \n",
    "    \n",
    "global_end_time = time.time()\n",
    "#logger.info(\"start time is {}, end time is {}\".format(global_start_time, global_end_time))\n",
    "logger.info('Time used in total: %.1f seconds' % (global_end_time - global_start_time))\n",
    "\n",
    "if oom:\n",
    "    print(\"Out of memory error detected, exiting\")\n",
    "    sys.exit(-2)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter nbconvert --to=python  nn_real_hvd-ntb-v7-rel.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!scp nn_real_hvd-ntb-v7-rel.py ubuntu@ec2-54-172-15-153.compute-1.amazonaws.com:/home/ubuntu/MLMortgage/notebooks/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!scp /home/ubuntu/MLMortgage/src/data/data_classes.py ubuntu@ec2-18-208-144-183.compute-1.amazonaws.com:/home/ubuntu/MLMortgage/src/data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mpirun -np 9 -H ec2-18-208-144-183.compute-1.amazonaws.com,ec2-54-163-27-28.compute-1.amazonaws.com,ec2-54-89-216-7.compute-1.amazonaws.com,ec2-18-234-208-171.compute-1.amazonaws.com,ec2-18-233-102-157.compute-1.amazonaws.com,ec2-34-238-152-99.compute-1.amazonaws.com,ec2-52-87-79-207.compute-1.amazonaws.com,ec2-54-165-210-130.compute-1.amazonaws.com,ec2-54-243-9-240.compute-1.amazonaws.com --mca plm_rsh_no_tree_spawn 1 --prefix /usr/local/mpi --bind-to none --map-by slot -x NCCL_DEBUG=INFO -x NCCL_MIN_NRINGS=2 -x LD_LIBRARY_PATH -x PATH  -mca pml ob1 -mca btl ^openib python nn_real_hvd-ntb-v7-rel.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow_p36]",
   "language": "python",
   "name": "conda-env-tensorflow_p36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
