{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Changelog:\n",
    "1.2\n",
    "  - Add logging to file and console\n",
    "1.1\n",
    "  - Center crop evaluation images\n",
    "  - Enable LARC learning rate control\n",
    "  - Correctly order UPDATE_OPS and global_step update during training.\n",
    "  - Set default learning rate policy to polynomial decay.\n",
    "  - Add cmd line options for checkpoint and summary intervals.\n",
    "  - Add loss scaling.\n",
    "  - Scale resnet learning rate by batch size.\n",
    "1.0\n",
    "  - Initial version\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import print_function\n",
    "from builtins import range\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import data_flow_ops\n",
    "from tensorflow.python.ops import init_ops\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import logging\n",
    "from collections import defaultdict\n",
    "import argparse\n",
    "\n",
    "try:\n",
    "    import horovod.tensorflow as hvd\n",
    "except:\n",
    "    print(\"Failed to import horovod module. \"\n",
    "          \"%s is intended for use with Uber's Horovod distributed training \"\n",
    "          \"framework. To create a Docker image with Horovod support see \"\n",
    "          \"docker-examples/Dockerfile.horovod.\" % __file__)\n",
    "    raise\n",
    "\n",
    "__version__ = \"1.0\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensorflow_version_tuple():\n",
    "    v = tf.__version__\n",
    "    major, minor, patch = v.split('.')\n",
    "    return (int(major), int(minor), patch)\n",
    "\n",
    "hvd.init()\n",
    "\n",
    "def print_r0(*args, **kwargs):\n",
    "    if hvd.rank() == 0:\n",
    "        print(*args, **kwargs)\n",
    "\n",
    "def float32_variable_storage_getter(getter, name, shape=None, dtype=None,\n",
    "                                    initializer=None, regularizer=None,\n",
    "                                    trainable=True,\n",
    "                                    *args, **kwargs):\n",
    "    storage_dtype = tf.float32 if trainable else dtype\n",
    "    variable = getter(name, shape, dtype=storage_dtype,\n",
    "                      initializer=initializer, regularizer=regularizer,\n",
    "                      trainable=trainable,\n",
    "                      *args, **kwargs)\n",
    "    if trainable and dtype != tf.float32:\n",
    "        variable = tf.cast(variable, dtype)\n",
    "    return variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPUNetworkBuilder(object):\n",
    "    \"\"\"This class provides convenient methods for constructing feed-forward\n",
    "    networks with internal data layout of 'NCHW'.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 is_training,\n",
    "                 dtype=tf.float32,\n",
    "                 activation='RELU',\n",
    "                 use_batch_norm=True,\n",
    "                 batch_norm_config = {'decay':   0.9,\n",
    "                                      'epsilon': 1e-4,\n",
    "                                      'scale':   True,\n",
    "                                      'zero_debias_moving_mean': False}):\n",
    "        self.dtype             = dtype\n",
    "        self.activation_func   = activation\n",
    "        self.is_training       = is_training\n",
    "        self.use_batch_norm    = use_batch_norm\n",
    "        self.batch_norm_config = batch_norm_config\n",
    "        self._layer_counts     = defaultdict(lambda: 0)\n",
    "    def _count_layer(self, layer_type):\n",
    "        idx  = self._layer_counts[layer_type]\n",
    "        name = layer_type + str(idx)\n",
    "        self._layer_counts[layer_type] += 1\n",
    "        return name\n",
    "    def _get_variable(self, name, shape, dtype=None,\n",
    "                      initializer=None, seed=None):\n",
    "        if dtype is None:\n",
    "            dtype = self.dtype\n",
    "        if initializer is None:\n",
    "            initializer = init_ops.glorot_uniform_initializer(seed=seed)\n",
    "        elif (isinstance(initializer, float) or\n",
    "              isinstance(initializer, int)):\n",
    "            initializer = tf.constant_initializer(float(initializer))\n",
    "        return tf.get_variable(name, shape, dtype, initializer)\n",
    "    def _to_nhwc(self, x):\n",
    "        return tf.transpose(x, [0,2,3,1])\n",
    "    def _from_nhwc(self, x):\n",
    "        return tf.transpose(x, [0,3,1,2])\n",
    "    def _bias(self, input_layer):\n",
    "        num_outputs = input_layer.get_shape().as_list()[1]\n",
    "        biases = self._get_variable('biases', [num_outputs], input_layer.dtype,\n",
    "                                    initializer=0)\n",
    "        if len(input_layer.get_shape()) == 4:\n",
    "            return tf.nn.bias_add(input_layer, biases,\n",
    "                                  data_format='NCHW')\n",
    "        else:\n",
    "            return input_layer + biases\n",
    "    def _batch_norm(self, input_layer, scope):\n",
    "        return tf.contrib.layers.batch_norm(input_layer,\n",
    "                                            is_training=self.is_training,\n",
    "                                            scope=scope,\n",
    "                                            data_format='NCHW',\n",
    "                                            fused=True,\n",
    "                                            **self.batch_norm_config)\n",
    "    def _bias_or_batch_norm(self, input_layer, scope, use_batch_norm):\n",
    "        if use_batch_norm is None:\n",
    "            use_batch_norm = self.use_batch_norm\n",
    "        if use_batch_norm:\n",
    "            return self._batch_norm(input_layer, scope)\n",
    "        else:\n",
    "            return self._bias(input_layer)\n",
    "    def input_layer(self, input_layer):\n",
    "        \"\"\"Converts input data into the internal format\"\"\"\n",
    "        x = self._from_nhwc(input_layer)\n",
    "        x = tf.cast(x, self.dtype)\n",
    "        # Rescale and shift to [-1,1]\n",
    "        x = x * (1./127.5) - 1\n",
    "        return x\n",
    "    def conv(self, input_layer, num_filters, filter_size,\n",
    "             filter_strides=(1,1), padding='SAME',\n",
    "             activation=None, use_batch_norm=None):\n",
    "        \"\"\"Applies a 2D convolution layer that includes bias or batch-norm\n",
    "        and an activation function.\n",
    "        \"\"\"\n",
    "        num_inputs = input_layer.get_shape().as_list()[1]\n",
    "        kernel_shape = [filter_size[0], filter_size[1],\n",
    "                        num_inputs, num_filters]\n",
    "        strides = [1, 1, filter_strides[0], filter_strides[1]]\n",
    "        with tf.variable_scope(self._count_layer('conv')) as scope:\n",
    "            kernel = self._get_variable('weights', kernel_shape,\n",
    "                                        input_layer.dtype)\n",
    "            if padding == 'SAME_RESNET': # ResNet models require custom padding\n",
    "                kh, kw = filter_size\n",
    "                rate = 1\n",
    "                kernel_size_effective = kh + (kw - 1) * (rate - 1)\n",
    "                pad_total = kernel_size_effective - 1\n",
    "                pad_beg = pad_total // 2\n",
    "                pad_end = pad_total - pad_beg\n",
    "                padding = [[0, 0], [0, 0],\n",
    "                           [pad_beg, pad_end], [pad_beg, pad_end]]\n",
    "                input_layer = tf.pad(input_layer, padding)\n",
    "                padding = 'VALID'\n",
    "            x = tf.nn.conv2d(input_layer, kernel, strides,\n",
    "                             padding=padding, data_format='NCHW')\n",
    "            x = self._bias_or_batch_norm(x, scope, use_batch_norm)\n",
    "            x = self.activate(x, activation)\n",
    "            return x\n",
    "    def deconv(self, input_layer, num_filters, filter_size,\n",
    "               filter_strides=(2,2), padding='SAME',\n",
    "               activation=None, use_batch_norm=None):\n",
    "        \"\"\"Applies a 'transposed convolution' layer that includes bias or\n",
    "        batch-norm and an activation function.\n",
    "        \"\"\"\n",
    "        num_inputs  = input_layer.get_shape().as_list()[1]\n",
    "        ih, iw      = input_layer.get_shape().as_list()[2:]\n",
    "        output_shape = [-1, num_filters,\n",
    "                        ih*filter_strides[0], iw*filter_strides[1]]\n",
    "        kernel_shape = [filter_size[0], filter_size[1],\n",
    "                        num_filters, num_inputs]\n",
    "        strides = [1, 1, filter_strides[0], filter_strides[1]]\n",
    "        with tf.variable_scope(self._count_layer('deconv')) as scope:\n",
    "            kernel = self._get_variable('weights', kernel_shape,\n",
    "                                        input_layer.dtype)\n",
    "            x = tf.nn.conv2d_transpose(input_layer, kernel, output_shape,\n",
    "                                       strides, padding=padding,\n",
    "                                       data_format='NCHW')\n",
    "            x = self._bias_or_batch_norm(x, scope, use_batch_norm)\n",
    "            x = self.activate(x, activation)\n",
    "            return x\n",
    "    def activate(self, input_layer, funcname=None):\n",
    "        \"\"\"Applies an activation function\"\"\"\n",
    "        if isinstance(funcname, tuple):\n",
    "            funcname = funcname[0]\n",
    "            params = funcname[1:]\n",
    "        if funcname is None:\n",
    "            funcname = self.activation_func\n",
    "        if funcname == 'LINEAR':\n",
    "            return input_layer\n",
    "        activation_map = {\n",
    "            'RELU':    tf.nn.relu,\n",
    "            'RELU6':   tf.nn.relu6,\n",
    "            'ELU':     tf.nn.elu,\n",
    "            'SIGMOID': tf.nn.sigmoid,\n",
    "            'TANH':    tf.nn.tanh,\n",
    "            'LRELU':   lambda x, name: tf.maximum(params[0]*x, x, name=name)\n",
    "        }\n",
    "        return activation_map[funcname](input_layer, name=funcname.lower())\n",
    "    def pool(self, input_layer, funcname, window_size,\n",
    "                 window_strides=(2,2),\n",
    "                 padding='VALID'):\n",
    "        \"\"\"Applies spatial pooling\"\"\"\n",
    "        pool_map = {\n",
    "            'MAX': tf.nn.max_pool,\n",
    "            'AVG': tf.nn.avg_pool\n",
    "        }\n",
    "        kernel_size    = [1, 1, window_size[0], window_size[1]]\n",
    "        kernel_strides = [1, 1, window_strides[0], window_strides[1]]\n",
    "        return pool_map[funcname](input_layer, kernel_size, kernel_strides,\n",
    "                                  padding, data_format='NCHW',\n",
    "                                  name=funcname.lower())\n",
    "    def project(self, input_layer, num_outputs, height, width,\n",
    "                activation=None):\n",
    "        \"\"\"Linearly projects to an image-like tensor\"\"\"\n",
    "        with tf.variable_scope(self._count_layer('project')):\n",
    "            x = self.fully_connected(input_layer, num_outputs*height*width,\n",
    "                                     activation=activation)\n",
    "            x = tf.reshape(x, [-1, num_outputs, height, width])\n",
    "            return x\n",
    "    def flatten(self, input_layer):\n",
    "        \"\"\"Flattens the spatial and channel dims into a single dim (4D->2D)\"\"\"\n",
    "        # Note: This ensures the output order matches that of NHWC networks\n",
    "        input_layer = self._to_nhwc(input_layer)\n",
    "        input_shape = input_layer.get_shape().as_list()\n",
    "        num_inputs  = input_shape[1]*input_shape[2]*input_shape[3]\n",
    "        return tf.reshape(input_layer, [-1, num_inputs], name='flatten')\n",
    "    def spatial_avg(self, input_layer):\n",
    "        \"\"\"Averages over spatial dimensions (4D->2D)\"\"\"\n",
    "        return tf.reduce_mean(input_layer, [2, 3], name='spatial_avg')\n",
    "    def fully_connected(self, input_layer, num_outputs, activation=None):\n",
    "        \"\"\"Applies a fully-connected set of weights\"\"\"\n",
    "        num_inputs = input_layer.get_shape().as_list()[1]\n",
    "        kernel_size = [num_inputs, num_outputs]\n",
    "        with tf.variable_scope(self._count_layer('fully_connected')):\n",
    "            kernel = self._get_variable('weights', kernel_size,\n",
    "                                        input_layer.dtype)\n",
    "            x = tf.matmul(input_layer, kernel)\n",
    "            x = self._bias(x)\n",
    "            x = self.activate(x, activation)\n",
    "            return x\n",
    "    def inception_module(self, input_layer, name, cols):\n",
    "        \"\"\"Applies an inception module with a given form\"\"\"\n",
    "        with tf.name_scope(name):\n",
    "            col_layers      = []\n",
    "            col_layer_sizes = []\n",
    "            for c, col in enumerate(cols):\n",
    "                col_layers.append([])\n",
    "                col_layer_sizes.append([])\n",
    "                x = input_layer\n",
    "                for l, layer in enumerate(col):\n",
    "                    ltype, args = layer[0], layer[1:]\n",
    "                    if   ltype == 'conv': x = self.conv(x, *args)\n",
    "                    elif ltype == 'pool': x = self.pool(x, *args)\n",
    "                    elif ltype == 'share':\n",
    "                        # Share matching layer from previous column\n",
    "                        x = col_layers[c-1][l]\n",
    "                    else: raise KeyError(\"Invalid layer type for \" +\n",
    "                                         \"inception module: '%s'\" % ltype)\n",
    "                    col_layers[c].append(x)\n",
    "            catdim  = 1\n",
    "            catvals = [layers[-1] for layers in col_layers]\n",
    "            x = tf.concat(catvals, catdim)\n",
    "            return x\n",
    "    def residual(self, input_layer, net, scale=1.0, activation='RELU'):\n",
    "        \"\"\"Applies a residual layer\"\"\"\n",
    "        input_size     = input_layer.get_shape().as_list()\n",
    "        num_inputs     = input_size[1]\n",
    "        output_layer   = scale*net(self, input_layer)\n",
    "        output_size    = output_layer.get_shape().as_list()\n",
    "        num_outputs    = output_size[1]\n",
    "        kernel_strides = (input_size[2]//output_size[2],\n",
    "                          input_size[3]//output_size[3])\n",
    "        with tf.name_scope('residual'):\n",
    "            if (num_outputs != num_inputs or\n",
    "                kernel_strides[0] != 1 or\n",
    "                kernel_strides[1] != 1):\n",
    "                input_layer = self.conv(input_layer, num_outputs, [1, 1],\n",
    "                                        kernel_strides, activation='LINEAR')\n",
    "            x = self.activate(input_layer + output_layer, activation)\n",
    "            return x\n",
    "    def dropout(self, input_layer, keep_prob=0.5):\n",
    "        \"\"\"Applies a dropout layer if is_training\"\"\"\n",
    "        if self.is_training:\n",
    "            dtype = input_layer.dtype\n",
    "            with tf.variable_scope(self._count_layer('dropout')):\n",
    "                keep_prob_tensor = tf.constant(keep_prob, dtype=dtype)\n",
    "                return tf.nn.dropout(input_layer, keep_prob_tensor)\n",
    "        else:\n",
    "            return input_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deserialize_image_record(record):\n",
    "    feature_map = {\n",
    "        'image/encoded':          tf.FixedLenFeature([ ], tf.string, ''),\n",
    "        'image/class/label':      tf.FixedLenFeature([1], tf.int64,  -1),\n",
    "        'image/class/text':       tf.FixedLenFeature([ ], tf.string, ''),\n",
    "        'image/object/bbox/xmin': tf.VarLenFeature(dtype=tf.float32),\n",
    "        'image/object/bbox/ymin': tf.VarLenFeature(dtype=tf.float32),\n",
    "        'image/object/bbox/xmax': tf.VarLenFeature(dtype=tf.float32),\n",
    "        'image/object/bbox/ymax': tf.VarLenFeature(dtype=tf.float32)\n",
    "    }\n",
    "    with tf.name_scope('deserialize_image_record'):\n",
    "        obj = tf.parse_single_example(record, feature_map)\n",
    "        imgdata = obj['image/encoded']\n",
    "        label   = tf.cast(obj['image/class/label'], tf.int32)\n",
    "        bbox    = tf.stack([obj['image/object/bbox/%s'%x].values\n",
    "                            for x in ['ymin', 'xmin', 'ymax', 'xmax']])\n",
    "        bbox = tf.transpose(tf.expand_dims(bbox, 0), [0,2,1])\n",
    "        text    = obj['image/class/text']\n",
    "        return imgdata, label, bbox, text\n",
    "\n",
    "def decode_jpeg(imgdata, channels=3):\n",
    "    return tf.image.decode_jpeg(imgdata, channels=channels,\n",
    "                                fancy_upscaling=False,\n",
    "                                dct_method='INTEGER_FAST')\n",
    "\n",
    "def decode_png(imgdata, channels=3):\n",
    "    return tf.image.decode_png(imgdata, channels=channels)\n",
    "\n",
    "def random_crop_and_resize_image(image, bbox, height, width):\n",
    "    with tf.name_scope('random_crop_and_resize'):\n",
    "        if FLAGS.eval:\n",
    "            image = tf.image.central_crop(image, 224./256.)\n",
    "        else:\n",
    "            bbox_begin, bbox_size, distorted_bbox = tf.image.sample_distorted_bounding_box(\n",
    "                tf.shape(image),\n",
    "                bounding_boxes=bbox,\n",
    "                min_object_covered=0.1,\n",
    "                aspect_ratio_range=[0.8, 1.25],\n",
    "                area_range=[0.1, 1.0],\n",
    "                max_attempts=100,\n",
    "                use_image_if_no_bounding_boxes=True)\n",
    "            # Crop the image to the distorted bounding box\n",
    "            image = tf.slice(image, bbox_begin, bbox_size)\n",
    "        # Resize to the desired output size\n",
    "        image = tf.image.resize_images(\n",
    "            image,\n",
    "            [height, width],\n",
    "            tf.image.ResizeMethod.BILINEAR,\n",
    "            align_corners=False)\n",
    "        image.set_shape([height, width, 3])\n",
    "        return image\n",
    "\n",
    "def distort_image_color(image, order):\n",
    "    with tf.name_scope('distort_color'):\n",
    "        image /= 255.\n",
    "        brightness = lambda img: tf.image.random_brightness(img, max_delta=32. / 255.)\n",
    "        saturation = lambda img: tf.image.random_saturation(img, lower=0.5, upper=1.5)\n",
    "        hue        = lambda img: tf.image.random_hue(img, max_delta=0.2)\n",
    "        contrast   = lambda img: tf.image.random_contrast(img, lower=0.5, upper=1.5)\n",
    "        if order == 0: ops = [brightness, saturation, hue, contrast]\n",
    "        else:          ops = [brightness, contrast, saturation, hue]\n",
    "        for op in ops:\n",
    "            image = op(image)\n",
    "        # The random_* ops do not necessarily clamp the output range\n",
    "        image = tf.clip_by_value(image, 0.0, 1.0)\n",
    "        # Restore the original scaling\n",
    "        image *= 255\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyPreprocessor(object):\n",
    "    def __init__(self, height, width, batch, nclass):\n",
    "        self.height = height\n",
    "        self.width  = width\n",
    "        self.batch = batch\n",
    "        self.nclass = nclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImagePreprocessor(object):\n",
    "    def __init__(self, height, width, subset='train', dtype=tf.uint8):\n",
    "        self.height = height\n",
    "        self.width  = width\n",
    "        self.subset = subset\n",
    "        self.dtype = dtype\n",
    "        self.nsummary = 10 # Max no. images to generate summaries for\n",
    "    def preprocess(self, imgdata, bbox, thread_id):\n",
    "        with tf.name_scope('preprocess_image'):\n",
    "            try:\n",
    "                image = decode_jpeg(imgdata)\n",
    "            except:\n",
    "                image = decode_png(imgdata)\n",
    "            if thread_id < self.nsummary:\n",
    "                image_with_bbox = tf.image.draw_bounding_boxes(\n",
    "                    tf.expand_dims(tf.to_float(image), 0), bbox)\n",
    "                tf.summary.image('original_image_and_bbox', image_with_bbox)\n",
    "            image = random_crop_and_resize_image(image, bbox,\n",
    "                                                 self.height, self.width)\n",
    "            if thread_id < self.nsummary:\n",
    "                tf.summary.image('cropped_resized_image',\n",
    "                                 tf.expand_dims(image, 0))\n",
    "            if not FLAGS.eval:\n",
    "                image = tf.image.random_flip_left_right(image)\n",
    "            if thread_id < self.nsummary:\n",
    "                tf.summary.image('flipped_image',\n",
    "                                 tf.expand_dims(image, 0))\n",
    "            if FLAGS.distort_color and not FLAGS.eval:\n",
    "                image = distort_image_color(image, order=thread_id%2)\n",
    "                if thread_id < self.nsummary:\n",
    "                    tf.summary.image('distorted_color_image',\n",
    "                                     tf.expand_dims(image, 0))\n",
    "        return image\n",
    "    def minibatch(self, batch_size):\n",
    "        record_input = data_flow_ops.RecordInput(\n",
    "            file_pattern=os.path.join(FLAGS.data_dir, '%s-*' % self.subset),\n",
    "            parallelism=64,\n",
    "            seed=301+hvd.rank(),\n",
    "            # Note: This causes deadlock during init if larger than dataset\n",
    "            buffer_size=FLAGS.input_buffer_size,\n",
    "            batch_size=batch_size)\n",
    "        records = record_input.get_yield_op()\n",
    "        # Split batch into individual images\n",
    "        records = tf.split(records, batch_size, 0)\n",
    "        records = [tf.reshape(record, []) for record in records]\n",
    "        # Deserialize and preprocess images into batches for each device\n",
    "        images = []\n",
    "        labels = []\n",
    "        with tf.name_scope('input_pipeline'):\n",
    "            for i, record in enumerate(records):\n",
    "                imgdata, label, bbox, text = deserialize_image_record(record)\n",
    "                image = self.preprocess(imgdata, bbox, thread_id=i)\n",
    "                label -= 1 # Change to 0-based (don't use background class)\n",
    "                images.append(image)\n",
    "                labels.append(label)\n",
    "            # Stack images back into a single tensor\n",
    "            images = tf.parallel_stack(images)\n",
    "            labels = tf.concat(labels, 0)\n",
    "            images = tf.reshape(images, [-1, self.height, self.width, 3])\n",
    "            images = tf.clip_by_value(images, 0., 255.)\n",
    "            images = tf.cast(images, self.dtype)\n",
    "        return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stage(tensors):\n",
    "    \"\"\"Stages the given tensors in a StagingArea for asynchronous put/get.\n",
    "    \"\"\"\n",
    "    stage_area = data_flow_ops.StagingArea(\n",
    "        dtypes=[tensor.dtype       for tensor in tensors],\n",
    "        shapes=[tensor.get_shape() for tensor in tensors])\n",
    "    put_op      = stage_area.put(tensors)\n",
    "    get_tensors = stage_area.get()\n",
    "\n",
    "    get_tensors = [tf.reshape(gt, t.get_shape())\n",
    "                   for (gt,t) in zip(get_tensors, tensors)]\n",
    "    return put_op, get_tensors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardTrainer(object):\n",
    "    def __init__(self, preprocessor, loss_func, nstep_per_epoch=None):\n",
    "        self.image_preprocessor = preprocessor\n",
    "        self.loss_func          = loss_func\n",
    "        with tf.device('/cpu:0'):\n",
    "            self.global_step = tf.get_variable(\n",
    "                'global_step', [],\n",
    "                initializer=tf.constant_initializer(0),\n",
    "                dtype=tf.int64,\n",
    "                trainable=False)\n",
    "        if FLAGS.lr_decay_policy == 'poly':\n",
    "            self.learning_rate = tf.train.polynomial_decay(\n",
    "                FLAGS.learning_rate,\n",
    "                self.global_step,\n",
    "                decay_steps=FLAGS.num_epochs*nstep_per_epoch,\n",
    "                end_learning_rate=0.,\n",
    "                power=FLAGS.lr_poly_power,\n",
    "                cycle=False)\n",
    "        else:\n",
    "            self.learning_rate = tf.train.exponential_decay(\n",
    "                FLAGS.learning_rate,\n",
    "                self.global_step,\n",
    "                decay_steps=FLAGS.lr_decay_epochs*nstep_per_epoch,\n",
    "                decay_rate=FLAGS.lr_decay_rate,\n",
    "                staircase=True)\n",
    "    def training_step(self, batch_size):\n",
    "        if type(self.image_preprocessor) is not DummyPreprocessor:\n",
    "            with tf.device('/cpu:0'):\n",
    "                images, labels = self.image_preprocessor.minibatch(batch_size)\n",
    "                # Stage images on the host\n",
    "                preload_op, (images, labels) = stage([images, labels])\n",
    "            with tf.device('/gpu:0'):\n",
    "                # Copy images from host to device\n",
    "                gpucopy_op, (images, labels) = stage([images, labels])\n",
    "        else:\n",
    "            with tf.device('/gpu:0'):\n",
    "                input_shape = [self.image_preprocessor.batch,\n",
    "                               self.image_preprocessor.height,\n",
    "                               self.image_preprocessor.width,\n",
    "                               3]\n",
    "                images = tf.truncated_normal(\n",
    "                    input_shape,\n",
    "                    dtype=tf.float32,\n",
    "                    stddev=1.e-1,\n",
    "                    name='synthetic_images')\n",
    "                labels = tf.random_uniform(\n",
    "                    [self.image_preprocessor.batch],\n",
    "                    minval=0,\n",
    "                    maxval=self.image_preprocessor.nclass-1,\n",
    "                    dtype=tf.int32,\n",
    "                    name='synthetic_labels')\n",
    "                preload_op, (images, labels) = stage([images, labels])\n",
    "                gpucopy_op = None\n",
    "\n",
    "        with tf.device('/gpu:0'):\n",
    "            # Evaluate the loss and compute the gradients\n",
    "            with tf.variable_scope(\n",
    "                    'GPU_0',\n",
    "                    # Force all variables to be stored as float32\n",
    "                    custom_getter=float32_variable_storage_getter) as var_scope:\n",
    "                loss, logits = self.loss_func(images, labels, var_scope)\n",
    "\n",
    "        with tf.device('/cpu:0'): # No in_top_k implem on GPU\n",
    "            top1 = tf.reduce_mean(\n",
    "                tf.cast(tf.nn.in_top_k(logits, labels, 1), tf.float32))\n",
    "            top5 = tf.reduce_mean(\n",
    "                tf.cast(tf.nn.in_top_k(logits, labels, 5), tf.float32))\n",
    "\n",
    "            averager = tf.train.ExponentialMovingAverage(0.90, name='loss_avg',\n",
    "                                                         zero_debias=True)\n",
    "            avg_op = averager.apply([loss])\n",
    "            loss_avg = averager.average(loss)\n",
    "            # Note: This must be done _after_ the averager.average() call\n",
    "            #         because it changes total_loss into a new object.\n",
    "            with tf.control_dependencies([avg_op]):\n",
    "                total_loss     = tf.identity(loss)\n",
    "                total_loss_avg = tf.identity(loss_avg)\n",
    "            tf.summary.scalar('total_loss_raw', total_loss)\n",
    "            tf.summary.scalar('total_loss_avg', total_loss_avg)\n",
    "            tf.summary.scalar('Top-1_accuracy', 100.*top1)\n",
    "            tf.summary.scalar('Top-5_accuracy', 100.*top5)\n",
    "            tf.summary.scalar('learning_rate', self.learning_rate)\n",
    "\n",
    "        # Apply the gradients to optimize the loss function\n",
    "        with tf.device('/gpu:0'):\n",
    "            opt = tf.train.MomentumOptimizer(self.learning_rate, FLAGS.momentum,\n",
    "                                             use_nesterov=True)\n",
    "            opt = hvd.DistributedOptimizer(opt) #HVD!!\n",
    "            update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS) or []\n",
    "            with tf.control_dependencies(update_ops):\n",
    "                if FLAGS.loss_scale != 1:\n",
    "                    loss = loss * float(FLAGS.loss_scale)\n",
    "                gradvars = opt.compute_gradients(loss,\n",
    "                    gate_gradients=tf.train.Optimizer.GATE_NONE)\n",
    "                if FLAGS.loss_scale != 1:\n",
    "                    inv_scale = 1. / float(FLAGS.loss_scale)\n",
    "                    gradvars = [(grad * inv_scale, var)\n",
    "                                for grad, var in gradvars]\n",
    "\n",
    "            if FLAGS.LARC_eta is not None:\n",
    "                LARC_eta = float(FLAGS.LARC_eta)\n",
    "                LARC_epsilon = float(FLAGS.LARC_epsilon)\n",
    "                v_list = [tf.norm(tensor=v, ord=2) for _, v in gradvars]\n",
    "                g_list = [tf.norm(tensor=g, ord=2) if g is not None else 0.0\n",
    "                          for g, _ in gradvars ]\n",
    "                v_norms = tf.stack(v_list)\n",
    "                g_norms = tf.stack(g_list)\n",
    "                zeds = tf.zeros_like(v_norms)\n",
    "                cond = tf.logical_and(\n",
    "                    tf.not_equal(v_norms, zeds),\n",
    "                    tf.not_equal(g_norms, zeds))\n",
    "                true_vals = tf.scalar_mul(LARC_eta, tf.div(v_norms, g_norms))\n",
    "                false_vals = tf.fill(tf.shape(v_norms), LARC_epsilon)\n",
    "                larc_local_lr = tf.where(cond, true_vals, false_vals)\n",
    "                if FLAGS.LARC_mode != \"scale\":\n",
    "                    ones = tf.ones_like(v_norms)\n",
    "                    lr = tf.fill(tf.shape(v_norms), self.learning_rate)\n",
    "                    larc_local_lr = tf.minimum(tf.div(larc_local_lr, lr), ones)\n",
    "\n",
    "                gradvars = [(tf.multiply(larc_local_lr[i], g), v)\n",
    "                            if g is not None else (None, v)\n",
    "                            for i, (g, v) in enumerate(gradvars) ]\n",
    "\n",
    "            train_op = opt.apply_gradients(gradvars)\n",
    "\n",
    "        with tf.device('/cpu:0'):\n",
    "            with tf.control_dependencies([train_op]):\n",
    "                increment_global_step_op = tf.assign_add(self.global_step, 1)\n",
    "        self.enqueue_ops = []\n",
    "        self.enqueue_ops.append(preload_op)\n",
    "        if gpucopy_op is not None:\n",
    "            self.enqueue_ops.append(gpucopy_op)\n",
    "        all_training_ops = (self.enqueue_ops + [increment_global_step_op])\n",
    "        return total_loss_avg, self.learning_rate, all_training_ops\n",
    "    def init(self, sess):\n",
    "        init_op = tf.global_variables_initializer()\n",
    "        sess.run(init_op)\n",
    "    def sync(self, sess):\n",
    "        sync_op = hvd.broadcast_global_variables(0)\n",
    "        sess.run(sync_op)\n",
    "    def prefill_pipeline(self, sess):\n",
    "        # Pre-fill the input pipeline with data\n",
    "        for i in range(len(self.enqueue_ops)):\n",
    "            sess.run(self.enqueue_ops[:i+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardEvaluator(object):\n",
    "    def __init__(self, preprocessor, eval_func):\n",
    "        self.eval_func          = eval_func\n",
    "        self.image_preprocessor = preprocessor\n",
    "    def evaluation_step(self, batch_size):\n",
    "        with tf.device('/cpu:0'):\n",
    "            images, labels = self.image_preprocessor.minibatch(batch_size)\n",
    "            # Stage images on the host\n",
    "            preload_op, (images, labels) = stage([images, labels])\n",
    "        with tf.device('/gpu:0'):\n",
    "            # Copy images from host to device\n",
    "            gpucopy_op, (images, labels) = stage([images, labels])\n",
    "            # Evaluate the loss and compute the gradients\n",
    "            with tf.variable_scope('GPU_0') as var_scope:\n",
    "                top1, top5 = self.eval_func(images, labels, var_scope)\n",
    "        self.enqueue_ops = [preload_op, gpucopy_op]\n",
    "        return top1, top5, self.enqueue_ops\n",
    "    def prefill_pipeline(self, sess):\n",
    "        # Pre-fill the input pipeline with data\n",
    "        for i in range(len(self.enqueue_ops)):\n",
    "            sess.run(self.enqueue_ops[:i+1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_trivial(net, input_layer):\n",
    "    \"\"\"A trivial model for benchmarking input pipeline performance\"\"\"\n",
    "    net.use_batch_norm = False\n",
    "    x = net.input_layer(input_layer)\n",
    "    x = net.flatten(x)\n",
    "    x = net.fully_connected(x, 1)\n",
    "    return x\n",
    "\n",
    "def inference_lenet5(net, input_layer):\n",
    "    \"\"\"Tiny network matching TF's MNIST tutorial model\"\"\"\n",
    "    net.use_batch_norm = False\n",
    "    x = net.input_layer(input_layer)\n",
    "    x = net.conv(x, 32,    (5,5))\n",
    "    x = net.pool(x, 'MAX', (2,2))\n",
    "    x = net.conv(x, 64,    (5,5))\n",
    "    x = net.pool(x, 'MAX', (2,2))\n",
    "    x = net.flatten(x)\n",
    "    x = net.fully_connected(x, 512)\n",
    "    return x\n",
    "\n",
    "def inference_overfeat(net, input_layer):\n",
    "    net.use_batch_norm = False\n",
    "    x = net.input_layer(input_layer)\n",
    "    x = net.conv(x, 96,   (11,11), (4,4), 'VALID')\n",
    "    x = net.pool(x, 'MAX', (2,2))\n",
    "    x = net.conv(x, 256,   (5,5), (1,1), 'VALID')\n",
    "    x = net.pool(x, 'MAX', (2,2))\n",
    "    x = net.conv(x, 512,   (3,3))\n",
    "    x = net.conv(x, 1024,  (3,3))\n",
    "    x = net.conv(x, 1024,  (3,3))\n",
    "    x = net.pool(x, 'MAX', (2,2))\n",
    "    x = net.flatten(x)\n",
    "    x = net.fully_connected(x, 3072)\n",
    "    x = net.fully_connected(x, 4096)\n",
    "    return x\n",
    "\n",
    "def inference_alexnet_owt(net, input_layer):\n",
    "    \"\"\"Alexnet One Weird Trick model\n",
    "    https://arxiv.org/abs/1404.5997\n",
    "    \"\"\"\n",
    "    net.use_batch_norm = False\n",
    "    x = net.input_layer(input_layer)\n",
    "    # Note: VALID requires padding the images by 3 in width and height\n",
    "    x = net.conv(x, 64, (11,11), (4,4), 'VALID')\n",
    "    x = net.pool(x, 'MAX', (3,3))\n",
    "    x = net.conv(x, 192,   (5,5))\n",
    "    x = net.pool(x, 'MAX', (3,3))\n",
    "    x = net.conv(x, 384,   (3,3))\n",
    "    x = net.conv(x, 256,   (3,3))\n",
    "    x = net.conv(x, 256,   (3,3))\n",
    "    x = net.pool(x, 'MAX', (3,3))\n",
    "    x = net.flatten(x)\n",
    "    x = net.fully_connected(x, 4096)\n",
    "    x = net.dropout(x)\n",
    "    x = net.fully_connected(x, 4096)\n",
    "    x = net.dropout(x)\n",
    "    return x\n",
    "\n",
    "def inference_vgg_impl(net, input_layer, layer_counts):\n",
    "    net.use_batch_norm = False\n",
    "    x = net.input_layer(input_layer)\n",
    "    for _ in range(layer_counts[0]): x = net.conv(x,  64, (3,3))\n",
    "    x = net.pool(x, 'MAX', (2,2))\n",
    "    for _ in range(layer_counts[1]): x = net.conv(x, 128, (3,3))\n",
    "    x = net.pool(x, 'MAX', (2,2))\n",
    "    for _ in range(layer_counts[2]): x = net.conv(x, 256, (3,3))\n",
    "    x = net.pool(x, 'MAX', (2,2))\n",
    "    for _ in range(layer_counts[3]): x = net.conv(x, 512, (3,3))\n",
    "    x = net.pool(x, 'MAX', (2,2))\n",
    "    for _ in range(layer_counts[4]): x = net.conv(x, 512, (3,3))\n",
    "    x = net.pool(x, 'MAX', (2,2))\n",
    "    x = net.flatten(x)\n",
    "    x = net.fully_connected(x, 4096)\n",
    "    x = net.fully_connected(x, 4096)\n",
    "    return x\n",
    "def inference_vgg(net, input_layer, nlayer):\n",
    "    \"\"\"Visual Geometry Group's family of models\n",
    "    https://arxiv.org/abs/1409.1556\n",
    "    \"\"\"\n",
    "    if   nlayer == 11: return inference_vgg_impl(net, input_layer, [1,1,2,2,2]) # A\n",
    "    elif nlayer == 13: return inference_vgg_impl(net, input_layer, [2,2,2,2,2]) # B\n",
    "    elif nlayer == 16: return inference_vgg_impl(net, input_layer, [2,2,3,3,3]) # D\n",
    "    elif nlayer == 19: return inference_vgg_impl(net, input_layer, [2,2,4,4,4]) # E\n",
    "    else: raise ValueError(\"Invalid nlayer (%i); must be one of: 11,13,16,19\" %\n",
    "                           nlayer)\n",
    "\n",
    "def inference_googlenet(net, input_layer):\n",
    "    \"\"\"GoogLeNet model\n",
    "    https://arxiv.org/abs/1409.4842\n",
    "    \"\"\"\n",
    "    net.use_batch_norm = False\n",
    "    def inception_v1(net, x, k, l, m, n, p, q):\n",
    "        cols = [[('conv', k, (1,1))],\n",
    "                [('conv', l, (1,1)), ('conv', m, (3,3))],\n",
    "                [('conv', n, (1,1)), ('conv', p, (5,5))],\n",
    "                [('pool', 'MAX', (3,3), (1,1), 'SAME'), ('conv', q, (1,1))]]\n",
    "        return net.inception_module(x, 'incept_v1', cols)\n",
    "    x = net.input_layer(input_layer)\n",
    "    x = net.conv(x,    64, (7,7), (2,2))\n",
    "    x = net.pool(x, 'MAX', (3,3), padding='SAME')\n",
    "    x = net.conv(x,    64, (1,1))\n",
    "    x = net.conv(x,   192, (3,3))\n",
    "    x = net.pool(x, 'MAX', (3,3), padding='SAME')\n",
    "    x = inception_v1(net, x,  64,  96, 128, 16,  32,  32)\n",
    "    x = inception_v1(net, x, 128, 128, 192, 32,  96,  64)\n",
    "    x = net.pool(x, 'MAX', (3,3), padding='SAME')\n",
    "    x = inception_v1(net, x, 192,  96, 208, 16,  48,  64)\n",
    "    x = inception_v1(net, x, 160, 112, 224, 24,  64,  64)\n",
    "    x = inception_v1(net, x, 128, 128, 256, 24,  64,  64)\n",
    "    x = inception_v1(net, x, 112, 144, 288, 32,  64,  64)\n",
    "    x = inception_v1(net, x, 256, 160, 320, 32, 128, 128)\n",
    "    x = net.pool(x, 'MAX', (3,3), padding='SAME')\n",
    "    x = inception_v1(net, x, 256, 160, 320, 32, 128, 128)\n",
    "    x = inception_v1(net, x, 384, 192, 384, 48, 128, 128)\n",
    "    x = net.spatial_avg(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_inception_v3(net, input_layer):\n",
    "    \"\"\"Google's Inception v3 model\n",
    "    https://arxiv.org/abs/1512.00567\n",
    "    \"\"\"\n",
    "    def inception_v3_a(net, x, n):\n",
    "        cols = [[('conv',  64, (1,1))],\n",
    "                [('conv',  48, (1,1)), ('conv',  64, (5,5))],\n",
    "                [('conv',  64, (1,1)), ('conv',  96, (3,3)), ('conv',  96, (3,3))],\n",
    "                [('pool', 'AVG', (3,3), (1,1), 'SAME'), ('conv',   n, (1,1))]]\n",
    "        return net.inception_module(x, 'incept_v3_a', cols)\n",
    "    def inception_v3_b(net, x):\n",
    "        cols = [[('conv',  64, (1,1)), ('conv',  96, (3,3)), ('conv',  96, (3,3), (2,2), 'VALID')],\n",
    "                [('conv', 384, (3,3), (2,2), 'VALID')],\n",
    "                [('pool', 'MAX', (3,3), (2,2), 'VALID')]]\n",
    "        return net.inception_module(x, 'incept_v3_b', cols)\n",
    "    def inception_v3_c(net, x, n):\n",
    "        cols = [[('conv', 192, (1,1))],\n",
    "                [('conv',   n, (1,1)), ('conv',   n, (1,7)), ('conv', 192, (7,1))],\n",
    "                [('conv',   n, (1,1)), ('conv',   n, (7,1)), ('conv',   n, (1,7)), ('conv',   n, (7,1)), ('conv', 192, (1,7))],\n",
    "                [('pool', 'AVG', (3,3), (1,1), 'SAME'), ('conv', 192, (1,1))]]\n",
    "        return net.inception_module(x, 'incept_v3_c', cols)\n",
    "    def inception_v3_d(net, x):\n",
    "        cols = [[('conv', 192, (1,1)), ('conv', 320, (3,3), (2,2), 'VALID')],\n",
    "                [('conv', 192, (1,1)), ('conv', 192, (1,7)), ('conv', 192, (7,1)), ('conv', 192, (3,3), (2,2), 'VALID')],\n",
    "                [('pool', 'MAX', (3,3), (2,2), 'VALID')]]\n",
    "        return net.inception_module(x, 'incept_v3_d',cols)\n",
    "    def inception_v3_e(net, x, pooltype):\n",
    "        cols = [[('conv', 320, (1,1))],\n",
    "                [('conv', 384, (1,1)), ('conv', 384, (1,3))],\n",
    "                [('share',),           ('conv', 384, (3,1))],\n",
    "                [('conv', 448, (1,1)), ('conv', 384, (3,3)), ('conv', 384, (1,3))],\n",
    "                [('share',),          ('share',),            ('conv', 384, (3,1))],\n",
    "                [('pool', pooltype, (3,3), (1,1), 'SAME'),   ('conv', 192, (1,1))]]\n",
    "        return net.inception_module(x, 'incept_v3_e', cols)\n",
    "\n",
    "    # TODO: This does not include the extra 'arm' that forks off\n",
    "    #         from before the 3rd-last module (the arm is designed\n",
    "    #         to speed up training in the early stages).\n",
    "    net.use_batch_norm = True\n",
    "    x = net.input_layer(input_layer)\n",
    "    x = net.conv(x,    32, (3,3), (2,2), padding='VALID')\n",
    "    x = net.conv(x,    32, (3,3), (1,1), padding='VALID')\n",
    "    x = net.conv(x,    64, (3,3), (1,1), padding='SAME')\n",
    "    x = net.pool(x, 'MAX', (3,3))\n",
    "    x = net.conv(x,    80, (1,1), (1,1), padding='VALID')\n",
    "    x = net.conv(x,   192, (3,3), (1,1), padding='VALID')\n",
    "    x = net.pool(x, 'MAX', (3,3))\n",
    "    x = inception_v3_a(net, x, 32)\n",
    "    x = inception_v3_a(net, x, 64)\n",
    "    x = inception_v3_a(net, x, 64)\n",
    "    x = inception_v3_b(net, x)\n",
    "    x = inception_v3_c(net, x, 128)\n",
    "    x = inception_v3_c(net, x, 160)\n",
    "    x = inception_v3_c(net, x, 160)\n",
    "    x = inception_v3_c(net, x, 192)\n",
    "    x = inception_v3_d(net, x)\n",
    "    x = inception_v3_e(net, x, 'AVG')\n",
    "    x = inception_v3_e(net, x, 'MAX')\n",
    "    x = net.spatial_avg(x)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet_bottleneck_v1(net, input_layer, depth, depth_bottleneck, stride,\n",
    "                         basic=False):\n",
    "    num_inputs = input_layer.get_shape().as_list()[1]\n",
    "    x = input_layer\n",
    "    s = stride\n",
    "    with tf.name_scope('resnet_v1'):\n",
    "        if depth == num_inputs:\n",
    "            if stride == 1:\n",
    "                shortcut = input_layer\n",
    "            else:\n",
    "                shortcut = net.pool(x, 'MAX', (1,1), (s,s))\n",
    "        else:\n",
    "            shortcut = net.conv(x, depth, (1,1), (s,s), activation='LINEAR')\n",
    "        if basic:\n",
    "            x = net.conv(x, depth_bottleneck, (3,3), (s,s), padding='SAME_RESNET')\n",
    "            x = net.conv(x, depth,            (3,3), activation='LINEAR')\n",
    "        else:\n",
    "            x = net.conv(x, depth_bottleneck, (1,1), (s,s))\n",
    "            x = net.conv(x, depth_bottleneck, (3,3), padding='SAME')\n",
    "            x = net.conv(x, depth,            (1,1), activation='LINEAR')\n",
    "        x = net.activate(x + shortcut)\n",
    "        return x\n",
    "\n",
    "def resnext_split_branch(net, input_layer, stride):\n",
    "    x = input_layer\n",
    "    with tf.name_scope('resnext_split_branch'):\n",
    "        x = net.conv(x, net.bottleneck_width, (1, 1), (stride, stride), activation='RELU', use_batch_norm=True)\n",
    "        x = net.conv(x, net.bottleneck_width, (3, 3), (1, 1), activation='RELU', use_batch_norm=True)\n",
    "    return x\n",
    "\n",
    "def resnext_shortcut(net, input_layer, stride, input_size, output_size):\n",
    "    x = input_layer\n",
    "    useConv = net.shortcut_type == 'C' or (net.shortcut_type == 'B' and input_size != output_size)\n",
    "    with tf.name_scope('resnext_shortcut'):\n",
    "        if useConv:\n",
    "            x = net.conv(x, output_size, (1,1), (stride, stride), use_batch_norm=True)\n",
    "        elif output_size == input_size:\n",
    "            if stride == 1:\n",
    "                x = input_layer\n",
    "            else:\n",
    "                x = net.pool(x, 'MAX', (1,1), (stride, stride))\n",
    "        else:\n",
    "            x = input_layer\n",
    "    return x\n",
    "\n",
    "def resnext_bottleneck_v1(net, input_layer, depth, depth_bottleneck, stride):\n",
    "    num_inputs = input_layer.get_shape().as_list()[1]\n",
    "    x = input_layer\n",
    "    with tf.name_scope('resnext_bottleneck_v1'):\n",
    "        shortcut = resnext_shortcut(net, x, stride, num_inputs, depth)\n",
    "        branches_list = []\n",
    "        for i in range(net.cardinality):\n",
    "            branch = resnext_split_branch(net, x, stride)\n",
    "            branches_list.append(branch)\n",
    "        concatenated_branches = tf.concat(values=branches_list, axis=1, name='concat')\n",
    "        bottleneck_depth = concatenated_branches.get_shape().as_list()[1]\n",
    "        x = net.conv(concatenated_branches, depth, (1, 1), (1, 1), activation=None)\n",
    "        x = net.activate(x + shortcut, 'RELU')\n",
    "    return x\n",
    "\n",
    "def inference_residual(net, input_layer, layer_counts, bottleneck_callback):\n",
    "    net.use_batch_norm = True\n",
    "    x = net.input_layer(input_layer)\n",
    "    x = net.conv(x, 64,    (7,7), (2,2), padding='SAME_RESNET')\n",
    "    x = net.pool(x, 'MAX', (3,3), (2,2), padding='SAME')\n",
    "    for i in range(layer_counts[0]):\n",
    "        x = bottleneck_callback(net, x,  256,  64, 1)\n",
    "    for i in range(layer_counts[1]):\n",
    "        x = bottleneck_callback(net, x, 512, 128, 2 if i==0 else 1)\n",
    "    for i in range(layer_counts[2]):\n",
    "        x = bottleneck_callback(net, x, 1024, 256, 2 if i==0 else 1)\n",
    "    for i in range(layer_counts[3]):\n",
    "        x = bottleneck_callback(net, x, 2048, 512, 2 if i==0 else 1)\n",
    "    x = net.spatial_avg(x)\n",
    "    return x\n",
    "\n",
    "def inference_resnet_v1_basic_impl(net, input_layer, layer_counts):\n",
    "    basic_resnet_bottleneck_callback = partial(resnet_bottleneck_v1, basic=True)\n",
    "    return inference_residual(net, input_layer, layer_counts, basic_resnet_bottleneck_callback)\n",
    "\n",
    "def inference_resnet_v1_impl(net, input_layer, layer_counts):\n",
    "    return inference_residual(net, input_layer, layer_counts, resnet_bottleneck_v1)\n",
    "\n",
    "def inference_resnext_v1_impl(net, input_layer, layer_counts):\n",
    "    return inference_residual(net, input_layer, layer_counts, resnext_bottleneck_v1)\n",
    "\n",
    "def inference_resnet_v1(net, input_layer, nlayer):\n",
    "    \"\"\"Deep Residual Networks family of models\n",
    "    https://arxiv.org/abs/1512.03385\n",
    "    \"\"\"\n",
    "    if   nlayer ==  18: return inference_resnet_v1_basic_impl(net, input_layer, [2,2, 2,2])\n",
    "    elif nlayer ==  34: return inference_resnet_v1_basic_impl(net, input_layer, [3,4, 6,3])\n",
    "    elif nlayer ==  50: return inference_resnet_v1_impl(net, input_layer, [3,4, 6,3])\n",
    "    elif nlayer == 101: return inference_resnet_v1_impl(net, input_layer, [3,4,23,3])\n",
    "    elif nlayer == 152: return inference_resnet_v1_impl(net, input_layer, [3,8,36,3])\n",
    "    else: raise ValueError(\"Invalid nlayer (%i); must be one of: 18,34,50,101,152\" %\n",
    "                           nlayer)\n",
    "\n",
    "def inference_resnext_v1(net, input_layer, nlayer):\n",
    "    \"\"\"Aggregated  Residual Networks family of models\n",
    "    https://arxiv.org/abs/1611.05431\n",
    "    \"\"\"\n",
    "    cardinality_to_bottleneck_width = { 1:64, 2:40, 4:24, 8:14, 32:4 }\n",
    "    net.cardinality = 32\n",
    "    net.shortcut_type = 'B'\n",
    "    assert net.cardinality in cardinality_to_bottleneck_width.keys(), \\\n",
    "    \"Invalid  cardinality (%i); must be one of: 1,2,4,8,32\" % net.cardinality\n",
    "    net.bottleneck_width = cardinality_to_bottleneck_width[net.cardinality]\n",
    "    if nlayer ==  50: return inference_resnext_v1_impl(net, input_layer, [3,4, 6,3])\n",
    "    elif nlayer == 101: return inference_resnext_v1_impl(net, input_layer, [3,4,23,3])\n",
    "    elif nlayer == 152: return inference_resnext_v1_impl(net, input_layer, [3,8,36,3])\n",
    "    else: raise ValueError(\"Invalid nlayer (%i); must be one of: 50,101,152\" %\n",
    "                           nlayer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stem functions\n",
    "def inception_v4_sa(net, x):\n",
    "    cols = [[('pool', 'MAX', (3,3))],\n",
    "            [('conv',  96, (3,3), (2,2), 'VALID')]]\n",
    "    return net.inception_module(x, 'incept_v4_sa', cols)\n",
    "def inception_v4_sb(net, x):\n",
    "    cols = [[('conv',  64, (1,1)), ('conv',  96, (3,3), (1,1), 'VALID')],\n",
    "            [('conv',  64, (1,1)), ('conv',  64, (7,1)), ('conv',  64, (1,7)), ('conv',  96, (3,3), (1,1), 'VALID')]]\n",
    "    return net.inception_module(x, 'incept_v4_sb', cols)\n",
    "def inception_v4_sc(net, x):\n",
    "    cols = [[('conv', 192, (3,3), (2,2), 'VALID')],\n",
    "            [('pool', 'MAX', (3,3))]]\n",
    "    return net.inception_module(x, 'incept_v4_sc', cols)\n",
    "# Reduction functions\n",
    "def inception_v4_ra(net, x, k, l, m, n):\n",
    "    cols = [[('pool', 'MAX', (3,3))],\n",
    "            [('conv',   n, (3,3), (2,2), 'VALID')],\n",
    "            [('conv',   k, (1,1)), ('conv',   l, (3,3)), ('conv',   m, (3,3), (2,2), 'VALID')]]\n",
    "    return net.inception_module(x, 'incept_v4_ra', cols)\n",
    "def inception_v4_rb(net, x):\n",
    "    cols = [[('pool', 'MAX', (3,3))],\n",
    "            [('conv', 192, (1,1)), ('conv', 192, (3,3), (2,2), 'VALID')],\n",
    "            [('conv', 256, (1,1)), ('conv', 256, (1,7)), ('conv', 320, (7,1)), ('conv', 320, (3,3), (2,2), 'VALID')]]\n",
    "    return net.inception_module(x, 'incept_v4_rb', cols)\n",
    "def inception_resnet_v2_rb(net, x):\n",
    "    cols = [[('pool', 'MAX', (3,3))],\n",
    "            # Note: These match Facebook's Torch implem\n",
    "            [('conv', 256, (1,1)), ('conv', 384, (3,3), (2,2), 'VALID')],\n",
    "            [('conv', 256, (1,1)), ('conv', 256, (3,3), (2,2), 'VALID')],\n",
    "            [('conv', 256, (1,1)), ('conv', 256, (3,3)), ('conv', 256, (3,3), (2,2), 'VALID')]]\n",
    "    return net.inception_module(x, 'incept_resnet_v2_rb', cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_inception_v4(net, input_layer):\n",
    "    \"\"\"Google's Inception v4 model\n",
    "    https://arxiv.org/abs/1602.07261\n",
    "    \"\"\"\n",
    "    def inception_v4_a(net, x):\n",
    "        cols = [[('pool', 'AVG', (3,3), (1,1), 'SAME'), ('conv',  96, (1,1))],\n",
    "                [('conv',  96, (1,1))],\n",
    "                [('conv',  64, (1,1)), ('conv',  96, (3,3))],\n",
    "                [('conv',  64, (1,1)), ('conv',  96, (3,3)), ('conv',  96, (3,3))]]\n",
    "        return net.inception_module(x, 'incept_v4_a', cols)\n",
    "    def inception_v4_b(net, x):\n",
    "        cols = [[('pool', 'AVG', (3,3), (1,1), 'SAME'), ('conv', 128, (1,1))],\n",
    "                [('conv', 384, (1,1))],\n",
    "                [('conv', 192, (1,1)), ('conv', 224, (1,7)), ('conv', 256, (7,1))],\n",
    "                [('conv', 192, (1,1)), ('conv', 192, (1,7)), ('conv', 224, (7,1)), ('conv', 224, (1,7)), ('conv', 256, (7,1))]]\n",
    "        return net.inception_module(x, 'incept_v4_b', cols)\n",
    "    def inception_v4_c(net, x):\n",
    "        cols = [[('pool', 'AVG', (3,3), (1,1), 'SAME'), ('conv', 256, (1,1))],\n",
    "                [('conv', 256, (1,1))],\n",
    "                [('conv', 384, (1,1)), ('conv', 256, (1,3))],\n",
    "                [('share',),           ('conv', 256, (3,1))],\n",
    "                [('conv', 384, (1,1)), ('conv', 448, (1,3)), ('conv', 512, (3,1)), ('conv', 256, (3,1))],\n",
    "                [('share',),           ('share',),           ('share',),           ('conv', 256, (1,3))]]\n",
    "        return net.inception_module(x, 'incept_v4_c', cols)\n",
    "\n",
    "    net.use_batch_norm = True\n",
    "    x = net.input_layer(input_layer)\n",
    "    x = net.conv(x, 32, (3,3), (2,2), padding='VALID')\n",
    "    x = net.conv(x, 32, (3,3), (1,1), padding='VALID')\n",
    "    x = net.conv(x, 64, (3,3))\n",
    "    x = inception_v4_sa(net, x)\n",
    "    x = inception_v4_sb(net, x)\n",
    "    x = inception_v4_sc(net, x)\n",
    "    for _ in range(4):\n",
    "        x = inception_v4_a(net, x)\n",
    "    x = inception_v4_ra(net, x, 192, 224, 256, 384)\n",
    "    for _ in range(7):\n",
    "        x = inception_v4_b(net, x)\n",
    "    x = inception_v4_rb(net, x)\n",
    "    for _ in range(3):\n",
    "        x = inception_v4_c(net, x)\n",
    "    x = net.spatial_avg(x)\n",
    "    x = net.dropout(x, 0.8)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_inception_resnet_v2(net, input_layer):\n",
    "    \"\"\"Google's Inception-Resnet v2 model\n",
    "    https://arxiv.org/abs/1602.07261\n",
    "    \"\"\"\n",
    "    def inception_resnet_v2_a(net, x):\n",
    "        cols = [[('conv',  32, (1,1))],\n",
    "                [('conv',  32, (1,1)), ('conv',  32, (3,3))],\n",
    "                [('conv',  32, (1,1)), ('conv',  48, (3,3)), ('conv',  64, (3,3))]]\n",
    "        x = net.inception_module(x, 'incept_resnet_v2_a', cols)\n",
    "        x = net.conv(x, 384, (1,1), activation='LINEAR')\n",
    "        return x\n",
    "    def inception_resnet_v2_b(net, x):\n",
    "        cols = [[('conv', 192, (1,1))],\n",
    "                [('conv', 128, (1,1)), ('conv', 160, (1,7)), ('conv', 192, (7,1))]]\n",
    "        x = net.inception_module(x, 'incept_resnet_v2_b', cols)\n",
    "        x = net.conv(x, 1152, (1,1), activation='LINEAR')\n",
    "        return x\n",
    "    def inception_resnet_v2_c(net, x):\n",
    "        cols = [[('conv', 192, (1,1))],\n",
    "                [('conv', 192, (1,1)), ('conv', 224, (1,3)), ('conv', 256, (3,1))]]\n",
    "        x = net.inception_module(x, 'incept_resnet_v2_c', cols)\n",
    "        x = net.conv(x, 2048, (1,1), activation='LINEAR')\n",
    "        return x\n",
    "\n",
    "    net.use_batch_norm = True\n",
    "    residual_scale = 0.2\n",
    "    x = net.input_layer(input_layer)\n",
    "    x = net.conv(x, 32, (3,3), (2,2), padding='VALID')\n",
    "    x = net.conv(x, 32, (3,3), (1,1), padding='VALID')\n",
    "    x = net.conv(x, 64, (3,3))\n",
    "    x = inception_v4_sa(net, x)\n",
    "    x = inception_v4_sb(net, x)\n",
    "    x = inception_v4_sc(net, x)\n",
    "    for _ in range(5):\n",
    "        x = net.residual(x, inception_resnet_v2_a, scale=residual_scale)\n",
    "    x = inception_v4_ra(net, x, 256, 256, 384, 384)\n",
    "    for _ in range(10):\n",
    "        x = net.residual(x, inception_resnet_v2_b, scale=residual_scale)\n",
    "    x = inception_resnet_v2_rb(net, x)\n",
    "    for _ in range(5):\n",
    "        x = net.residual(x, inception_resnet_v2_c, scale=residual_scale)\n",
    "    x = net.spatial_avg(x)\n",
    "    x = net.dropout(x, 0.8)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_evaluation(nstep, sess, top1_op, top5_op, enqueue_ops):\n",
    "    print(\"Evaluating\")\n",
    "    top1s = []\n",
    "    top5s = []\n",
    "    print(\"  Step  Top-1  Top-5\")\n",
    "    for step in range(nstep):\n",
    "        try:\n",
    "            top1, top5 = sess.run([top1_op, top5_op, enqueue_ops])[:2]\n",
    "            if step == 0 or (step+1) % FLAGS.display_every == 0:\n",
    "                print(\"% 6i %5.1f%% %5.1f%%\" % (step+1, top1*100, top5*100))\n",
    "            top1s.append(top1)\n",
    "            top5s.append(top5)\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"Keyboard interrupt\")\n",
    "            break\n",
    "    nstep = len(top1s)\n",
    "    if nstep == 0:\n",
    "        return\n",
    "    top1s = np.asarray(top1s) * 100.\n",
    "    top5s = np.asarray(top5s) * 100.\n",
    "    top1_mean = np.mean(top1s)\n",
    "    top5_mean = np.mean(top5s)\n",
    "    if nstep > 2:\n",
    "        top1_uncertainty = np.std(top1s, ddof=1) / np.sqrt(float(nstep))\n",
    "        top5_uncertainty = np.std(top5s, ddof=1) / np.sqrt(float(nstep))\n",
    "    else:\n",
    "        top1_uncertainty = float('nan')\n",
    "        top5_uncertainty = float('nan')\n",
    "    top1_madstd = 1.4826*np.median(np.abs(top1s - np.median(top1s)))\n",
    "    top5_madstd = 1.4826*np.median(np.abs(top5s - np.median(top5s)))\n",
    "    print('-' * 64)\n",
    "    print('Validation Top-1: %.3f %% +/- %.2f (jitter = %.1f)' % (\n",
    "        top1_mean, top1_uncertainty, top1_madstd))\n",
    "    print('Validation Top-5: %.3f %% +/- %.2f (jitter = %.1f)' % (\n",
    "        top5_mean, top5_uncertainty, top5_madstd))\n",
    "    print('-' * 64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_records(tf_record_pattern):\n",
    "    def count_records(tf_record_filename):\n",
    "        count = 0\n",
    "        for _ in tf.python_io.tf_record_iterator(tf_record_filename):\n",
    "            count += 1\n",
    "        return count\n",
    "    filenames = sorted(tf.gfile.Glob(tf_record_pattern))\n",
    "    nfile = len(filenames)\n",
    "    return (count_records(filenames[0])*(nfile-1) +\n",
    "            count_records(filenames[-1]))\n",
    "\n",
    "def add_bool_argument(cmdline, shortname, longname=None, default=False, help=None):\n",
    "    if longname is None:\n",
    "        shortname, longname = None, shortname\n",
    "    elif default == True:\n",
    "        raise ValueError(\"\"\"Boolean arguments that are True by default should not have short names.\"\"\")\n",
    "    name = longname[2:]\n",
    "    feature_parser = cmdline.add_mutually_exclusive_group(required=False)\n",
    "    if shortname is not None:\n",
    "        feature_parser.add_argument(shortname, '--'+name, dest=name, action='store_true', help=help, default=default)\n",
    "    else:\n",
    "        feature_parser.add_argument(           '--'+name, dest=name, action='store_true', help=help, default=default)\n",
    "    feature_parser.add_argument('--no'+name, dest=name, action='store_false')\n",
    "    return cmdline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_start_time = time.time()\n",
    "tf.set_random_seed(1234+hvd.rank())\n",
    "np.random.seed(4321+hvd.rank())\n",
    "cmdline = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "# Basic options\n",
    "cmdline.add_argument('-m', '--model', default='googlenet', \n",
    "                     help=\"\"\"Name of model to run:\n",
    "                     trivial, lenet,\n",
    "                     alexnet, googlenet, vgg[11,13,16,19],\n",
    "                     inception[3,4], resnet[18,34,50,101,152],\n",
    "                     resnext[50,101,152], inception-resnet2.\"\"\")\n",
    "cmdline.add_argument('--data_dir', default= None, #\"/home/ubuntu/examples/horovod/cnn/data\",\n",
    "                     help=\"\"\"Path to dataset in TFRecord format\n",
    "                     (aka Example protobufs). Files should be\n",
    "                     named 'train-*' and 'validation-*'.\"\"\")\n",
    "cmdline.add_argument('-b', '--batch_size', default=64, type=int,\n",
    "                     help=\"\"\"Size of each minibatch.\"\"\")\n",
    "cmdline.add_argument('--num_batches', default=50, type=int,\n",
    "                     help=\"\"\"Number of batches to run.\n",
    "                     Ignored during eval.\"\"\")\n",
    "cmdline.add_argument('--num_epochs', default=None, type=int,\n",
    "                     help=\"\"\"Number of epochs to run.\n",
    "                     Overrides --num_batches. Ignored during eval.\"\"\")\n",
    "cmdline.add_argument('--log_dir', default=\"/home/ubuntu/examples/horovod/cnn/cnn_logdir\",\n",
    "                     help=\"\"\"Directory in which to write training\n",
    "                     summaries and checkpoints.\"\"\")\n",
    "cmdline.add_argument('--display_every', default=1, type=int,\n",
    "                     help=\"\"\"How often (in iterations) to print out\n",
    "                     running information.\"\"\")\n",
    "cmdline.add_argument('--save_interval', default=43200, type=int,\n",
    "                     help=\"\"\"Time in seconds between checkpoints.\"\"\")\n",
    "cmdline.add_argument('--summary_interval', default=3600, type=int,\n",
    "                     help=\"\"\"Time in seconds between saves of summary statistics.\"\"\")\n",
    "add_bool_argument(cmdline, '--eval',\n",
    "                  help=\"\"\"Evaluate the top-1 and top-5 accuracy of\n",
    "                  a checkpointed model.\"\"\")\n",
    "add_bool_argument(cmdline, '--fp16',\n",
    "                  help=\"\"\"Train using float16 (half) precision instead\n",
    "                  of float32.\"\"\")\n",
    "\n",
    "global FLAGS\n",
    "FLAGS, unknown_args = cmdline.parse_known_args()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(batch_size=64, data_dir=None, display_every=1, eval=False, fp16=False, log_dir='/home/ubuntu/examples/horovod/cnn/cnn_logdir', model='googlenet', num_batches=50, num_epochs=None, save_interval=43200, summary_interval=3600)\n",
      "['-f', '/run/user/1000/jupyter/kernel-370a27a6-2f0b-432e-885b-b9bb2c8a4f2a.json']\n"
     ]
    }
   ],
   "source": [
    "print(FLAGS)\n",
    "print(unknown_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(FLAGS.log_dir):\n",
    "        os.makedirs(FLAGS.log_dir)\n",
    "\n",
    "# create logger with 'aws-tf-cnn'\n",
    "logger = logging.getLogger('aws-tf-hvd-cnn')\n",
    "logger.setLevel(logging.DEBUG)  # INFO, ERROR\n",
    "# file handler which logs debug messages\n",
    "fh = logging.FileHandler(os.path.join(FLAGS.log_dir, 'aws-tf-hvd-cnn.log'))\n",
    "fh.setLevel(logging.DEBUG)\n",
    "# console handler\n",
    "ch = logging.StreamHandler()\n",
    "ch.setLevel(logging.INFO)\n",
    "# add formatter to the handlers\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "fh.setFormatter(formatter)\n",
    "ch.setFormatter(formatter)\n",
    "# add handlers to logger\n",
    "logger.addHandler(fh)\n",
    "logger.addHandler(ch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-08 21:54:37,840 - aws-tf-hvd-cnn - INFO - TensorFlow:  1.8.0\n",
      "2018-10-08 21:54:37,842 - aws-tf-hvd-cnn - INFO - Parameters specified:\n",
      "2018-10-08 21:54:37,843 - aws-tf-hvd-cnn - INFO -   -f\n",
      "  /run/user/1000/jupyter/kernel-370a27a6-2f0b-432e-885b-b9bb2c8a4f2a.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200\n"
     ]
    }
   ],
   "source": [
    "nclass = 1000\n",
    "batch_size = FLAGS.batch_size\n",
    "subset = 'validation' if FLAGS.eval else 'train'\n",
    "\n",
    "tfversion = tensorflow_version_tuple()\n",
    "logger.info(\"TensorFlow:  %i.%i.%s\" % tfversion)\n",
    "logger.info(\"Parameters specified:\")\n",
    "logger.info('\\n'.join(['  '+arg for arg in sys.argv[1:]]))\n",
    "\n",
    "nrecord = 0\n",
    "if FLAGS.data_dir is not None and FLAGS.data_dir != '':\n",
    "    nrecord = get_num_records(os.path.join(FLAGS.data_dir, '%s-*' % subset))\n",
    "else:\n",
    "    nrecord = FLAGS.num_batches * batch_size * hvd.size()\n",
    "\n",
    "print(nrecord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-08 21:55:48,315 - aws-tf-hvd-cnn - INFO - Num ranks:  1\n",
      "2018-10-08 21:55:48,316 - aws-tf-hvd-cnn - INFO - Synthetic\n",
      "2018-10-08 21:55:48,317 - aws-tf-hvd-cnn - INFO - Model: googlenet\n",
      "2018-10-08 21:55:48,318 - aws-tf-hvd-cnn - INFO - Total batch size: 64\n",
      "2018-10-08 21:55:48,319 - aws-tf-hvd-cnn - INFO - 64, per device\n",
      "2018-10-08 21:55:48,320 - aws-tf-hvd-cnn - INFO - Data format: 'NCHW'\n",
      "2018-10-08 21:55:48,321 - aws-tf-hvd-cnn - INFO - Data type:  fp32\n"
     ]
    }
   ],
   "source": [
    "# Training hyperparameters\n",
    "FLAGS.learning_rate         = 0.001 # Model-specific values are set below\n",
    "FLAGS.momentum              = 0.9\n",
    "FLAGS.lr_decay_policy       = 'poly'\n",
    "FLAGS.lr_decay_epochs       = 30\n",
    "FLAGS.lr_decay_rate         = 0.1\n",
    "FLAGS.lr_poly_power         = 2.\n",
    "FLAGS.weight_decay          = 1e-4\n",
    "FLAGS.input_buffer_size     = min(10000, nrecord)\n",
    "FLAGS.distort_color         = False\n",
    "FLAGS.nstep_burnin          = 20\n",
    "FLAGS.loss_scale            = 256. # TODO: May need to decide this based on model\n",
    "FLAGS.LARC_eta              = 0.003\n",
    "FLAGS.LARC_epsilon          = 1.\n",
    "FLAGS.LARC_mode             = 'clip'\n",
    "\n",
    "model_dtype = tf.float16 if FLAGS.fp16 else tf.float32\n",
    "\n",
    "logger.info(\"Num ranks:  {}\".format(hvd.size()))\n",
    "logger.info(\"Num of images: {}\".format(nrecord)) if FLAGS.data_dir is not None else logger.info('Synthetic')\n",
    "logger.info(\"Model: {}\".format(FLAGS.model))\n",
    "logger.info(\"Total batch size: {}\".format(batch_size * hvd.size()))\n",
    "logger.info(\"{}, per device\".format(batch_size))\n",
    "logger.info(\"Data format: 'NCHW'\")\n",
    "logger.info(\"Data type:  fp16\") if model_dtype == tf.float16 else logger.info('Data type:  fp32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 1\n"
     ]
    }
   ],
   "source": [
    "if FLAGS.num_epochs is not None:\n",
    "    if FLAGS.data_dir is None:\n",
    "        logger.error(\"num_epochs requires data_dir to be specified\")\n",
    "        raise ValueError(\"num_epochs requires data_dir to be specified\")\n",
    "    nstep = nrecord * FLAGS.num_epochs // (batch_size * hvd.size())\n",
    "else:\n",
    "    nstep = FLAGS.num_batches\n",
    "    FLAGS.num_epochs = max(nstep * batch_size * hvd.size() // nrecord, 1)\n",
    "print(nstep, FLAGS.num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "224 224 <function inference_googlenet at 0x7f1a3ba6d7b8>\n"
     ]
    }
   ],
   "source": [
    " model_name = FLAGS.model\n",
    "if   model_name == 'trivial':\n",
    "    height, width = 224, 224\n",
    "    model_func = inference_trivial\n",
    "elif model_name == 'lenet':\n",
    "    height, width = 28, 28\n",
    "    model_func = inference_lenet5\n",
    "elif model_name == 'alexnet':\n",
    "    height, width = 227, 227\n",
    "    model_func = inference_alexnet_owt\n",
    "    FLAGS.learning_rate = 0.03\n",
    "elif model_name == 'overfeat':\n",
    "    height, width = 231, 231\n",
    "    model_func = inference_overfeat\n",
    "elif model_name.startswith('vgg'):\n",
    "    height, width = 224, 224\n",
    "    nlayer = int(model_name[len('vgg'):])\n",
    "    model_func = lambda net, images: inference_vgg(net, images, nlayer)\n",
    "    FLAGS.learning_rate = 0.02\n",
    "elif model_name == 'googlenet':\n",
    "    height, width = 224, 224\n",
    "    model_func = inference_googlenet\n",
    "    FLAGS.learning_rate = 0.04\n",
    "elif model_name == 'inception3':\n",
    "    height, width = 299, 299\n",
    "    model_func = inference_inception_v3\n",
    "    FLAGS.learning_rate = 0.2\n",
    "elif model_name.startswith('resnet'):\n",
    "    height, width = 224, 224\n",
    "    nlayer = int(model_name[len('resnet'):])\n",
    "    model_func = lambda net, images: inference_resnet_v1(net, images, nlayer)\n",
    "    FLAGS.learning_rate = 1. * (batch_size * hvd.size() / 1024.0) if nlayer > 18 else 0.5\n",
    "elif model_name.startswith('resnext'):\n",
    "    height, width = 224, 224\n",
    "    nlayer = int(model_name[len('resnext'):])\n",
    "    model_func = lambda net, images: inference_resnext_v1(net, images, nlayer)\n",
    "    FLAGS.learning_rate = 0.1\n",
    "elif model_name == 'inception4':\n",
    "    height, width = 299, 299\n",
    "    model_func = inference_inception_v4\n",
    "    FLAGS.learning_rate = 0.045\n",
    "elif model_name == 'inception-resnet2':\n",
    "    height, width = 299, 299\n",
    "    model_func = inference_inception_resnet_v2\n",
    "    FLAGS.learning_rate = 0.045\n",
    "else:\n",
    "    logger.error(\"Invalid model type: {}\".format(model_name))\n",
    "    raise ValueError(\"Invalid model type: %s\" % model_name)\n",
    "\n",
    "print(height, width, model_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.DummyPreprocessor object at 0x7f1a38168630>\n"
     ]
    }
   ],
   "source": [
    "if FLAGS.data_dir is None:\n",
    "    preprocessor = DummyPreprocessor(height, width, batch_size, nclass)\n",
    "else:\n",
    "    preprocessor = ImagePreprocessor(height, width, subset)\n",
    "\n",
    "print(preprocessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_func(images, labels, var_scope):\n",
    "    # Build the forward model\n",
    "    net = GPUNetworkBuilder(True, dtype=model_dtype)\n",
    "    output = model_func(net, images)\n",
    "    # Add final FC layer to produce nclass outputs\n",
    "    logits = net.fully_connected(output, nclass, activation='LINEAR')\n",
    "    if logits.dtype != tf.float32:\n",
    "        logits = tf.cast(logits, tf.float32)\n",
    "    loss = tf.losses.sparse_softmax_cross_entropy(\n",
    "        logits=logits, labels=labels)\n",
    "    # Add weight decay\n",
    "    if FLAGS.weight_decay is not None and FLAGS.weight_decay != 0.:\n",
    "        params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\n",
    "                                   scope=var_scope.name)\n",
    "        l2_loss = tf.add_n([tf.nn.l2_loss(w) for w in params])\n",
    "        if l2_loss.dtype != tf.float32:\n",
    "            l2_loss = tf.cast(l2_loss, tf.float32)\n",
    "        loss += FLAGS.weight_decay * l2_loss\n",
    "    return loss, logits\n",
    "\n",
    "def eval_func(images, labels, var_scope):\n",
    "    net = GPUNetworkBuilder(False, dtype=model_dtype)\n",
    "    output = model_func(net, images)\n",
    "    logits = net.fully_connected(output, nclass, activation='LINEAR')\n",
    "    if logits.dtype != tf.float32:\n",
    "        logits = tf.cast(logits, tf.float32)\n",
    "    with tf.device('/cpu:0'):\n",
    "        top1 = tf.reduce_mean(\n",
    "            tf.cast(tf.nn.in_top_k(logits, labels, 1), tf.float32))\n",
    "        top5 = tf.reduce_mean(\n",
    "            tf.cast(tf.nn.in_top_k(logits, labels, 5), tf.float32))\n",
    "    return top1, top5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Variable global_step already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"<ipython-input-8-df456ffc3140>\", line 10, in __init__\n    trainable=False)\n  File \"<ipython-input-51-4cb7201b34c4>\", line 16, in <module>\n    trainer = FeedForwardTrainer(preprocessor, loss_func, nstep_per_epoch)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-4cb7201b34c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mnstep_per_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnrecord\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mhvd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFeedForwardTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocessor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnstep_per_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Building training graph\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     total_loss, learning_rate, train_ops = trainer.training_step(\n",
      "\u001b[0;32m<ipython-input-8-df456ffc3140>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, preprocessor, loss_func, nstep_per_epoch)\u001b[0m\n\u001b[1;32m      8\u001b[0m                 \u001b[0minitializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                 \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m                 trainable=False)\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_decay_policy\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'poly'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             self.learning_rate = tf.train.polynomial_decay(\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\u001b[0m\n\u001b[1;32m   1315\u001b[0m       \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       \u001b[0muse_resource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_getter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_getter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1317\u001b[0;31m       constraint=constraint)\n\u001b[0m\u001b[1;32m   1318\u001b[0m get_variable_or_local_docstring = (\n\u001b[1;32m   1319\u001b[0m     \"\"\"%s\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\u001b[0m\n\u001b[1;32m   1077\u001b[0m           \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1078\u001b[0m           \u001b[0muse_resource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_getter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_getter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1079\u001b[0;31m           constraint=constraint)\n\u001b[0m\u001b[1;32m   1080\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\u001b[0m\n\u001b[1;32m    423\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m           \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_resource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m           constraint=constraint)\n\u001b[0m\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m   def _get_partitioned_variable(\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_true_getter\u001b[0;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, constraint)\u001b[0m\n\u001b[1;32m    392\u001b[0m           \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m           use_resource=use_resource, constraint=constraint)\n\u001b[0m\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcustom_getter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_get_single_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource, constraint)\u001b[0m\n\u001b[1;32m    731\u001b[0m                          \u001b[0;34m\"reuse=tf.AUTO_REUSE in VarScope? \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m                          \"Originally defined at:\\n\\n%s\" % (\n\u001b[0;32m--> 733\u001b[0;31m                              name, \"\".join(traceback.format_list(tb))))\n\u001b[0m\u001b[1;32m    734\u001b[0m       \u001b[0mfound_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfound_var\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Variable global_step already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"<ipython-input-8-df456ffc3140>\", line 10, in __init__\n    trainable=False)\n  File \"<ipython-input-51-4cb7201b34c4>\", line 16, in <module>\n    trainer = FeedForwardTrainer(preprocessor, loss_func, nstep_per_epoch)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "if FLAGS.eval:\n",
    "    if FLAGS.data_dir is None:\n",
    "        logger.error(\"eval requires data_dir to be specified\")\n",
    "        raise ValueError(\"eval requires data_dir to be specified\")\n",
    "    if FLAGS.fp16:\n",
    "        logger.error(\"eval cannot be run with in fp16\")\n",
    "        raise ValueError(\"eval cannot be run with in fp16\")\n",
    "    if hvd.size() > 1:\n",
    "        logger.error(\"Multi-GPU evaluation is not supported\")\n",
    "        raise ValueError(\"Multi-GPU evaluation is not supported\")\n",
    "    evaluator = FeedForwardEvaluator(preprocessor, eval_func)\n",
    "    logger.info(\"Building evaluation graph\")\n",
    "    top1_op, top5_op, enqueue_ops = evaluator.evaluation_step(batch_size)\n",
    "else:\n",
    "    nstep_per_epoch = nrecord // (batch_size * hvd.size())\n",
    "    trainer = FeedForwardTrainer(preprocessor, loss_func, nstep_per_epoch)\n",
    "    logger.info(\"Building training graph\")\n",
    "    total_loss, learning_rate, train_ops = trainer.training_step(\n",
    "        batch_size)\n",
    "\n",
    "logger.info(\"Creating session\")\n",
    "config = tf.ConfigProto(allow_soft_placement = True)\n",
    "config.intra_op_parallelism_threads = 1\n",
    "config.inter_op_parallelism_threads = 10\n",
    "config.gpu_options.force_gpu_compatible = True\n",
    "config.gpu_options.visible_device_list = str(hvd.local_rank())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-08 22:21:34,136 - aws-tf-hvd-cnn - INFO - Initializing variables\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session(config=config)\n",
    "\n",
    "train_writer = None\n",
    "saver = None\n",
    "summary_ops = None\n",
    "if hvd.rank() == 0 and len(FLAGS.log_dir):\n",
    "    log_dir = FLAGS.log_dir\n",
    "    train_writer = tf.summary.FileWriter(log_dir, sess.graph)\n",
    "    summary_ops = tf.summary.merge_all()\n",
    "    last_summary_time = time.time()\n",
    "    saver = tf.train.Saver(keep_checkpoint_every_n_hours=3)\n",
    "    last_save_time = time.time()\n",
    "\n",
    "if not FLAGS.eval:\n",
    "    logger.info(\"Initializing variables\")\n",
    "    trainer.init(sess)\n",
    "\n",
    "restored = False\n",
    "if hvd.rank() == 0 and saver is not None:\n",
    "    ckpt = tf.train.get_checkpoint_state(FLAGS.log_dir)\n",
    "    checkpoint_file = os.path.join(FLAGS.log_dir, \"checkpoint\")\n",
    "    if ckpt and ckpt.model_checkpoint_path:\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "        restored = True\n",
    "        logger.info(\"Restored session from checkpoint {}\".format(ckpt.model_checkpoint_path))\n",
    "    else:\n",
    "        if not os.path.exists(FLAGS.log_dir):\n",
    "            os.mkdir(FLAGS.log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    " if FLAGS.eval:\n",
    "    if not restored:\n",
    "        logger.error(\"No checkpoint found for evaluation\")\n",
    "        raise ValueError(\"No checkpoint found for evaluation\")\n",
    "    else:\n",
    "        logger.info(\"Pre-filling input pipeline\")\n",
    "        evaluator.prefill_pipeline(sess)\n",
    "        nstep = nrecord // batch_size\n",
    "        run_evaluation(nstep, sess, top1_op, top5_op, enqueue_ops)\n",
    "        sys.exit(0)\n",
    "        \n",
    "\n",
    "trainer.sync(sess)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-08 23:06:04,687 - aws-tf-hvd-cnn - INFO - Pre-filling input pipeline\n",
      "2018-10-08 23:06:04,729 - aws-tf-hvd-cnn - INFO - Writing summaries to /home/ubuntu/examples/horovod/cnn/cnn_logdir\n",
      "2018-10-08 23:06:04,730 - aws-tf-hvd-cnn - INFO - Training\n",
      "2018-10-08 23:06:04,731 - aws-tf-hvd-cnn - INFO -   Step Epoch Img/sec   Loss   LR\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint written to /home/ubuntu/examples/horovod/cnn/cnn_logdir/checkpoint-0\n"
     ]
    }
   ],
   "source": [
    "if hvd.rank() == 0 and not restored:\n",
    "    if saver is not None:\n",
    "        save_path = saver.save(sess, checkpoint_file, global_step=0)\n",
    "        print(\"Checkpoint written to\", save_path)\n",
    "\n",
    "logger.info(\"Pre-filling input pipeline\")\n",
    "trainer.prefill_pipeline(sess)\n",
    "logger.info(\"Writing summaries to {}\".format(FLAGS.log_dir))\n",
    "logger.info(\"Training\")\n",
    "logger.info(\"  Step Epoch Img/sec   Loss   LR\")\n",
    "batch_times = []\n",
    "oom = False\n",
    "step0 = int(sess.run(trainer.global_step))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-08 23:08:37,655 - aws-tf-hvd-cnn - INFO -      1     1     2.8   7.369 0.04000\n",
      "2018-10-08 23:08:40,070 - aws-tf-hvd-cnn - INFO -      2     1    26.5   7.369 0.03842\n",
      "2018-10-08 23:08:40,560 - aws-tf-hvd-cnn - INFO -      3     1   130.8   7.369 0.03686\n",
      "2018-10-08 23:08:41,054 - aws-tf-hvd-cnn - INFO -      4     1   129.7   7.369 0.03534\n",
      "2018-10-08 23:08:41,549 - aws-tf-hvd-cnn - INFO -      5     1   129.8   7.369 0.03386\n",
      "2018-10-08 23:08:42,045 - aws-tf-hvd-cnn - INFO -      6     1   129.4   7.369 0.03240\n",
      "2018-10-08 23:08:42,541 - aws-tf-hvd-cnn - INFO -      7     1   129.4   7.369 0.03098\n",
      "2018-10-08 23:08:43,038 - aws-tf-hvd-cnn - INFO -      8     1   129.0   7.369 0.02958\n",
      "2018-10-08 23:08:43,535 - aws-tf-hvd-cnn - INFO -      9     1   129.0   7.369 0.02822\n",
      "2018-10-08 23:08:44,033 - aws-tf-hvd-cnn - INFO -     10     1   129.0   7.369 0.02690\n",
      "2018-10-08 23:08:44,528 - aws-tf-hvd-cnn - INFO -     11     1   129.7   7.369 0.02560\n",
      "2018-10-08 23:08:45,024 - aws-tf-hvd-cnn - INFO -     12     1   129.2   7.369 0.02434\n",
      "2018-10-08 23:08:45,522 - aws-tf-hvd-cnn - INFO -     13     1   129.0   7.369 0.02310\n",
      "2018-10-08 23:08:46,016 - aws-tf-hvd-cnn - INFO -     14     1   129.7   7.369 0.02190\n",
      "2018-10-08 23:08:46,513 - aws-tf-hvd-cnn - INFO -     15     1   129.0   7.369 0.02074\n",
      "2018-10-08 23:08:47,010 - aws-tf-hvd-cnn - INFO -     16     1   129.0   7.369 0.01960\n",
      "2018-10-08 23:08:47,509 - aws-tf-hvd-cnn - INFO -     17     1   128.7   7.369 0.01850\n",
      "2018-10-08 23:08:48,001 - aws-tf-hvd-cnn - INFO -     18     1   130.3   7.368 0.01742\n",
      "2018-10-08 23:08:48,500 - aws-tf-hvd-cnn - INFO -     19     1   128.6   7.368 0.01638\n",
      "2018-10-08 23:08:48,994 - aws-tf-hvd-cnn - INFO -     20     1   130.0   7.369 0.01538\n",
      "2018-10-08 23:08:49,487 - aws-tf-hvd-cnn - INFO -     21     1   130.0   7.369 0.01440\n",
      "2018-10-08 23:08:49,981 - aws-tf-hvd-cnn - INFO -     22     1   129.8   7.369 0.01346\n",
      "2018-10-08 23:08:50,481 - aws-tf-hvd-cnn - INFO -     23     1   128.4   7.368 0.01254\n",
      "2018-10-08 23:08:50,978 - aws-tf-hvd-cnn - INFO -     24     1   129.1   7.369 0.01166\n",
      "2018-10-08 23:08:51,474 - aws-tf-hvd-cnn - INFO -     25     1   129.2   7.369 0.01082\n",
      "2018-10-08 23:08:51,968 - aws-tf-hvd-cnn - INFO -     26     1   130.0   7.369 0.01000\n",
      "2018-10-08 23:08:52,464 - aws-tf-hvd-cnn - INFO -     27     1   129.3   7.369 0.00922\n",
      "2018-10-08 23:08:52,965 - aws-tf-hvd-cnn - INFO -     28     1   128.0   7.368 0.00846\n",
      "2018-10-08 23:08:53,463 - aws-tf-hvd-cnn - INFO -     29     1   128.7   7.368 0.00774\n",
      "2018-10-08 23:08:53,962 - aws-tf-hvd-cnn - INFO -     30     1   128.7   7.368 0.00706\n",
      "2018-10-08 23:08:54,460 - aws-tf-hvd-cnn - INFO -     31     1   128.7   7.368 0.00640\n",
      "2018-10-08 23:08:54,957 - aws-tf-hvd-cnn - INFO -     32     1   129.1   7.368 0.00578\n",
      "2018-10-08 23:08:55,455 - aws-tf-hvd-cnn - INFO -     33     1   128.9   7.368 0.00518\n",
      "2018-10-08 23:08:55,952 - aws-tf-hvd-cnn - INFO -     34     1   129.2   7.368 0.00462\n",
      "2018-10-08 23:08:56,449 - aws-tf-hvd-cnn - INFO -     35     1   129.0   7.368 0.00410\n",
      "2018-10-08 23:08:56,945 - aws-tf-hvd-cnn - INFO -     36     1   129.2   7.368 0.00360\n",
      "2018-10-08 23:08:57,445 - aws-tf-hvd-cnn - INFO -     37     1   128.4   7.368 0.00314\n",
      "2018-10-08 23:08:57,944 - aws-tf-hvd-cnn - INFO -     38     1   128.5   7.368 0.00270\n",
      "2018-10-08 23:08:58,442 - aws-tf-hvd-cnn - INFO -     39     1   128.9   7.368 0.00230\n",
      "2018-10-08 23:08:58,940 - aws-tf-hvd-cnn - INFO -     40     1   128.9   7.368 0.00194\n",
      "2018-10-08 23:08:59,435 - aws-tf-hvd-cnn - INFO -     41     1   129.6   7.368 0.00160\n",
      "2018-10-08 23:08:59,929 - aws-tf-hvd-cnn - INFO -     42     1   129.7   7.368 0.00130\n",
      "2018-10-08 23:09:00,424 - aws-tf-hvd-cnn - INFO -     43     1   129.6   7.368 0.00102\n",
      "2018-10-08 23:09:00,924 - aws-tf-hvd-cnn - INFO -     44     1   128.5   7.368 0.00078\n",
      "2018-10-08 23:09:01,422 - aws-tf-hvd-cnn - INFO -     45     1   128.8   7.368 0.00058\n",
      "2018-10-08 23:09:01,921 - aws-tf-hvd-cnn - INFO -     46     1   128.4   7.368 0.00040\n",
      "2018-10-08 23:09:02,420 - aws-tf-hvd-cnn - INFO -     47     1   128.6   7.368 0.00026\n",
      "2018-10-08 23:09:02,919 - aws-tf-hvd-cnn - INFO -     48     1   128.6   7.368 0.00014\n",
      "2018-10-08 23:09:03,420 - aws-tf-hvd-cnn - INFO -     49     1   127.9   7.368 0.00006\n",
      "2018-10-08 23:09:04,437 - aws-tf-hvd-cnn - INFO -     50     1   128.4   7.368 0.00002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint written to /home/ubuntu/examples/horovod/cnn/cnn_logdir/checkpoint-50\n"
     ]
    }
   ],
   "source": [
    "for step in range(step0, nstep):\n",
    "    ops_to_run = [total_loss, learning_rate] + train_ops\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        if (hvd.rank() == 0 and summary_ops is not None and\n",
    "            (step == 0 or step+1 == nstep or\n",
    "             time.time() - last_summary_time > FLAGS.summary_interval)):\n",
    "            if step != 0:\n",
    "                last_summary_time += FLAGS.summary_interval\n",
    "            summary, loss, lr = sess.run([summary_ops] + ops_to_run)[:3]\n",
    "            train_writer.add_summary(summary, step)\n",
    "        else:\n",
    "            loss, lr = sess.run(ops_to_run)[:2]\n",
    "        elapsed = time.time() - start_time\n",
    "    except KeyboardInterrupt:\n",
    "        logger.info(\"Keyboard interrupt\")\n",
    "        break\n",
    "    except tf.errors.ResourceExhaustedError:\n",
    "        elapsed = -1.\n",
    "        loss    = 0.\n",
    "        lr      = -1\n",
    "        oom = True\n",
    "\n",
    "    if (hvd.rank() == 0 and saver is not None and\n",
    "        (time.time() - last_save_time > FLAGS.save_interval or step+1 == nstep)):\n",
    "        last_save_time += FLAGS.save_interval\n",
    "        save_path = saver.save(sess, checkpoint_file,\n",
    "                               global_step=trainer.global_step)\n",
    "        print(\"Checkpoint written to\", save_path)\n",
    "\n",
    "    if step >= FLAGS.nstep_burnin:\n",
    "        batch_times.append(elapsed)\n",
    "    img_per_sec = batch_size / elapsed\n",
    "    effective_accuracy = 100. / math.exp(min(loss,20.))\n",
    "    if step == 0 or (step+1) % FLAGS.display_every == 0:\n",
    "        epoch = step*batch_size*hvd.size() // nrecord\n",
    "        logger.info(\"%6i %5i %7.1f %7.3f %7.5f\" % (\n",
    "            step+1, epoch+1, img_per_sec*hvd.size(), loss, lr))\n",
    "    if oom:\n",
    "        break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-08 23:09:39,904 - aws-tf-hvd-cnn - INFO - ----------------------------------------------------------------\n",
      "2018-10-08 23:09:39,905 - aws-tf-hvd-cnn - INFO - Images/sec: 128.9 +/- 0.1 (jitter = 0.5)\n",
      "2018-10-08 23:09:39,906 - aws-tf-hvd-cnn - INFO - ----------------------------------------------------------------\n",
      "2018-10-08 23:09:39,907 - aws-tf-hvd-cnn - INFO - Time used in total: 4445.9 seconds\n"
     ]
    }
   ],
   "source": [
    "nstep = len(batch_times)\n",
    "if nstep > 0:\n",
    "    batch_times = np.array(batch_times)\n",
    "    speeds = batch_size*hvd.size() / batch_times\n",
    "    speed_mean = np.mean(speeds)\n",
    "    if nstep > 2:\n",
    "        speed_uncertainty = np.std(speeds, ddof=1) / np.sqrt(float(nstep))\n",
    "    else:\n",
    "        speed_uncertainty = float('nan')\n",
    "    speed_madstd = 1.4826*np.median(np.abs(speeds - np.median(speeds)))\n",
    "    speed_jitter = speed_madstd\n",
    "    logger.info('-' * 64)\n",
    "    logger.info('Images/sec: %.1f +/- %.1f (jitter = %.1f)' % (\n",
    "        speed_mean, speed_uncertainty, speed_jitter))\n",
    "    logger.info('-' * 64)\n",
    "else:\n",
    "    logger.info(\"No results, did not get past burn-in phase (%i steps)\" %\n",
    "          FLAGS.nstep_burnin)\n",
    "\n",
    "if train_writer is not None:\n",
    "    train_writer.close()\n",
    "\n",
    "global_end_time = time.time()\n",
    "#logger.info(\"start time is {}, end time is {}\".format(global_start_time, global_end_time))\n",
    "logger.info('Time used in total: %.1f seconds' % (global_end_time - global_start_time))\n",
    "\n",
    "if oom:\n",
    "    print(\"Out of memory error detected, exiting\")\n",
    "    sys.exit(-2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow_p36]",
   "language": "python",
   "name": "conda-env-tensorflow_p36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
