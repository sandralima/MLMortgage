{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sandr\\AppData\\Local\\conda\\conda\\envs\\tensorflowenvironment\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C:\\\\Users\\\\sandr\\\\Documents\\\\GitHub\\\\MLMortgage\\\\src\\\\data', '', 'C:\\\\Users\\\\sandr\\\\AppData\\\\Local\\\\conda\\\\conda\\\\envs\\\\tensorflowenvironment\\\\python36.zip', 'C:\\\\Users\\\\sandr\\\\AppData\\\\Local\\\\conda\\\\conda\\\\envs\\\\tensorflowenvironment\\\\DLLs', 'C:\\\\Users\\\\sandr\\\\AppData\\\\Local\\\\conda\\\\conda\\\\envs\\\\tensorflowenvironment\\\\lib', 'C:\\\\Users\\\\sandr\\\\AppData\\\\Local\\\\conda\\\\conda\\\\envs\\\\tensorflowenvironment', 'C:\\\\Users\\\\sandr\\\\AppData\\\\Local\\\\conda\\\\conda\\\\envs\\\\tensorflowenvironment\\\\lib\\\\site-packages', 'C:\\\\Users\\\\sandr\\\\AppData\\\\Local\\\\conda\\\\conda\\\\envs\\\\tensorflowenvironment\\\\lib\\\\site-packages\\\\Mako-1.0.7-py3.6.egg', 'C:\\\\Users\\\\sandr\\\\AppData\\\\Local\\\\conda\\\\conda\\\\envs\\\\tensorflowenvironment\\\\lib\\\\site-packages\\\\win32', 'C:\\\\Users\\\\sandr\\\\AppData\\\\Local\\\\conda\\\\conda\\\\envs\\\\tensorflowenvironment\\\\lib\\\\site-packages\\\\win32\\\\lib', 'C:\\\\Users\\\\sandr\\\\AppData\\\\Local\\\\conda\\\\conda\\\\envs\\\\tensorflowenvironment\\\\lib\\\\site-packages\\\\Pythonwin', 'C:\\\\Users\\\\sandr\\\\AppData\\\\Local\\\\conda\\\\conda\\\\envs\\\\tensorflowenvironment\\\\lib\\\\site-packages\\\\IPython\\\\extensions', 'C:\\\\Users\\\\sandr\\\\.ipython']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import argparse\n",
    "import psutil\n",
    "import numpy as np\n",
    "import logging\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import glob\n",
    "from tensorflow.python.framework import ops\n",
    "import math\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "import ftplib\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "from scipy.stats.stats import pearsonr\n",
    "\n",
    "nb_dir = os.path.join(Path(os.getcwd()).parents[0], 'src', 'data')\n",
    "if nb_dir not in sys.path:\n",
    "    sys.path.insert(0, nb_dir)\n",
    "print(sys.path)\n",
    "import features_selection as fs\n",
    "import make_dataset as md\n",
    "import build_data as bd\n",
    "import get_raw_data as grd\n",
    "\n",
    "models_dir = os.path.join(Path(os.getcwd()).parents[0], 'src', 'models')\n",
    "if models_dir not in sys.path:\n",
    "    sys.path.insert(0, models_dir)\n",
    "import nn_real as nn\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "pd.options.display.max_columns = 500\n",
    "pd.options.display.max_rows = 500\n",
    "pd.set_option('io.hdf.default_format','table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sandr\\Documents\\GitHub\\MLMortgage\\data\\raw C:\\Users\\sandr\\Documents\\GitHub\\MLMortgage\\data\\processed\n"
     ]
    },
    {
     "ename": "HDF5ExtError",
     "evalue": "HDF5 error back trace\n\n  File \"C:\\bld\\hdf5_1519268879329\\work\\hdf5-1.8.18\\src\\H5F.c\", line 604, in H5Fopen\n    unable to open file\n  File \"C:\\bld\\hdf5_1519268879329\\work\\hdf5-1.8.18\\src\\H5Fint.c\", line 1087, in H5F_open\n    unable to read superblock\n  File \"C:\\bld\\hdf5_1519268879329\\work\\hdf5-1.8.18\\src\\H5Fsuper.c\", line 294, in H5F_super_read\n    unable to load superblock\n  File \"C:\\bld\\hdf5_1519268879329\\work\\hdf5-1.8.18\\src\\H5AC.c\", line 1262, in H5AC_protect\n    H5C_protect() failed.\n  File \"C:\\bld\\hdf5_1519268879329\\work\\hdf5-1.8.18\\src\\H5C.c\", line 3574, in H5C_protect\n    can't load entry\n  File \"C:\\bld\\hdf5_1519268879329\\work\\hdf5-1.8.18\\src\\H5C.c\", line 7954, in H5C_load_entry\n    unable to load entry\n  File \"C:\\bld\\hdf5_1519268879329\\work\\hdf5-1.8.18\\src\\H5Fsuper_cache.c\", line 476, in H5F_sblock_load\n    truncated file: eof = 499029954, sblock->base_addr = 0, stored_eoa = 1150129075\n\nEnd of HDF5 error back trace\n\nUnable to open/create file 'C:\\Users\\sandr\\Documents\\GitHub\\MLMortgage\\data\\processed\\chuncks_random_c1mill\\allTrans_12millRandNoInv_25mill-train_.h5'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHDF5ExtError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-9fd644f0068e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;31m#To sum up the dataset per worker (assuming the same size of files per worker approximately):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[0mfiles_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_files_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m \u001b[0marchitecture\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marchitecture_settings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiles_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marchitecture\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-9fd644f0068e>\u001b[0m in \u001b[0;36marchitecture_settings\u001b[1;34m(files_dict)\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[0mtotal_records\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfiles_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m             \u001b[1;32mwith\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mHDFStore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mdataset_file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mok_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m                     \u001b[0mindex_length\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_storer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'/features'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_columns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\tensorflowenvironment\\lib\\site-packages\\pandas\\io\\pytables.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, path, mode, complevel, complib, fletcher32, **kwargs)\u001b[0m\n\u001b[0;32m    489\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fletcher32\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfletcher32\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    490\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filters\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 491\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    492\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    493\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__fspath__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\tensorflowenvironment\\lib\\site-packages\\pandas\\io\\pytables.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(self, mode, **kwargs)\u001b[0m\n\u001b[0;32m    602\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    603\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 604\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtables\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    605\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mIOError\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pragma: no cover\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    606\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;34m'can not be written'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\tensorflowenvironment\\lib\\site-packages\\tables\\file.py\u001b[0m in \u001b[0;36mopen_file\u001b[1;34m(filename, mode, title, root_uep, filters, **kwargs)\u001b[0m\n\u001b[0;32m    318\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    319\u001b[0m     \u001b[1;31m# Finally, create the File instance, and return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 320\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mroot_uep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    321\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    322\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\tensorflowenvironment\\lib\\site-packages\\tables\\file.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, filename, mode, title, root_uep, filters, **kwargs)\u001b[0m\n\u001b[0;32m    782\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    783\u001b[0m         \u001b[1;31m# Now, it is time to initialize the File extension\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 784\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_g_new\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    785\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    786\u001b[0m         \u001b[1;31m# Check filters and set PyTables format version for new files.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mtables\\hdf5extension.pyx\u001b[0m in \u001b[0;36mtables.hdf5extension.File._g_new\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mHDF5ExtError\u001b[0m: HDF5 error back trace\n\n  File \"C:\\bld\\hdf5_1519268879329\\work\\hdf5-1.8.18\\src\\H5F.c\", line 604, in H5Fopen\n    unable to open file\n  File \"C:\\bld\\hdf5_1519268879329\\work\\hdf5-1.8.18\\src\\H5Fint.c\", line 1087, in H5F_open\n    unable to read superblock\n  File \"C:\\bld\\hdf5_1519268879329\\work\\hdf5-1.8.18\\src\\H5Fsuper.c\", line 294, in H5F_super_read\n    unable to load superblock\n  File \"C:\\bld\\hdf5_1519268879329\\work\\hdf5-1.8.18\\src\\H5AC.c\", line 1262, in H5AC_protect\n    H5C_protect() failed.\n  File \"C:\\bld\\hdf5_1519268879329\\work\\hdf5-1.8.18\\src\\H5C.c\", line 3574, in H5C_protect\n    can't load entry\n  File \"C:\\bld\\hdf5_1519268879329\\work\\hdf5-1.8.18\\src\\H5C.c\", line 7954, in H5C_load_entry\n    unable to load entry\n  File \"C:\\bld\\hdf5_1519268879329\\work\\hdf5-1.8.18\\src\\H5Fsuper_cache.c\", line 476, in H5F_sblock_load\n    truncated file: eof = 499029954, sblock->base_addr = 0, stored_eoa = 1150129075\n\nEnd of HDF5 error back trace\n\nUnable to open/create file 'C:\\Users\\sandr\\Documents\\GitHub\\MLMortgage\\data\\processed\\chuncks_random_c1mill\\allTrans_12millRandNoInv_25mill-train_.h5'"
     ]
    }
   ],
   "source": [
    "RAW_DIR = os.path.join(Path(os.getcwd()).parents[0], 'data', 'raw') \n",
    "PRO_DIR = os.path.join(Path(os.getcwd()).parents[0], 'data', 'processed')\n",
    "RANDOM_SEED = 123  # Set the seed to get reproducable results.\n",
    "DT_FLOAT = np.float32\n",
    "NP_FLOAT = np.dtype('float32')\n",
    "\n",
    "train_dir = 'chuncks_random_c1mill' #'chuncks_random_c1mill_slices' #'chuncks_random_c1millx2_train'\n",
    "valid_dir = 'chuncks_random_c1mill_valid'\n",
    "test_dir = ''\n",
    "train_period=[121,323] #[121,279] #[121, 143] \n",
    "valid_period=[324,329] #[280,285] #[144, 147] \n",
    "test_period=[330,342] #[286,304] #[148, 155]\n",
    "batch_size=100000\n",
    "\n",
    "print(RAW_DIR, PRO_DIR)\n",
    "\n",
    "def get_files_dict(train_dir, valid_dir, test_dir):        \n",
    "    ext = \"*.h5\"\n",
    "\n",
    "    files_dict = {'train': glob.glob(os.path.join(PRO_DIR, train_dir, ext)), \n",
    "                  'valid': glob.glob(os.path.join(PRO_DIR, valid_dir, ext)), \n",
    "                  'test': glob.glob(os.path.join(PRO_DIR, test_dir, ext))}\n",
    "\n",
    "    return files_dict\n",
    "\n",
    "def architecture_settings(files_dict):\n",
    "    architecture = {}    \n",
    "    ok_inputs = True\n",
    "    \n",
    "    for key in files_dict.keys():\n",
    "        total_records = 0\n",
    "        for file in files_dict[key]:                                \n",
    "            with pd.HDFStore(file) as dataset_file:\n",
    "                if (ok_inputs): \n",
    "                    index_length = len(dataset_file.get_storer(key+'/features').attrs.data_columns)\n",
    "                    architecture['n_input'] = dataset_file.get_storer(key+ '/features').ncols - index_length\n",
    "                    architecture['n_classes'] = dataset_file.get_storer(key+'/labels').ncols - index_length\n",
    "                    ok_inputs = False                \n",
    "                total_records += dataset_file.get_storer(key + '/features').nrows\n",
    "        architecture[key + '_num_examples'] = total_records                            \n",
    "    \n",
    "    architecture['total_num_examples'] = architecture['train_num_examples'] # 10000000\n",
    "    return architecture\n",
    "\n",
    "#To sum up the dataset per worker (assuming the same size of files per worker approximately):\n",
    "files_dict = get_files_dict(train_dir, valid_dir, test_dir)\n",
    "architecture = architecture_settings(files_dict)\n",
    "print(architecture)\n",
    "\n",
    "DATA = md.get_h5_data(PRO_DIR, architecture, train_dir, valid_dir, None, train_period=train_period, valid_period=None, test_period=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(DATA.train._dict[0]['nrows'])\n",
    "print(len(DATA.train.features_list), DATA.train.features_list)\n",
    "print(len(DATA.train.labels_list), DATA.train.labels_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (DATA.validation._dict):\n",
    "    print(DATA.validation._dict[0]['nrows'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = np.array(DATA.train._dict[0]['dataset_features'][:3000000]) #df_chunk.values\n",
    "#del DATA.train._dict[0]['dataset_features']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_Y = np.argmax(np.array(DATA.train._dict[0]['dataset_labels'][:3000000]), axis=1) #y.values\n",
    "#del DATA.train._dict[0]['dataset_labels']\n",
    "print(train_X.shape)\n",
    "print(train_Y.shape)\n",
    "print('Training Dataset shape %s' % Counter(train_Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X = np.array(DATA.validation._dict[0]['dataset_features'])\n",
    "#del DATA.validation._dict[0]['dataset_features']\n",
    "test_Y = np.argmax(np.array(DATA.validation._dict[0]['dataset_labels']), axis=1)\n",
    "#del DATA.validation._dict[0]['dataset_labels']\n",
    "print(test_X.shape)\n",
    "print(test_Y.shape)\n",
    "print('Validation Dataset shape %s' % Counter(test_Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "xg_train = xgb.DMatrix(train_X, label=train_Y)\n",
    "xg_test = xgb.DMatrix(test_X, label=test_Y)\n",
    "watchlist = [(xg_train, 'train'), (xg_test, 'test')]\n",
    "# setup parameters for xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {}\n",
    "# use softmax multi-class classification\n",
    "param['objective'] = 'multi:softmax'\n",
    "# scale weight of positive examples\n",
    "param['eta'] = 0.1 # 0.3\n",
    "param['max_depth'] = 6 #3\n",
    "param['silent'] = 1\n",
    "param['nthread'] = 4\n",
    "param['num_class'] = 7\n",
    "param['tree_method'] = 'gpu_hist' #'gpu_hist' > 'exact' == 'hist' > 'gpu_exact'\n",
    "# param['n_gpus'] = 1 #kernel died!\n",
    "param['eval_metric'] = ['mlogloss', 'merror'] # 'auc', 'aucpr',\n",
    "#param['max_delta_step'] = 1 #by default = 0\n",
    "num_round = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evals_result = {}\n",
    "bst = xgb.train(param, xg_train, num_round, watchlist, early_stopping_rounds=10, evals_result=evals_result) #  xgb_model=None, Validation error needs to decrease at least every early_stopping_rounds to continue training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bst)\n",
    "for e_name, e_mtrs in evals_result.items():\n",
    "    print('- {}'.format(e_name))\n",
    "    for e_mtr_name, e_mtr_vals in e_mtrs.items():\n",
    "        print('   - {}'.format(e_mtr_name))\n",
    "        print('      - {}'.format(e_mtr_vals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bst.best_score, bst.best_iteration)\n",
    "print(bst.best_ntree_limit)\n",
    "# print(xgb.get_xgb_params()) # get_params this is only for sklearn implementation of XGBoost!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get prediction:\n",
    "# pred = bst.predict(xg_test)\n",
    "# get the best prediction:\n",
    "pred = bst.predict(xg_test, ntree_limit=bst.best_ntree_limit)\n",
    "error_rate = np.sum(pred != test_Y) / test_Y.shape[0]\n",
    "print('Test error using softmax = {}'.format(error_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(test_Y, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "results = confusion_matrix(test_Y, pred, labels=[0, 1, 2, 3, 4, 5, 6])\n",
    "print('Confusion Matrix: ', results)\n",
    "#F1Score: \n",
    "print('F1Score-Macro: ', f1_score(test_Y, pred, average='macro'))\n",
    "\n",
    "print('F1Score-Micro: ', f1_score(test_Y, pred, average='micro'))\n",
    "\n",
    "print('F1Score-Weighted: ', f1_score(test_Y, pred, average='weighted'))\n",
    "\n",
    "print('F1Score-AverageNone: ', f1_score(test_Y, pred, average=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bst.save_model('allNoInvTrans_3mill_25mill_001.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dump model\n",
    "#bst.dump_model('dump.raw.txt')\n",
    "# dump model with feature map\n",
    "bst.dump_model('allNoInvTrans_3mill_25mill_dump.txt', 'allNoInvTrans_3mill_25mill_featmap.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the model:\n",
    "# bst = xgb.Booster({'nthread': 4})  # init model\n",
    "bst1 = xgb.Booster(model_file='allNoInvTrans_3mill_25mill_001.model')\n",
    "# bst.load_model('model.bin')  # load data\n",
    "\n",
    "#testing set:\n",
    "#dtest = xgb.DMatrix(data)\n",
    "#ypred = bst.predict(dtest)\n",
    "# If early stopping is enabled during training:\n",
    "ypred = bst.predict(xg_test, ntree_limit=bst.best_ntree_limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xgb.get_score())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to plot the importance of variables:\n",
    "import matplotlib.pyplot as plt\n",
    "xgb.plot_importance(bst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphviz \n",
    "# Specifying the ordinal number of the target tree. \n",
    "xgb.plot_tree(bst, num_trees=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot graphviz object:\n",
    "xgb.to_graphviz(bst, num_trees=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
