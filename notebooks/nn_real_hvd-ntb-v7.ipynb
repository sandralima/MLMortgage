{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beginning Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel package will be renamed from release 2.8; in order to keep installing from binary please use \"pip install psycopg2-binary\" instead. For details see: <http://initd.org/psycopg/docs/install.html#binary-install-from-pypi>.\n",
      "  \"\"\")\n",
      "2018-11-13 19:33:25,543 - matplotlib - DEBUG - CACHEDIR=/home/ubuntu/.cache/matplotlib\n",
      "2018-11-13 19:33:25,546 - matplotlib.font_manager - DEBUG - Using fontManager instance from /home/ubuntu/.cache/matplotlib/fontList.json\n",
      "2018-11-13 19:33:25,677 - matplotlib.backends - DEBUG - backend module://ipykernel.pylab.backend_inline version unknown\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/ubuntu/MLMortgage/src/data', '', '/home/ubuntu/src/cntk/bindings/python', '/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python36.zip', '/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6', '/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/lib-dynload', '/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages', '/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/IPython/extensions', '/home/ubuntu/.ipython']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-13 19:33:25,913 - matplotlib.backends - DEBUG - backend module://ipykernel.pylab.backend_inline version unknown\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import argparse\n",
    "import psutil\n",
    "import numpy as np\n",
    "import logging\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import glob\n",
    "from tensorflow.python.framework import ops\n",
    "import math\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "import ftplib\n",
    "\n",
    "\n",
    "nb_dir = os.path.join(Path(os.getcwd()).parents[0], 'src', 'data')\n",
    "if nb_dir not in sys.path:\n",
    "    sys.path.insert(0, nb_dir)\n",
    "print(sys.path)\n",
    "import features_selection as fs\n",
    "import make_dataset as md\n",
    "import build_data as bd\n",
    "import get_raw_data as grd\n",
    "import data_classes\n",
    "\n",
    "models_dir = os.path.join(Path(os.getcwd()).parents[0], 'src', 'models')\n",
    "if models_dir not in sys.path:\n",
    "    sys.path.insert(0, models_dir)\n",
    "import nn_real as nn\n",
    "\n",
    "try:\n",
    "    import horovod.tensorflow as hvd\n",
    "except:\n",
    "    print(\"Failed to import horovod module. \"\n",
    "          \"%s is intended for use with Uber's Horovod distributed training \"\n",
    "          \"framework. To create a Docker image with Horovod support see \"\n",
    "          \"docker-examples/Dockerfile.horovod.\" % __file__)\n",
    "    raise\n",
    "\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/MLMortgage/data/raw /home/ubuntu/MLMortgage/data/processed\n"
     ]
    }
   ],
   "source": [
    "RAW_DIR = os.path.join(Path(os.getcwd()).parents[0], 'data', 'raw') \n",
    "PRO_DIR = os.path.join(Path(os.getcwd()).parents[0], 'data', 'processed')\n",
    "RANDOM_SEED = 123  # Set the seed to get reproducable results.\n",
    "DT_FLOAT = tf.float32\n",
    "NP_FLOAT = np.dtype('float32')\n",
    "\n",
    "print(RAW_DIR, PRO_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hvd.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FLAGS_setting(FLAGS, net_number):\n",
    "    # To determine an optimal set of hyperparameters, see Section 11.4.2 of the\n",
    "    # deep learning book. Has (1) grid, (2) random, and (3) Bayesian\n",
    "    # model-based search methods.Swersky et al. have a paper mentioned in that\n",
    "    # section (published in 2014).\n",
    "\n",
    "    # Hyperparameters\n",
    "    # FLAGS.epoch_num = 2  # 14  # 17  # 35  # 15\n",
    "    #print(\"FLAGS.epoch_num\", FLAGS.epoch_num)\n",
    "    # FLAGS.batch_size = 141600 # 4425 # 4000  \n",
    "    FLAGS.dropout_keep = 0.9  # 0.9  # 0.95  # .75  # .6\n",
    "    # ### parameters for training optimizer.\n",
    "    #FLAGS.learning_rate = .1  # .075  # .15  # .25\n",
    "    FLAGS.momentum = .5  # used by the momentum SGD.\n",
    "\n",
    "    # ### parameters for inverse_time_decay\n",
    "    FLAGS.decay_rate = 1\n",
    "    FLAGS.decay_step = 800 * 4400 #steps_per_epoch 1 * 80000 #according to paper: 800 epochs\n",
    "    FLAGS.rate_min = .0015\n",
    "    # ### parameters for exponential_decay\n",
    "    # FLAGS.decay_base = .96  # .96\n",
    "    # FLAGS.decay_step = 15000  # 12320  # 4 * 8700\n",
    "\n",
    "    # ### parameters for regularization\n",
    "    FLAGS.reg_rate = .01 * 1e-3  # * 1e-3\n",
    "\n",
    "    FLAGS.batch_norm = True  # False  #\n",
    "    FLAGS.dropout = True\n",
    "    # A flag to show the results on the held-out test set. Keep this at False.\n",
    "    FLAGS.test_flag = True\n",
    "    FLAGS.xla = True  # False\n",
    "    FLAGS.stratified_flag = False\n",
    "    #FLAGS.batch_type = 'batch'    \n",
    "    FLAGS.weighted_sampling = False  # True  #\n",
    "    # FLAGS.logdir =  os.path.join(Path.home(), 'real_summaries')  # \n",
    "    #FLAGS.n_hidden = 3\n",
    "    #FLAGS.s_hidden = [200, 140, 140]\n",
    "    # FLAGS.allow_summaries = False\n",
    "    FLAGS.epoch_flag = 0    \n",
    "    \n",
    "    #FLAGS.max_epoch_size = 141600*70 #137 # -1\n",
    "    \n",
    "    FLAGS.valid_batch_size = 150000\n",
    "    FLAGS.test_batch_size = 1200000\n",
    "    \n",
    "    FLAGS.train_dir = 'chuncks_random_c1millx2_train'\n",
    "    FLAGS.valid_dir = 'chuncks_random_c1millx2_valid'\n",
    "    FLAGS.test_dir = 'chuncks_random_c1millx2_test'\n",
    "    FLAGS.train_period=[121,323] #[121,279] #[121, 143] \n",
    "    FLAGS.valid_period=[324,329] #[280,285] #[144, 147] \n",
    "    FLAGS.test_period=[330,342] #[286,304] #[148, 155]\n",
    "\n",
    "    FLAGS.epoch_num=4 \n",
    "    FLAGS.max_epoch_size=-1 \n",
    "    FLAGS.batch_size=4425*2 # two files per worker except at master!\n",
    "    FLAGS.lr_decay_policy       = 'time'\n",
    "    FLAGS.lr_decay_epochs       = 30\n",
    "    FLAGS.lr_decay_rate         = 0.1\n",
    "    FLAGS.lr_poly_power         = 2.\n",
    "    FLAGS.eval = False # True=Evaluation else Training\n",
    "    FLAGS.save_interval = 450\n",
    "    FLAGS.nstep_burnin = 20 # step from to count consuming time for a batch\n",
    "    FLAGS.summary_interval = 1800 # Time in seconds between saves of summary statistics\n",
    "    FLAGS.display_every = 100 # How often (in iterations) to print out running information\n",
    "    FLAGS.total_examples = 38500000 #-1 to training all dataset, otherwise the training will have a fixed length\n",
    "    \n",
    "    #Retrieveng from ftp:\n",
    "    FLAGS.ftp_dir = 'processed/c1mill'\n",
    "    \n",
    "    \n",
    "    if FLAGS.n_hidden < 0 : raise ValueError('The size of hidden layer must be at least 0')\n",
    "    if (FLAGS.n_hidden > 0) and (FLAGS.n_hidden != len(FLAGS.s_hidden)) : raise ValueError('Sizes in hidden layers should match!')\n",
    "    \n",
    "    if (net_number==0):\n",
    "        FLAGS.name ='default_settings'        \n",
    "    elif (net_number==1):\n",
    "        FLAGS.name ='Xworkers_1mill'\n",
    "        FLAGS.batch_layer_type = 'batch'        \n",
    "        \n",
    "    return FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNPARSED ['-f', '/run/user/1000/jupyter/kernel-b35e644c-fe1a-4004-b1e8-43aa59787fca.json']\n",
      "existent directory\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "FLAGS, UNPARSED = nn.update_parser(argparse.ArgumentParser())\n",
    "print(\"UNPARSED\", UNPARSED)\n",
    "FLAGS.logdir = Path(str('/home/ubuntu/summ_15ep_2wrk/'))\n",
    "if not os.path.exists(os.path.join(FLAGS.logdir)): #os.path.exists\n",
    "    os.makedirs(os.path.join(FLAGS.logdir))\n",
    "else:\n",
    "    print('existent directory')\n",
    "FLAGS = FLAGS_setting(FLAGS, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FLAGS Namespace(allow_summaries=True, batch_layer_type='batch', batch_norm=True, batch_size=8850, batch_type='layer', data_dir='/input_data', decay_rate=1, decay_step=3520000, display_every=100, dropout=True, dropout_keep=0.9, epoch_flag=0, epoch_num=4, eval=False, ftp_dir='processed/c1mill', learning_rate=0.1, log_file=None, logdir=PosixPath('/home/ubuntu/summ_15ep_2wrk'), loss_tolerance=0.0001, lr_decay_epochs=30, lr_decay_policy='time', lr_decay_rate=0.1, lr_poly_power=2.0, max_epoch_size=-1, momentum=0.5, n_hidden=3, name='Xworkers_1mill', nstep_burnin=20, rate_min=0.0015, reg_rate=1e-05, s_hidden=[200, 140, 140], save_interval=450, stratified_flag=False, summary_interval=1800, test_batch_size=1200000, test_dir='chuncks_random_c1millx2_test', test_flag=True, test_period=[330, 342], total_examples=38500000, train_dir='chuncks_random_c1millx2_train', train_period=[121, 323], valid_batch_size=150000, valid_dir='chuncks_random_c1millx2_valid', valid_period=[324, 329], weighted_sampling=False, xla=True)\n"
     ]
    }
   ],
   "source": [
    "print(\"FLAGS\", FLAGS) #you can change the FLAGS by adding the setting before this line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Builder, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPUNetworkBuilder(object):\n",
    "    \"\"\"This class provides convenient methods for constructing feed-forward\n",
    "    networks with internal data layout of 'NCHW'.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 # is_training,\n",
    "                 dtype=DT_FLOAT,\n",
    "                 activation='RELU',\n",
    "                 use_batch_norm=True,\n",
    "                 batch_norm_config = {'decay':   0.9,\n",
    "                                      'epsilon': 1e-4,\n",
    "                                      'scale':   True,\n",
    "                                      'zero_debias_moving_mean': False}):\n",
    "        self.dtype             = dtype\n",
    "        self.activation_func   = activation\n",
    "        # self.is_training       = is_training\n",
    "        self.use_batch_norm    = use_batch_norm\n",
    "        self.batch_norm_config = batch_norm_config\n",
    "        #self._layer_counts     = defaultdict(lambda: 0)        \n",
    "        \n",
    "    def variable_summaries(self, name, var, allow_summaries):\n",
    "        \"\"\"Create summaries for the given Tensor (for TensorBoard visualization (TB graphs)).\n",
    "            Calculate the mean, min, max, histogram and standardeviation for 'var' variable and save the information\n",
    "            in tf.summary.\n",
    "\n",
    "        Args: \n",
    "             name (String): the of the scope for summaring. For min, max and standardeviation 'calculate_std' is used as sub-scope.\n",
    "             var (Tensor): This is the tensor variable for building summaries.\n",
    "        Returns: \n",
    "            None\n",
    "        Raises:        \n",
    "        \"\"\"\n",
    "        if allow_summaries:\n",
    "            with tf.name_scope(name):\n",
    "                mean = tf.reduce_mean(var)\n",
    "                tf.summary.scalar('mean', mean)\n",
    "                with tf.name_scope('calculate_std'):\n",
    "                    stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "                tf.summary.scalar('stddev', stddev)\n",
    "                tf.summary.scalar('min', tf.reduce_min(var))\n",
    "                tf.summary.scalar('max', tf.reduce_max(var))\n",
    "                tf.summary.histogram('histogram', var)\n",
    "                \n",
    "    def _variable_on_cpu(self, name,\n",
    "                     shape,\n",
    "                     initializer=None,\n",
    "                     regularizer=None,\n",
    "                     dtype=DT_FLOAT):\n",
    "        \"\"\"Create a Variable or get an existing one stored on CPU memory.    \n",
    "\n",
    "        Args:\n",
    "            name (String): name of the variable.\n",
    "            shape (list of ints): Shape of the variable.\n",
    "            initializer: Default None. Initializer for Variable.\n",
    "            regularizer (A (Tensor -> Tensor or None) function): Default None. Regularizer for Variable.\n",
    "            dtype (TYPE): Type of the new variable.\n",
    "        Returns:\n",
    "            Variable Tensor\n",
    "        \"\"\"\n",
    "        with tf.device('/gpu:1'): # this operation is assigned to this device, but this make a copy of data when is transferred on and off the device, which is expensive.\n",
    "            var = tf.get_variable(\n",
    "                name,\n",
    "                shape,\n",
    "                initializer=initializer,\n",
    "                regularizer=regularizer,\n",
    "                dtype=dtype)\n",
    "        return var\n",
    "\n",
    "    def _create_variable(self, name,\n",
    "                         shape, allow_summaries, \n",
    "                         initializer=None,\n",
    "                         regularizer=None,\n",
    "                         dtype=DT_FLOAT):\n",
    "        \"\"\"Call _variable_on_cpu methods and variable_summaries for the 'name' tensor variable. \n",
    "\n",
    "        Args:\n",
    "            name (String): name of the variable.\n",
    "            shape (list of ints): Shape of the variable.\n",
    "            initializer: Default None. Initializer for Variable.\n",
    "            regularizer (A (Tensor -> Tensor or None) function): Default None. Regularizer for Variable.\n",
    "            dtype (TYPE): Type of the new variable.\n",
    "        Returns:\n",
    "            Variable Tensor\n",
    "        \"\"\"\n",
    "        var = self._variable_on_cpu(name, shape, initializer, regularizer, dtype)\n",
    "        self.variable_summaries(name + '/summaries', var, allow_summaries)\n",
    "        return var\n",
    "\n",
    "    def create_weights(self, name, shape, reg_rate, allow_summaries):\n",
    "        \"\"\"Create a Variable initialized with weights which are truncated normal distribution and regularized by\n",
    "        l1_regularizer (L1 regularization encourages sparsity, Regularization can help prevent overfitting).    \n",
    "\n",
    "        Args:\n",
    "            name (String): name of the variable.\n",
    "            shape (list of ints): Shape of the variable.\n",
    "        Returns:\n",
    "            Variable Tensor\n",
    "        \"\"\"    \n",
    "        dtype = DT_FLOAT\n",
    "        # kernel_initializer = tf.uniform_unit_scaling_initializer(\n",
    "        #     factor=1.43, dtype=DT_FLOAT)\n",
    "        # kernel_initializer = tf.contrib.layers.xavier_initializer(\n",
    "        #     uniform=True, dtype=DT_FLOAT)\n",
    "        kernel_initializer = tf.truncated_normal_initializer(\n",
    "            stddev=(1.0 / np.sqrt(shape[0])), dtype=dtype)\n",
    "\n",
    "        regularizer = tf.contrib.layers.l2_regularizer(\n",
    "            np.float32(reg_rate), 'penalty')\n",
    "        return self._create_variable(name, shape, allow_summaries, kernel_initializer, regularizer,\n",
    "                                dtype)\n",
    "\n",
    "    def bias_variable(self, name, shape, layer_name, weighted_sampling): # FLAGS.weighted_sampling\n",
    "        \"\"\"Create a bias variable with appropriate initialization. In case of FLAGS.weighted_sampling==False\n",
    "        and layer_name contains 'soft' the bias variable will contain a np.array of Negative values. Otherwise\n",
    "        the bias variable will be initialized in zero.\n",
    "\n",
    "        Args:\n",
    "            name (String): name of the variable.\n",
    "            shape (list of ints): Shape of the variable.\n",
    "            layer_name (String): name of the layer.\n",
    "        Returns:\n",
    "            Variable Tensor.\n",
    "        \"\"\"\n",
    "        def initial_bias(layer_name):\n",
    "            \"\"\"Get the initial value for the bias of the layer with layer_name.\"\"\"\n",
    "            if (not weighted_sampling) and 'soft' in layer_name:\n",
    "                return np.array(\n",
    "                    [-4.66, -3.81, -4.81, -3.90, -0.08, -3.90, -7.51],\n",
    "                    dtype=NP_FLOAT) + NP_FLOAT(4.1)\n",
    "            return 0.0\n",
    "\n",
    "        initial_value = initial_bias(layer_name)\n",
    "        with tf.name_scope(name) as scope:\n",
    "            initial = tf.constant(initial_value, shape=shape)\n",
    "            bias = tf.Variable(initial, name=scope)\n",
    "            self.variable_summaries('summaries', bias)\n",
    "        return bias        \n",
    "    \n",
    "    def dropout_layer(self, name, tensor_before, FLAGS):\n",
    "        \"\"\"Compute dropout to tensor_before with name scoping and a placeholder for keep_prob. \n",
    "        With probability keep_prob, outputs the input element scaled up by 1 / keep_prob, otherwise outputs 0.\n",
    "\n",
    "        Args:\n",
    "            name (String): name of the scope.\n",
    "            tensor_before (Tensor): Variable Tensor.        \n",
    "        Returns:\n",
    "            Variable Tensor of the same shape of tensor_before.\n",
    "        \"\"\"   \n",
    "        if not FLAGS.dropout:\n",
    "            print('There is not dropout for' + name)\n",
    "            return tensor_before\n",
    "        with tf.name_scope(name) as scope:\n",
    "            keep_prob = tf.placeholder(DT_FLOAT, None, name='keep_proba')\n",
    "            tf.summary.scalar('keep_probability', keep_prob)\n",
    "            dropped = tf.nn.dropout(tensor_before, keep_prob=keep_prob, name=scope)\n",
    "            self.variable_summaries('input_dropped_out', dropped, FLAGS.allow_summaries)\n",
    "        return dropped\n",
    "\n",
    "    def batch_normalization(self, name, input_tensor, train_flag, FLAGS):\n",
    "        \"\"\"Perform batch normalization over the input tensor.\n",
    "        Batch normalization helps avoid overfitting and we're able to use more\n",
    "        aggressive (larger) learning rates, resulting in faster convergence.\n",
    "        training parameter: Either a Python boolean, or a TensorFlow boolean scalar tensor (e.g. a placeholder). \n",
    "        Whether to return the output in training mode (normalized with statistics of the current batch) or in \n",
    "        inference mode (normalized with moving statistics). NOTE: make sure to set this parameter correctly, \n",
    "        or else your training/inference will not work properly.\n",
    "\n",
    "        Args:\n",
    "            name (String): name of the scope and the name of the layer.\n",
    "            input_tensor (Tensor): Variable Tensor.        \n",
    "        Returns:\n",
    "            Variable Tensor # the same shape of input_tensor??.\n",
    "        \"\"\"\n",
    "        # if not FLAGS.batch_norm:\n",
    "        #     return input_tensor\n",
    "        # train_flag = tf.get_default_graph().get_tensor_by_name('train_flag:0')\n",
    "        with tf.name_scope(name):\n",
    "            normalized = tf.layers.batch_normalization(\n",
    "                input_tensor,\n",
    "                center=True,\n",
    "                scale=True,\n",
    "                training=train_flag,\n",
    "                name=name)  # renorm=True, renorm_momentum=0.99)\n",
    "            self.variable_summaries('normalized_batch', normalized, FLAGS.allow_summaries)\n",
    "        return normalized\n",
    "\n",
    "    def layer_normalization(self, name, input_tensor, FLAGS):\n",
    "        \"\"\"Perform layer normalization.\n",
    "\n",
    "        Layer normalization helps avoid overfitting and we're able to use more\n",
    "        aggressive (larger) learning rates, resulting in faster convergence.\n",
    "        Can be used as a normalizer function for conv2d and fully_connected.\n",
    "\n",
    "        Given a tensor inputs of rank R, moments are calculated and normalization \n",
    "        is performed over axes begin_norm_axis ... R - 1. \n",
    "        Scaling and centering, if requested, is performed over axes begin_params_axis .. R - 1.\n",
    "        \"\"\"\n",
    "        # if not FLAGS.batch_norm:\n",
    "        #     return input_tensor\n",
    "        with tf.name_scope(name):\n",
    "            normalized = tf.contrib.layers.layer_norm(\n",
    "                input_tensor, center=True, scale=True, scope=name)\n",
    "            self.variable_summaries('normalized_layer', normalized, FLAGS.allow_summaries)\n",
    "        return normalized\n",
    "\n",
    "\n",
    "    def normalize(self, name, input_tensor, train_flag, FLAGS):\n",
    "        \"\"\"Perform either type (batch/layer) of normalization.\"\"\"\n",
    "        if not FLAGS.batch_norm:\n",
    "            return input_tensor\n",
    "        if FLAGS.batch_type.lower() == 'batch':\n",
    "            return self.batch_normalization(name, input_tensor, train_flag, FLAGS)\n",
    "        if FLAGS.batch_type.lower() == 'layer':\n",
    "            return self.layer_normalization(name, input_tensor, FLAGS)\n",
    "        raise ValueError('Invalid value for batch_type: ' + FLAGS.batch_type)\n",
    "\n",
    "    def nn_layer(self, input_tensor, output_dim, layer_name, FLAGS, act, train_flag):\n",
    "        \"\"\"Create a simple neural net layer.\n",
    "\n",
    "        It performs the affine transformation and uses the activation function to\n",
    "        nonlinearize. It further sets up name scoping so that the resultant graph\n",
    "        is easy to read, and adds a number of summary ops.\n",
    "        \"\"\"\n",
    "        input_dim = input_tensor.shape[1].value    \n",
    "        with tf.variable_scope(layer_name): # A context manager for defining ops that creates variables (layers).\n",
    "            weights = self.create_weights('weights', [input_dim, output_dim], FLAGS.reg_rate, FLAGS.allow_summaries)\n",
    "            # This is outdated and no longer applies: Do not change the order of\n",
    "            # batch normalization and drop out. batch # normalization has to stay\n",
    "            # __before__ the drop out layer.\n",
    "            self.variable_summaries('input', input_tensor, FLAGS.allow_summaries)\n",
    "            input_tensor = self.dropout_layer('dropout', input_tensor, FLAGS)\n",
    "            with tf.name_scope('mix'):\n",
    "                mixed = tf.matmul(input_tensor, weights)\n",
    "                tf.summary.histogram('maybe_guassian', mixed)\n",
    "            # Batch or layer normalization has to stay __after__ the affine\n",
    "            # transformation (the bias term doens't really matter because of the\n",
    "            # beta term in the normalization equation).\n",
    "            # See pp. 5 of the batch normalization paper:\n",
    "            # ```We add the BN transform immediately before the nonlinearity, by\n",
    "            # normalizing x = W u + b```\n",
    "            # biases = bias_variable('biases', [output_dim], layer_name)\n",
    "            preactivate = self.normalize('layer_normalization', mixed, train_flag, FLAGS)  # + biases\n",
    "            # tf.summary.histogram('pre_activations', preactivate)\n",
    "            # preactivate = dropout_layer('dropout', preactivate)\n",
    "            with tf.name_scope('activation') as scope:\n",
    "                activations = self.activate(preactivate, funcname=act)\n",
    "                tf.summary.histogram('activations', activations)\n",
    "        return activations        \n",
    "    \n",
    "    def activate(self, input_layer, funcname=None):\n",
    "        \"\"\"Applies an activation function\"\"\"\n",
    "        if isinstance(funcname, tuple):\n",
    "            funcname = funcname[0]\n",
    "            params = funcname[1:]\n",
    "        if funcname is None:\n",
    "            funcname = self.activation_func\n",
    "        if funcname == 'LINEAR':\n",
    "            return input_layer\n",
    "        activation_map = {\n",
    "            'IDENT':   tf.identity,\n",
    "            'RELU':    tf.nn.relu,\n",
    "            'RELU6':   tf.nn.relu6,\n",
    "            'ELU':     tf.nn.elu,\n",
    "            'SIGMOID': tf.nn.sigmoid,\n",
    "            'TANH':    tf.nn.tanh,\n",
    "            'LRELU':   lambda x, name: tf.maximum(params[0]*x, x, name=name)\n",
    "        }\n",
    "        return activation_map[funcname](input_layer, name=funcname.lower())\n",
    "    \n",
    "    def add_hidden_layers(self, features, architecture, FLAGS, train_flag, act=None):\n",
    "        \"\"\"Add hidden layers to the model using the architecture parameters.\"\"\"\n",
    "        hidden_out = features\n",
    "        jit_scope = tf.contrib.compiler.jit.experimental_jit_scope #JIT compiler compiles and runs parts of TF graphs via XLA, fusing multiple operators (kernel fusion) nto a small number of compiled kernels.\n",
    "        with jit_scope(): #this operation will be compiled with XLA.\n",
    "            for hid_i in range(1, FLAGS.n_hidden + 1):\n",
    "                hidden_out = self.nn_layer(hidden_out,\n",
    "                                      architecture['n_hidden_{:1d}'.format(hid_i)],\n",
    "                                      '{:1d}_hidden'.format(hid_i), FLAGS, act, train_flag)\n",
    "        return hidden_out        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(object):\n",
    "    \n",
    "    def __init__(self, func, nstep_per_epoch=None, dtype='trainer'):\n",
    "        \n",
    "        if dtype == 'trainer':            \n",
    "            self.nstep_per_epoch = nstep_per_epoch\n",
    "            #self.architecture = architecture\n",
    "            #self.FLAGS = FLAGS\n",
    "            with tf.device('/cpu:0'):\n",
    "                #self.global_step = tf.contrib.framework.get_or_create_global_step()\n",
    "                # tf.train.get_global_step()\n",
    "                self.global_step = tf.get_variable(\n",
    "                    'global_step', [],\n",
    "                    initializer=tf.constant_initializer(0),\n",
    "                    dtype=tf.int64,\n",
    "                    trainable=False)\n",
    "        elif dtype != 'evaluator': #Evaluator\n",
    "            raise ValueError('Invalid dtype value: ' + dtype)\n",
    "\n",
    "        self.func = func\n",
    "        self.dtype = dtype\n",
    "\n",
    "    def get_learning_rate(self, initial_learning_rate):\n",
    "        \"\"\"Get the learning rate.\"\"\"\n",
    "        with tf.name_scope('learning_rate') as scope:\n",
    "            if FLAGS.lr_decay_policy == 'poly':\n",
    "                return tf.train.polynomial_decay(\n",
    "                                        initial_learning_rate,\n",
    "                                        self.global_step,\n",
    "                                        decay_steps=FLAGS.epoch_num*self.nstep_per_epoch,\n",
    "                                        end_learning_rate=0.,\n",
    "                                        power=FLAGS.lr_poly_power,\n",
    "                                        cycle=False)\n",
    "            elif FLAGS.lr_decay_policy == 'exp':\n",
    "                return tf.train.exponential_decay(\n",
    "                                        initial_learning_rate,\n",
    "                                        self.global_step,\n",
    "                                        decay_steps=FLAGS.lr_decay_epochs*self.nstep_per_epoch,\n",
    "                                        decay_rate=FLAGS.lr_decay_rate,\n",
    "                                        staircase=True)\n",
    "            else:            \n",
    "                # decayed_lr = tf.train.exponential_decay(\n",
    "                #     initial_learning_rate,\n",
    "                #     global_step,\n",
    "                #     FLAGS.decay_step,\n",
    "                #     FLAGS.decay_base,\n",
    "                #     staircase=False)\n",
    "                decayed_lr = tf.train.inverse_time_decay(\n",
    "                    initial_learning_rate,\n",
    "                    self.global_step,\n",
    "                    decay_steps=FLAGS.decay_step,\n",
    "                    decay_rate=FLAGS.decay_rate)\n",
    "                final_lr = tf.clip_by_value(\n",
    "                    decayed_lr, FLAGS.rate_min, 1000, name=scope)\n",
    "                tf.summary.scalar('value', final_lr)\n",
    "                return final_lr\n",
    "        # return self.learning_rate \n",
    "\n",
    "    def get_accuracy(self, labels_int, logits, name):\n",
    "        \"\"\"Get the accuracy tensor.\"\"\"\n",
    "        with tf.name_scope(name) as scope:\n",
    "            # For a classifier model, we can use the in_top_k Op.\n",
    "            # It returns a bool tensor with shape [batch_size] that is true for\n",
    "            # the examples where the label is in the top k (here k=1)\n",
    "            # of all logits for that example.\n",
    "            correct = tf.nn.in_top_k(\n",
    "                logits, labels_int, 1, name='correct_prediction') # returns a tensor of type bool.\n",
    "            return tf.reduce_mean(tf.cast(correct, DT_FLOAT), name=scope)\n",
    "\n",
    "    # auc = get_auc(labels, probs, True, 'metrics/auc')\n",
    "    def get_auc(self, labels, scores, hist_flag, name):\n",
    "        \"\"\"Calculate the AUC of the two-way classifier for the given class.\"\"\"\n",
    "\n",
    "        def get_auc_using_histogram(labels, scores, class_, name):\n",
    "            \"\"\"Calculate the AUC.\"\"\"\n",
    "            class_ind = class_dict[class_.upper()]\n",
    "            with tf.name_scope(name) as scope:\n",
    "                auc, update_op = tf.contrib.metrics.auc_using_histogram( # his Op maintains Variables containing histograms of the scores associated with True and False labels. \n",
    "                    tf.cast(labels[:, class_ind], tf.bool),\n",
    "                    scores[:, class_ind],\n",
    "                    score_range=[0.0, 1.0],\n",
    "                    nbins=200,\n",
    "                    collections=None,\n",
    "                    name=scope)\n",
    "            ops.add_to_collections(ops.GraphKeys.UPDATE_OPS, update_op)\n",
    "            # print(update_op.name)\n",
    "            # print(auc) # it doesn't work because FailedPreconditionError (see above for traceback): Attempting to use uninitialized value metrics/auc/0//hist_accumulate/hist_true_acc\n",
    "            # aucp = tf.Print(auc,[auc], message='AUC the label: ' + class_) # it doesnt work because it doesnt run in a session\n",
    "            # print(aucp)\n",
    "            return auc\n",
    "\n",
    "        def get_auc_metric(labels, scores, class_, name):\n",
    "            \"\"\"Determine the AUC using conventional methods.\"\"\"\n",
    "            class_ind = class_dict[class_.upper()]\n",
    "            with tf.name_scope(name) as scope:\n",
    "                auc, _ = tf.metrics.auc( # Computes the approximate AUC via a Riemann sum.\n",
    "                    tf.cast(labels[:, class_ind], tf.bool), # ?? Print out!!\n",
    "                    scores[:, class_ind],\n",
    "                    weights=None,\n",
    "                    num_thresholds=200,\n",
    "                    metrics_collections=None,\n",
    "                    updates_collections=ops.GraphKeys.UPDATE_OPS,\n",
    "                    curve='ROC',\n",
    "                    name=scope)\n",
    "            # print(auc.op.name)\n",
    "            return auc\n",
    "\n",
    "        classes = ['0', '3', '6', '9', 'C', 'F', 'R']\n",
    "        class_dict = {classes[ind]: ind for ind in range(len(classes))}\n",
    "        if hist_flag:\n",
    "            auc_func = get_auc_using_histogram\n",
    "        else:\n",
    "            auc_func = get_auc_metric\n",
    "        with tf.name_scope(name) as scope:\n",
    "            aucv = [\n",
    "                    auc_func(labels, scores, class_, str(ind)) for ind, class_ in enumerate(classes) # pair (index ej. 0, value ej. '0')\n",
    "                   ]      \n",
    "            auc_values = tf.stack( # Pack along first dim\n",
    "                aucv,\n",
    "                axis=0,\n",
    "                name=scope)\n",
    "            # aucv = tf.Print(auc_values,[auc_values], message='AUC for all labels: ')\n",
    "            # print(aucv) # or maybe aucv.eval() or var = tf.Variable(aucv) and then var.eval(session=sess), or ovar = sess.run(var) but Attempting to use uninitialized value metrics/auc/Variable\n",
    "            return auc_values\n",
    "\n",
    "\n",
    "    # conf_mtx = get_confusion_matrix(labels_int, predictions, len(classes), 'metrics/confusion')\n",
    "    def get_confusion_matrix(self, labels_int, predictions, num_classes, name):\n",
    "        \"\"\"Get the confusion matrix.\n",
    "        Both prediction and labels must be 1-D arrays of the same shape in order for \n",
    "        this function to work.\n",
    "        \"\"\"\n",
    "        with tf.name_scope(name) as scope:\n",
    "            conf = tf.confusion_matrix(\n",
    "                labels_int,\n",
    "                predictions=predictions,\n",
    "                num_classes=num_classes,\n",
    "                dtype=tf.int32,\n",
    "                name=scope,\n",
    "                weights=None)\n",
    "        # print(conf.op.name)\n",
    "        return conf #return a K x K Matriz K = num_classes\n",
    "\n",
    "\n",
    "    def get_m_hand(self, labels, scores, name):\n",
    "        \"\"\"Implement the M measure described in Hand.\n",
    "\n",
    "        See ```A Simple Generalisation of the Area Under the ROC Curve for Multiple\n",
    "        Class Classification Problems``` Hand, Till 2001.    \n",
    "\n",
    "        \"\"\"\n",
    "        def get_auc_using_histogram(labels, scores, first_ind, second_ind, scope):\n",
    "            \"\"\"Calculate the AUC.\n",
    "            Calculate the AUC value by maintainig histograms of boolean variables (labels and \n",
    "            scores masked by the First-Second Individuals rule).\n",
    "            \"\"\"\n",
    "            mask = (labels[:, first_ind] + labels[:, second_ind]) > 0 #one in at least one column.\n",
    "            auc, update_op = tf.contrib.metrics.auc_using_histogram( # maintains variables containing histograms of the scores associated with True, False labels. \n",
    "                tf.cast(tf.boolean_mask(labels[:, first_ind], mask), tf.bool), # tf.boolean_mask: Apply boolean mask to tensor. Numpy equivalent is tensor[mask].\n",
    "                tf.boolean_mask(scores[:, first_ind], mask),\n",
    "                score_range=[0.0, 1.0],\n",
    "                nbins=500,\n",
    "                collections=None,\n",
    "                name=scope)\n",
    "            ops.add_to_collections(ops.GraphKeys.UPDATE_OPS, update_op)\n",
    "            # print(update_op.name)\n",
    "            return auc\n",
    "\n",
    "        temp_array = []\n",
    "        with tf.name_scope(name) as main_scope:\n",
    "            for first_ind in range(7):\n",
    "                for second_ind in range(7):\n",
    "                    if first_ind != second_ind:\n",
    "                        final_name = '{:d}{:d}'.format(first_ind, second_ind)\n",
    "                        with tf.name_scope(final_name) as scope:\n",
    "                            auc = get_auc_using_histogram(\n",
    "                                labels, scores, first_ind, second_ind, scope)\n",
    "                        temp_array.append(auc)\n",
    "            return tf.stack(temp_array, axis=0, name=main_scope) # Stacks a list of rank-R tensors into one rank-(R+1) tensor.\n",
    "\n",
    "\n",
    "    def get_auc_pr_curve(self, labels, scores, name, num_thresholds):    \n",
    "        with tf.name_scope(name) as scope:                             \n",
    "            AUC_PR = []\n",
    "            AUC_data = []\n",
    "            for i in range(7):  \n",
    "                data, update_op = tf.contrib.metrics.precision_recall_at_equal_thresholds(\n",
    "                                name='pr_data',\n",
    "                                predictions=scores[:, i],\n",
    "                                labels=tf.cast(labels[:, i], tf.bool),\n",
    "                                num_thresholds=10, use_locking=True)\n",
    "                ops.add_to_collections(ops.GraphKeys.UPDATE_OPS, update_op)\n",
    "                AUC_data.append((tf.stack(data.recall), tf.stack(data.precision), tf.stack(data.thresholds)))   # we cant use sklearn with tensorflow definition!\n",
    "                auc, _ = tf.metrics.auc(labels[:, i], scores[:, i], weights=None, num_thresholds=10, \n",
    "                                        curve='PR', updates_collections=ops.GraphKeys.UPDATE_OPS, metrics_collections=None, summation_method='careful_interpolation') # \n",
    "                # ops.add_to_collections(ops.GraphKeys.UPDATE_OPS, update_op)\n",
    "                AUC_PR.append(auc)\n",
    "            # print(AUC_data)\n",
    "            return tf.stack( # Pack the array of scalar tensor along one dim tensor\n",
    "                AUC_PR,\n",
    "                axis=0,\n",
    "                name=scope), AUC_data\n",
    "\n",
    "    def log_loss(self, labels, probs, name):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            labels: Labels tensor, int32 - [batch_size, n_classes], with one-hot\n",
    "            encoded values.\n",
    "            logits: Probabilities tensor, float32 - [batch_size, n_classes].\n",
    "        \"\"\"\n",
    "        with tf.name_scope(name) as scope:\n",
    "            total_loss = 0\n",
    "            for j in range(probs.shape[1].value):\n",
    "                loss = tf.losses.log_loss(labels[:, j], probs[:, j], loss_collection=None)\n",
    "                total_loss += loss\n",
    "\n",
    "            return tf.div(total_loss, np.float32(probs.shape[1].value), name=scope)\n",
    "    \n",
    "    def calculate_metrics(self, labels, logits):\n",
    "        \"\"\"Evaluate the quality of the logits at predicting the label.\n",
    "\n",
    "        Args:\n",
    "            labels: Labels tensor, int32 - [batch_size, n_classes], with one-hot\n",
    "            encoded values.\n",
    "            logits: Logits tensor, float32 - [batch_size, n_classes].\n",
    "        Returns:\n",
    "            A scalar float32 tensor with the fraction of examples (out of\n",
    "            batch_size) that were predicted correctly.\n",
    "        \"\"\"\n",
    "        classes = ['0', '3', '6', '9', 'C', 'F', 'R']\n",
    "        with tf.name_scope('metrics'):\n",
    "            labels_int = tf.argmax(labels, 1, name='intlabels') #tf.argmax: Returns the index with the largest value across axes=1 of a tensor.\t\t\n",
    "            predictions = tf.argmax(logits, 1, name='predictions')        \n",
    "            probs = tf.nn.softmax(logits, name='probs') # Computes softmax activations. softmax = tf.exp(logits) / tf.reduce_sum(tf.exp(logits), axis)        \n",
    "\n",
    "        m_list = self.get_m_hand(labels, probs, 'metrics/m_measure')\n",
    "        accuracy = self.get_accuracy(labels_int, logits, 'metrics/accuracy')    \n",
    "        auc = self.get_auc(labels, probs, True, 'metrics/auc')    \n",
    "        conf_mtx = self.get_confusion_matrix(labels_int, predictions,\n",
    "                                        len(classes), 'metrics/confusion')\n",
    "        lloss = self.log_loss(labels, probs, 'metrics/log_loss')\n",
    "        pr_auc, pr_data = self.get_auc_pr_curve(labels, probs, 'metrics/auc_pr', 200)\n",
    "\n",
    "        # this is for the definition of the graph:\n",
    "        return accuracy, conf_mtx, auc, m_list, lloss, pr_auc, pr_data\n",
    "    \n",
    "    def training_step(self, architecture, FLAGS):        \n",
    "        features = tf.placeholder(\n",
    "            DT_FLOAT, [None, architecture['n_input']], name='features')\n",
    "        labels = tf.placeholder(\n",
    "            DT_FLOAT, [None, architecture['n_classes']], name='targets')\n",
    "        # epoch_flag = tf.placeholder(tf.int32, None, name='epoch_flag')\n",
    "        example_weights = tf.placeholder(\n",
    "            DT_FLOAT, [None], name='example_weights')\n",
    "        \n",
    "        with tf.device('/gpu:0'):\n",
    "            # Evaluate the loss:\n",
    "            loss, logits = self.func(features, labels, example_weights, architecture, FLAGS)\n",
    "\n",
    "        with tf.device('/cpu:0'): # No in_top_k implem on GPU\n",
    "            accuracy, conf_mtx, auc_list, m_list, lloss, auc_pr, auc_data = self.calculate_metrics(labels, logits)\n",
    "            better_acc = tf.reduce_mean(tf.diag_part(conf_mtx / tf.reduce_sum(conf_mtx, axis=1, keepdims=True))) #recall\n",
    "            precision = tf.reduce_mean(tf.diag_part(conf_mtx / tf.reduce_sum(conf_mtx, axis=0, keepdims=True)))            \n",
    "            auc_mean = tf.reduce_mean(auc_list)\n",
    "            m_list_mean = tf.reduce_mean(m_list)\n",
    "            auc_pr_mean = tf.reduce_mean(auc_pr)\n",
    "            \n",
    "            with tf.name_scope('0_performance'):\n",
    "                # Scalar summaries to track the loss and accuracy over time in TB.\n",
    "                tf.summary.scalar('0accuracy', accuracy)\n",
    "                tf.summary.scalar('1better_accuracy', better_acc)\n",
    "                tf.summary.scalar('12precision', precision)\n",
    "                # tf.summary.scalar('f1score', f1score)\n",
    "                tf.summary.scalar('2auc_aoc', auc_mean)\n",
    "                tf.summary.scalar('3m_measure', m_list_mean)\n",
    "                tf.summary.scalar('4loss', loss)\n",
    "                tf.summary.scalar('5log_loss', lloss)\n",
    "                tf.summary.scalar('6auc_pr', auc_pr_mean)\n",
    "\n",
    "        # Apply the gradients to optimize the loss function\n",
    "        with tf.device('/gpu:0'):            \n",
    "            update_ops = ops.get_collection(ops.GraphKeys.UPDATE_OPS)\n",
    "            # print(update_ops)\n",
    "            with ops.control_dependencies(update_ops):\n",
    "                with tf.name_scope('train') as scope:\n",
    "                    # print_loss = tf.Print(loss, [loss], name='print_loss') \n",
    "\n",
    "                    # Create a variable to track the global step.\n",
    "        #            global_step = tf.get_variable(\n",
    "        #                'train/global_step',\n",
    "        #                shape=[],\n",
    "        #                initializer=tf.constant_initializer(0, dtype=tf.int32),\n",
    "        #                trainable=False)            \n",
    "                    # Horovod: adjust learning rate based on number of GPUs.\n",
    "                    # optimizer = tf.train.GradientDescentOptimizer(1.0 * hvd.size())\n",
    "                    final_learning_rate = self.get_learning_rate(FLAGS.learning_rate * hvd.size())\n",
    "\n",
    "                    # optimizer = tf.train.GradientDescentOptimizer(final_learning_rate)\n",
    "                    optimizer = tf.train.MomentumOptimizer(final_learning_rate, FLAGS.momentum, use_nesterov=True)\n",
    "                    # optimizer = tf.train.AdagradOptimizer(final_learning_rate)\n",
    "\n",
    "                    # Use the optimizer to apply the gradients that minimize the loss\n",
    "                    # (and increment the global step counter) as a single training step.\n",
    "        #            return optimizer.minimize(\n",
    "        #                loss, global_step=global_step, name=scope)\n",
    "                    optimizer = hvd.DistributedOptimizer(optimizer) #HVD!!\n",
    "                    train_op = optimizer.minimize(loss, global_step=self.global_step, name=scope)\n",
    "            \n",
    "                        \n",
    "        return train_op, final_learning_rate, conf_mtx, accuracy, better_acc, precision, auc_list, auc_mean, m_list, m_list_mean, loss, lloss, auc_pr, auc_pr_mean, auc_data\n",
    "\n",
    "    def evaluation_step(self, batch_size):\n",
    "        \n",
    "        if (self.dtype!='evaluator'):\n",
    "            raise ValueError('Invalid function for dtype: ' + self.dtype)\n",
    "            \n",
    "        features = tf.placeholder(\n",
    "            DT_FLOAT, [None, architecture['n_input']], name='features')\n",
    "        labels = tf.placeholder(\n",
    "            DT_FLOAT, [None, architecture['n_classes']], name='targets')\n",
    "        example_weights = tf.placeholder(\n",
    "            DT_FLOAT, [None], name='example_weights')\n",
    "\n",
    "        with tf.device('/gpu:0'):\n",
    "            # Evaluate the loss:\n",
    "            logits = self.func(features, labels, architecture, FLAGS)        \n",
    "            accuracy, conf_mtx = self.calculate_metrics(labels, logits)[:2]\n",
    "            better_acc = tf.reduce_mean(tf.diag_part(conf_mtx / tf.reduce_sum(conf_mtx, axis=1, keepdims=True)))\n",
    "            precision = tf.reduce_mean(tf.diag_part(conf_mtx / tf.reduce_sum(conf_mtx, axis=0, keepdims=True)))\n",
    "            #f1score = 2 * precision * better_acc / (precision + better_acc)\n",
    "            \n",
    "\n",
    "        return conf_mtx, accuracy, better_acc, precision #,f1score  #, auc_list, m_list, lloss, auc_pr, auc_data\n",
    "    \n",
    "    def init(self):\n",
    "        # init_op = tf.global_variables_initializer()\n",
    "        # sess.run(init_op)        \n",
    "        \"\"\"Add an Op to the graph to initialize the global and local variables.\"\"\"\n",
    "        with tf.name_scope('init') as scope:\n",
    "            with tf.name_scope('global'):\n",
    "                global_init = tf.global_variables_initializer()\n",
    "            with tf.name_scope('local'):\n",
    "                local_init = tf.local_variables_initializer()\n",
    "                # print(local_init.name)\n",
    "            #init_op = tf.group(global_init, local_init, name=scope)\n",
    "        return global_init, local_init\n",
    "        \n",
    "    def sync(self, sess):\n",
    "        sync_op = hvd.broadcast_global_variables(0)\n",
    "        sess.run(sync_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_func(features, labels, weights, architecture, FLAGS):\n",
    "    # Build the forward model\n",
    "    net = GPUNetworkBuilder(dtype=DT_FLOAT)\n",
    "    train_flag = tf.placeholder(tf.bool, None, name='train_flag')\n",
    "    with tf.name_scope('input_normalization') as scope:\n",
    "        feature_norm = features\n",
    "        net.variable_summaries('input_normalized', feature_norm, FLAGS.allow_summaries)\n",
    "    hidden_out = net.add_hidden_layers(feature_norm, architecture, FLAGS, train_flag)\n",
    "    # Linear output layer for the logits\n",
    "    logits = (net.nn_layer(hidden_out, architecture['n_classes'],'9_softmax_linear', FLAGS, 'IDENT', train_flag))\n",
    "    \n",
    "    with tf.name_scope('loss') as scope:\n",
    "        with tf.name_scope('regularization'):\n",
    "            penalty = tf.losses.get_regularization_loss(name='penalty') #Gets the total regularization loss from an optional scope name (sum for ol + 3h + 2h + 1h).\n",
    "            tf.summary.scalar('weight_norm', penalty / (1e-8 + FLAGS.reg_rate)) #for printing out\n",
    "        with tf.name_scope('cross_entropy') as xentropy_scope:\n",
    "            weighted_cross_entropy = tf.losses.softmax_cross_entropy(\n",
    "                onehot_labels=labels,\n",
    "                logits=logits,\n",
    "                weights=weights,  # 1.0,\n",
    "                scope=xentropy_scope,\n",
    "                loss_collection=ops.GraphKeys.LOSSES)\n",
    "            tf.summary.scalar('weighted_cross_entropy', weighted_cross_entropy)\n",
    "        loss= tf.add(weighted_cross_entropy, penalty, name=scope) # Returns x + y element-wise.    \n",
    "            \n",
    "    return loss, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_func(features, labels, architecture, FLAGS):\n",
    "    # Build the forward model\n",
    "    FLAGS.allow_summaries = False\n",
    "    net = GPUNetworkBuilder(dtype=DT_FLOAT)\n",
    "    train_flag = tf.placeholder(tf.bool, None, name='train_flag')    \n",
    "    hidden_out = net.add_hidden_layers(features, architecture, FLAGS, train_flag)\n",
    "    # Linear output layer for the logits\n",
    "    logits = (net.nn_layer(hidden_out, architecture['n_classes'],'9_softmax_linear', FLAGS, 'IDENT', train_flag))\n",
    "                \n",
    "    return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Logger Xworkers_1mill_0 (DEBUG)>\n"
     ]
    }
   ],
   "source": [
    "global_start_time = time.time()\n",
    "tf.set_random_seed(1234+hvd.rank())\n",
    "np.random.seed(4321+hvd.rank())\n",
    "\n",
    "# create logger:\n",
    "log_name = FLAGS.name + '_' + str(hvd.rank())\n",
    "logger = logging.getLogger(log_name)\n",
    "logger.setLevel(logging.DEBUG)  # INFO, ERROR\n",
    "# file handler which logs debug messages\n",
    "fh = logging.FileHandler(os.path.join(FLAGS.logdir, log_name + '.log'))\n",
    "fh.setLevel(logging.DEBUG)\n",
    "# console handler\n",
    "ch = logging.StreamHandler()\n",
    "ch.setLevel(logging.INFO)\n",
    "# add formatter to the handlers\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "fh.setFormatter(formatter)\n",
    "ch.setFormatter(formatter)\n",
    "# add handlers to logger\n",
    "logger.addHandler(fh)\n",
    "logger.addHandler(ch)\n",
    "print(logger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download the data if it has not been downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-13 19:33:26,366 - Xworkers_1mill_0 - INFO - FTP connection stablished by worker:  0\n",
      "2018-11-13 19:33:26,366 - Xworkers_1mill_0 - INFO - FTP connection stablished by worker:  0\n",
      "2018-11-13 19:33:26,586 - Xworkers_1mill_0 - INFO - FTP connection closed by worker:  0\n",
      "2018-11-13 19:33:26,586 - Xworkers_1mill_0 - INFO - FTP connection closed by worker:  0\n"
     ]
    }
   ],
   "source": [
    "def download_data(rank, FLAGS):\n",
    "    server = ftplib.FTP()\n",
    "    server.connect(str(os.environ.get(\"FTP_HOST\")), int(os.environ.get(\"FTP_PORT\")))\n",
    "    server.login(os.environ.get(\"FTP_USER\"), os.environ.get(\"FTP_PASS\"))\n",
    "\n",
    "    server.cwd(FLAGS.ftp_dir)               # change into ftp_dir directory\n",
    "    logger.info(\"FTP connection stablished by worker:  {}\".format(rank))\n",
    "    \n",
    "    filenames = server.nlst() # get filenames within the directory\n",
    "        \n",
    "    train_suffix = 'train_%d.h5' % rank\n",
    "    if (rank==0):\n",
    "        if FLAGS.eval:        \n",
    "            fname_suffix = 'test_%d.h5' % rank\n",
    "            filenames = [elem for elem in filenames if fname_suffix in elem]        \n",
    "        else:\n",
    "            valid_suffix = 'valid_%d.h5' % rank\n",
    "            filenames = [elem for elem in filenames if (train_suffix in elem or valid_suffix in elem)]                \n",
    "    else:\n",
    "        filenames = [elem for elem in filenames if (train_suffix in elem)]\n",
    "\n",
    "    for filename in filenames:        \n",
    "        if FLAGS.eval:\n",
    "            local_path = os.path.join(PRO_DIR, FLAGS.test_dir, filename)    \n",
    "        else:\n",
    "            if (str('train') in filename[-10:-5]):\n",
    "                local_path = os.path.join(PRO_DIR, FLAGS.train_dir, filename)    \n",
    "            elif (str('valid') in filename[-10:-5]):\n",
    "                local_path = os.path.join(PRO_DIR, FLAGS.valid_dir, filename)   \n",
    "            else: \n",
    "                continue\n",
    "                \n",
    "        if not os.path.exists(local_path):            \n",
    "            file = open(local_path, 'wb')\n",
    "            server.retrbinary('RETR '+ filename, file.write, 8*1024)            \n",
    "            file.close()\n",
    "            logger.info(\"file downloaded:  {}\".format(filename))\n",
    "\n",
    "    server.quit() # This is the “polite” way to close a connection\n",
    "    logger.info(\"FTP connection closed by worker:  {}\".format(rank))\n",
    "\n",
    "def download_data_by_rank(rank, FLAGS):\n",
    "    server = ftplib.FTP()\n",
    "    server.connect(str(os.environ.get(\"FTP_HOST\")), int(os.environ.get(\"FTP_PORT\")))\n",
    "    server.login(os.environ.get(\"FTP_USER\"), os.environ.get(\"FTP_PASS\"))\n",
    "\n",
    "    server.cwd(FLAGS.ftp_dir)               # change into ftp_dir directory\n",
    "    logger.info(\"FTP connection stablished by worker:  {}\".format(rank))\n",
    "    \n",
    "    filenames = server.nlst() # get filenames within the directory\n",
    "    fname_suffix = '_%d.h5' % rank\n",
    "    filenames = [elem for elem in filenames if fname_suffix in elem]        \n",
    "    \n",
    "    for filename in filenames:            \n",
    "        local_path = os.path.join(PRO_DIR, FLAGS.train_dir, filename)                    \n",
    "        if not os.path.exists(local_path):            \n",
    "            file = open(local_path, 'wb')\n",
    "            server.retrbinary('RETR '+ filename, file.write, 8*1024)            \n",
    "            file.close()\n",
    "            logger.info(\"file downloaded:  {}\".format(filename))\n",
    "\n",
    "    server.quit() \n",
    "    logger.info(\"FTP connection closed by worker:  {}\".format(rank))\n",
    "\n",
    "\n",
    "#download_data_by_rank(hvd.rank(), FLAGS)\n",
    "download_data(hvd.rank(), FLAGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_records(tf_record_pattern):\n",
    "    def count_records(file_name):\n",
    "        count = 0\n",
    "        for _ in tf.python_io.tf_record_iterator(tf_record_filename):\n",
    "            count += 1\n",
    "        return count\n",
    "    filenames = sorted(tf.gfile.Glob(tf_record_pattern))\n",
    "    nfile = len(filenames)\n",
    "    return (count_records(filenames[0])*(nfile-1) +\n",
    "            count_records(filenames[-1]))\n",
    "\n",
    "def get_files_dict(FLAGS):        \n",
    "    ext = \"*.h5\"\n",
    "\n",
    "    if (hvd.rank()==0):\n",
    "        files_dict = {'train': glob.glob(os.path.join(PRO_DIR, FLAGS.train_dir, ext)), \n",
    "                      'valid': glob.glob(os.path.join(PRO_DIR, FLAGS.valid_dir, ext)), \n",
    "                      'test': glob.glob(os.path.join(PRO_DIR, FLAGS.test_dir, ext))}\n",
    "    else:\n",
    "        files_dict = {'train': glob.glob(os.path.join(PRO_DIR, FLAGS.train_dir, ext))}\n",
    "\n",
    "    return files_dict\n",
    "\n",
    "def architecture_settings(files_dict, FLAGS):\n",
    "    architecture = {}\n",
    "    architecture['rank'] = hvd.rank()\n",
    "    ok_inputs = True\n",
    "    for key in files_dict.keys():\n",
    "        total_records = 0\n",
    "        for file in files_dict[key]:                                \n",
    "            with pd.HDFStore(file) as dataset_file:\n",
    "                if (ok_inputs): \n",
    "                    index_length = len(dataset_file.get_storer(key+'/features').attrs.data_columns)\n",
    "                    architecture['n_input'] = dataset_file.get_storer(key+ '/features').ncols - index_length\n",
    "                    architecture['n_classes'] = dataset_file.get_storer(key+'/labels').ncols - index_length\n",
    "                    ok_inputs = False                \n",
    "                total_records += dataset_file.get_storer(key + '/features').nrows\n",
    "        architecture[key + '_num_examples'] = total_records                            \n",
    "    \n",
    "    if FLAGS.eval:\n",
    "        architecture['total_num_examples'] = architecture['test_num_examples']\n",
    "    else:\n",
    "        if FLAGS.total_examples == -1:\n",
    "            architecture['total_num_examples'] = architecture['train_num_examples']\n",
    "        else:\n",
    "            architecture['total_num_examples'] = FLAGS.total_examples \n",
    "    \n",
    "    for hid_i in range(1, FLAGS.n_hidden+1):\n",
    "        architecture['n_hidden_{:1d}'.format(hid_i)] = FLAGS.s_hidden[hid_i-1]\n",
    "    # print('rank: ', hvd.rank(), 'architecture', architecture)   \n",
    "    # time.sleep(5)\n",
    "    return architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To sum up the dataset per worker (assuming the same size of files per worker approximately):\n",
    "files_dict = get_files_dict(FLAGS)\n",
    "architecture = architecture_settings(files_dict, FLAGS)\n",
    "\n",
    "nrecord = architecture['total_num_examples']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-13 19:33:26,687 - Xworkers_1mill_0 - INFO - Num ranks:  1\n",
      "2018-11-13 19:33:26,687 - Xworkers_1mill_0 - INFO - Num ranks:  1\n",
      "2018-11-13 19:33:26,689 - Xworkers_1mill_0 - INFO - Num of records: 38500000\n",
      "2018-11-13 19:33:26,689 - Xworkers_1mill_0 - INFO - Num of records: 38500000\n",
      "2018-11-13 19:33:26,691 - Xworkers_1mill_0 - INFO - Total batch size: 8850\n",
      "2018-11-13 19:33:26,691 - Xworkers_1mill_0 - INFO - Total batch size: 8850\n",
      "2018-11-13 19:33:26,693 - Xworkers_1mill_0 - INFO - 8850, per device\n",
      "2018-11-13 19:33:26,693 - Xworkers_1mill_0 - INFO - 8850, per device\n",
      "2018-11-13 19:33:26,696 - Xworkers_1mill_0 - INFO - Data type: <dtype: 'float32'>\n",
      "2018-11-13 19:33:26,696 - Xworkers_1mill_0 - INFO - Data type: <dtype: 'float32'>\n",
      "2018-11-13 19:33:26,698 - Xworkers_1mill_0 - INFO - architecture: {'rank': 0, 'n_input': 258, 'n_classes': 7, 'train_num_examples': 38912492, 'valid_num_examples': 2082911, 'test_num_examples': 6472386, 'total_num_examples': 38500000, 'n_hidden_1': 200, 'n_hidden_2': 140, 'n_hidden_3': 140}\n",
      "2018-11-13 19:33:26,698 - Xworkers_1mill_0 - INFO - architecture: {'rank': 0, 'n_input': 258, 'n_classes': 7, 'train_num_examples': 38912492, 'valid_num_examples': 2082911, 'test_num_examples': 6472386, 'total_num_examples': 38500000, 'n_hidden_1': 200, 'n_hidden_2': 140, 'n_hidden_3': 140}\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"Num ranks:  {}\".format(hvd.size()))\n",
    "logger.info(\"Num of records: {}\".format(nrecord))\n",
    "logger.info(\"Total batch size: {}\".format(FLAGS.batch_size * hvd.size()))\n",
    "logger.info(\"{}, per device\".format(FLAGS.batch_size))\n",
    "logger.info(\"Data type: {}\".format(DT_FLOAT)) \n",
    "logger.info(\"architecture: {}\".format(architecture)) \n",
    "# time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-13 19:33:26,708 - Xworkers_1mill_0 - INFO - Number of steps per epoch: 4350\n",
      "2018-11-13 19:33:26,708 - Xworkers_1mill_0 - INFO - Number of steps per epoch: 4350\n",
      "2018-11-13 19:33:26,717 - Xworkers_1mill_0 - INFO - Building training graph\n",
      "2018-11-13 19:33:26,717 - Xworkers_1mill_0 - INFO - Building training graph\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/contrib/metrics/python/ops/histogram_ops.py:83: remove_squeezable_dimensions (from tensorflow.contrib.framework.python.framework.tensor_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please switch to remove_squeezable_dimensions from tf.confusion_matrix. Note that the order of the inputs and outputs of labels and predictions have also been switched.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-13 19:33:31,587 - tensorflow - WARNING - From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/contrib/metrics/python/ops/histogram_ops.py:83: remove_squeezable_dimensions (from tensorflow.contrib.framework.python.framework.tensor_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please switch to remove_squeezable_dimensions from tf.confusion_matrix. Note that the order of the inputs and outputs of labels and predictions have also been switched.\n",
      "2018-11-13 19:33:44,076 - Xworkers_1mill_0 - INFO - Graph building completed....\n",
      "2018-11-13 19:33:44,076 - Xworkers_1mill_0 - INFO - Graph building completed....\n",
      "2018-11-13 19:33:44,083 - Xworkers_1mill_0 - INFO - Creating session\n",
      "2018-11-13 19:33:44,083 - Xworkers_1mill_0 - INFO - Creating session\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.FeedForward object at 0x7f288c018940>\n"
     ]
    }
   ],
   "source": [
    "if FLAGS.eval:\n",
    "    if FLAGS.test_dir is None:\n",
    "        logger.error(\"eval requires data_dir to be specified\")\n",
    "        raise ValueError(\"eval requires data_dir to be specified\")\n",
    "    if hvd.size() > 1:\n",
    "        logger.error(\"Multi-GPU evaluation is not supported\")\n",
    "        raise ValueError(\"Multi-GPU evaluation is not supported\")\n",
    "    evaluator = FeedForward(eval_func, dtype='evaluator')\n",
    "    logger.info(\"Building evaluation graph\")\n",
    "    conf_mtx_op, accuracy_op, better_acc_op, precision_op = evaluator.evaluation_step(FLAGS.test_batch_size)    \n",
    "    print(evaluator)\n",
    "else:    \n",
    "    nstep_per_epoch = nrecord // FLAGS.batch_size # if it is kwnow the total size: (FLAGS.batch_size * hvd.size())\n",
    "    logger.info(\"Number of steps per epoch: %d\" % nstep_per_epoch)\n",
    "    # model_func = lambda features, labels, architecture, FLAGS: loss_func(features, labels, architecture, FLAGS) # inference_vgg(net, images, nlayer)\n",
    "    trainer = FeedForward(loss_func, nstep_per_epoch=nstep_per_epoch)\n",
    "    logger.info(\"Building training graph\")    \n",
    "    train_ops, learning_rate_op, conf_mtx_op, accuracy_op, better_acc_op, precision_op, auc_list_op, auc_mean_op, m_list_op, m_list_mean_op, total_loss_op, lloss_op, auc_pr_op, auc_pr_mean_op, auc_data_op = trainer.training_step(architecture, FLAGS)\n",
    "    logger.info(\"Graph building completed....\")\n",
    "    print(trainer)\n",
    "    global_init, local_init = trainer.init()\n",
    "\n",
    "logger.info(\"Creating session\")\n",
    "config = tf.ConfigProto(allow_soft_placement = True)\n",
    "config.intra_op_parallelism_threads = 1\n",
    "config.inter_op_parallelism_threads = 10\n",
    "config.gpu_options.force_gpu_compatible = True\n",
    "config.gpu_options.visible_device_list = str(hvd.local_rank())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(FLAGS.logdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining summary (writer) and checkpoint (saver) files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-13 19:33:47,103 - Xworkers_1mill_0 - INFO - Initializing variables\n",
      "2018-11-13 19:33:47,103 - Xworkers_1mill_0 - INFO - Initializing variables\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /home/ubuntu/summ_15ep_2wrk/checkpoint-0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-13 19:33:47,692 - tensorflow - INFO - Restoring parameters from /home/ubuntu/summ_15ep_2wrk/checkpoint-0\n",
      "2018-11-13 19:33:47,818 - Xworkers_1mill_0 - INFO - Restored session from checkpoint /home/ubuntu/summ_15ep_2wrk/checkpoint-0\n",
      "2018-11-13 19:33:47,818 - Xworkers_1mill_0 - INFO - Restored session from checkpoint /home/ubuntu/summ_15ep_2wrk/checkpoint-0\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session(config=config)\n",
    "\n",
    "train_writer = None\n",
    "valid_writer = None\n",
    "saver = None\n",
    "summary_ops = None\n",
    "\n",
    "\n",
    "if hvd.rank() == 0 and FLAGS.logdir is not None:\n",
    "    train_writer = tf.summary.FileWriter(os.path.join(FLAGS.logdir), sess.graph)\n",
    "    valid_writer = tf.summary.FileWriter(os.path.join(FLAGS.logdir, 'valid'), graph=None)\n",
    "    summary_ops = tf.summary.merge_all()\n",
    "    last_summary_time = time.time()\n",
    "    saver = tf.train.Saver(keep_checkpoint_every_n_hours=1)\n",
    "    last_save_time = time.time()\n",
    "\n",
    "if not FLAGS.eval:        \n",
    "    logger.info(\"Initializing variables\")    \n",
    "    sess.run([global_init, local_init])\n",
    "\n",
    "restored = False\n",
    "if hvd.rank() == 0 and saver is not None:\n",
    "    ckpt = tf.train.get_checkpoint_state(FLAGS.logdir)\n",
    "    checkpoint_file = os.path.join(FLAGS.logdir, \"checkpoint\")\n",
    "    if ckpt and ckpt.model_checkpoint_path:\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "        restored = True\n",
    "        logger.info(\"Restored session from checkpoint {}\".format(ckpt.model_checkpoint_path))\n",
    "    else:\n",
    "        if not os.path.exists(FLAGS.logdir):\n",
    "            os.mkdir(FLAGS.logdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running evaluation from a checkpoint file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_better_acc(conf_mtx, axis=1):\n",
    "    cfsum = conf_mtx.sum(axis=axis, keepdims=True)\n",
    "    conf_mtx1 = np.divide(conf_mtx, cfsum, out=np.zeros_like(conf_mtx, dtype=np.float32), where=(cfsum!=0), dtype=np.float32)    \n",
    "    bett_acc = conf_mtx1.diagonal().mean()\n",
    "    return bett_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/MLMortgage/data/processed/chuncks_random_c1millx2_train/temporalloandynmodifmrstaticitur20mill-21mill_non_index-train_0.h5 ...to load\n",
      "/home/ubuntu/MLMortgage/data/processed/chuncks_random_c1millx2_train/temporalloandynmodifmrstaticitur20mill-21mill_non_index-train_0.h5  loaded in RAM\n",
      "/home/ubuntu/MLMortgage/data/processed/chuncks_random_c1millx2_train/1-1mill_cs1200_non_index_train_0.h5 ...to load\n",
      "/home/ubuntu/MLMortgage/data/processed/chuncks_random_c1millx2_train/1-1mill_cs1200_non_index_train_0.h5  loaded in RAM\n",
      "class_weights [  615022  1099592   382906   842623 34855682   662117    42058]\n",
      "class_weights [0.984 0.971 0.99  0.978 0.095 0.983 0.999]\n",
      "/home/ubuntu/MLMortgage/data/processed/chuncks_random_c1millx2_valid/2mill-3mill_cs1200_non_index_1_valid_0.h5 ...to load\n",
      "/home/ubuntu/MLMortgage/data/processed/chuncks_random_c1millx2_valid/2mill-3mill_cs1200_non_index_1_valid_0.h5  loaded in RAM\n",
      "/home/ubuntu/MLMortgage/data/processed/chuncks_random_c1millx2_valid/1-11mill_cs1200_non_index_valid_0.h5 ...to load\n",
      "/home/ubuntu/MLMortgage/data/processed/chuncks_random_c1millx2_valid/1-11mill_cs1200_non_index_valid_0.h5  loaded in RAM\n",
      "class_weights [  47983   58018   22477   70322 1808624   73042    2445]\n",
      "class_weights [0.977 0.972 0.989 0.966 0.132 0.965 0.999]\n"
     ]
    }
   ],
   "source": [
    "if not FLAGS.eval:\n",
    "\n",
    "    if (hvd.rank()==0):\n",
    "        DATA = md.get_h5_data(PRO_DIR, architecture, FLAGS.train_dir, FLAGS.valid_dir, None, train_period=FLAGS.train_period, valid_period=FLAGS.valid_period, test_period=FLAGS.test_period)         \n",
    "    else:\n",
    "        DATA = md.get_h5_data(PRO_DIR, architecture, FLAGS.train_dir, None, None, train_period=FLAGS.train_period, valid_period=FLAGS.valid_period, test_period=FLAGS.test_period) \n",
    "        \n",
    "    #logger.info('Features List: {}'.format(DATA.train.features_list))\n",
    "    #logger.info('Labels List: {}'.format(DATA.train.labels_list))\n",
    "    \n",
    "else:\n",
    "    DATA = md.get_h5_data(PRO_DIR, architecture, None, None, FLAGS.test_dir, train_period=FLAGS.train_period, valid_period=FLAGS.valid_period, test_period=FLAGS.test_period) \n",
    "    #logger.info('Features List: {}'.format(DATA.test.features_list))\n",
    "    #logger.info('Labels List: {}'.format(DATA.test.labels_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38500000\n",
      "38500000\n"
     ]
    }
   ],
   "source": [
    "print(DATA.train._dict[0]['nrows'])\n",
    "print(nrecord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feed_dict(tag, DATA, FLAGS):\n",
    "    \"\"\"Create the feed dictionary for mapping data onto placeholders in the graph.\"\"\"\n",
    "    if tag == 'batch':\n",
    "        if hvd.rank()==0:\n",
    "            batch_size = FLAGS.batch_size #(FLAGS.batch_size // 2)\n",
    "        else:\n",
    "            batch_size = FLAGS.batch_size\n",
    "        \n",
    "        features, targets = DATA.train.next_random_batch(batch_size)        \n",
    "        #example_weights = np.sum(np.multiply(np.float32(oh_labels),class_weights4), axis=1)\n",
    "        example_weights = np.sum(np.multiply(np.float32(targets),DATA.train.class_weights), axis=1)\n",
    "        #print('example_weights', example_weights)\n",
    "        #example_weights = np.array([1.0], dtype=NP_FLOAT) # [1.0] * architecture['total_num_examples']                    \n",
    "        # logger.info('features shape: {}'.format(features.shape))\n",
    "    elif tag == 'train':\n",
    "        features = DATA.train.orig.features\n",
    "        targets = DATA.train.orig.labels\n",
    "        example_weights = np.ones_like(targets.iloc[:, 1].values)\n",
    "    elif tag == 'valid':\n",
    "        features, targets, example_weights = DATA.validation.next_sequential_batch(FLAGS.valid_batch_size)\n",
    "    else:\n",
    "        features, targets, example_weights = DATA.test.next_sequential_batch(FLAGS.test_batch_size)\n",
    "\n",
    "    # features[:, :7] = targets\n",
    "    if tag == 'batch':\n",
    "        k_prob_input = 0.9  # 0.9  # .85  # .75  # 0.8  # 0.6\n",
    "        k_prob = FLAGS.dropout_keep\n",
    "        t_flag = True\n",
    "    else:\n",
    "        k_prob_input = 1.0\n",
    "        k_prob = 1.0\n",
    "        t_flag = False\n",
    "\n",
    "    # Change the python dictionary to an io-buffer for a better performance.\n",
    "    # See here:\n",
    "    # https://www.tensorflow.org/performance/performance_guide\n",
    "    feed_d = {\n",
    "        'features:0': features,\n",
    "        'targets:0': targets,\n",
    "        'example_weights:0': example_weights,        \n",
    "        'train_flag:0': t_flag,\n",
    "        #'epoch_flag:0': FLAGS.epoch_flag,\n",
    "        #'1_hidden/dropout/keep_proba:0': k_prob_input,\n",
    "        #'2_hidden/dropout/keep_proba:0': k_prob,\n",
    "        # '3_hidden/dropout/keep_proba:0': k_prob,\n",
    "        # '4_hidden/dropout/keep_proba:0': k_prob,\n",
    "        # '5_hidden/dropout/keep_proba:0': k_prob,\n",
    "        '9_softmax_linear/dropout/keep_proba:0': k_prob\n",
    "    }\n",
    "\t\n",
    "\t# for any tag:\n",
    "    if (FLAGS.n_hidden > 0) :\n",
    "        # print ('k_prob_input', k_prob_input, type(k_prob_input))\n",
    "        feed_d['1_hidden/dropout/keep_proba:0'] = k_prob_input\n",
    "        for hid_i in range(2, FLAGS.n_hidden+1):\n",
    "            feed_d['{:1d}_hidden/dropout/keep_proba:0'.format(hid_i)] = k_prob\n",
    "    # print('feed_d', feed_d)\n",
    "    # print('batch shape: ', features.shape)    \n",
    "    return feed_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_evaluation(nstep, sess, enqueue_ops):\n",
    "    logger.info(\"Evaluating Model\")\n",
    "    accuracys = []\n",
    "    better_accs = []\n",
    "    logger.info(\"  Step  Accuracy  Better-Accuracy   Precision   f1score_micro   f1score_macro\")\n",
    "    for step in range(nstep):\n",
    "        try:\n",
    "            feed = create_feed_dict('test', DATA, FLAGS)     \n",
    "            #logger.info('feed dictionary was created')\n",
    "            conf_mtx, accuracy, better_acc, precision = sess.run(enqueue_ops, feed_dict=feed)\n",
    "            #logger.info('operations were ran')\n",
    "            if (math.isnan(better_acc)):                \n",
    "                better_acc = calculate_better_acc(conf_mtx)                    \n",
    "                #print('beter_acc nan')\n",
    "            if (math.isnan(precision)):                \n",
    "                precision = calculate_better_acc(conf_mtx, axis=0)                    \n",
    "                #print('precision nan')\n",
    "            f1score_micro = 2 * precision * better_acc / (precision + better_acc)\n",
    "            f1score_macro = calculate_macro_f1score(np.array(conf_mtx, dtype='float32'))\n",
    "            \n",
    "            #if step == 0 or (step+1) % FLAGS.display_every == 0:\n",
    "            logger.info(\"% 6i %5.1f%% %5.1f%% %5.1f%% %5.5f%%  %5.5f%%\" % (step+1, accuracy*100, better_acc*100, precision*100, f1score_micro, f1score_macro))\n",
    "            accuracys.append(accuracy)\n",
    "            better_accs.append(better_acc)\n",
    "            #print('acc and bett_acc appended')\n",
    "            \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"Keyboard interrupt\")\n",
    "            break\n",
    "        except  Exception  as e:        \n",
    "            raise ValueError('Error running Evaluation: ' + str(e))    \n",
    "\n",
    "    nstep = len(accuracys)\n",
    "    if nstep == 0:\n",
    "        return\n",
    "    accuracys = np.asarray(accuracys) * 100.\n",
    "    better_accs = np.asarray(better_accs) * 100.\n",
    "    acc_mean = np.mean(accuracys)\n",
    "    bettacc_mean = np.mean(better_accs)\n",
    "    if nstep > 2:\n",
    "        acc_uncertainty = np.std(accuracys, ddof=1) / np.sqrt(float(nstep))\n",
    "        bettacc_uncertainty = np.std(better_accs, ddof=1) / np.sqrt(float(nstep))\n",
    "    else:\n",
    "        acc_uncertainty = float('nan')\n",
    "        bettacc_uncertainty = float('nan')\n",
    "    acc_madstd = 1.4826*np.median(np.abs(accuracys - acc_mean))\n",
    "    bettacc_madstd = 1.4826*np.median(np.abs(better_accs - bettacc_mean))\n",
    "    logger.info('-' * 64)\n",
    "    logger.info('Validation Accuracy: %.3f %% +/- %.2f (jitter = %.1f)' % (\n",
    "        acc_mean, acc_uncertainty, acc_madstd))\n",
    "    logger.info('Validation Better Accuracy: %.3f %% +/- %.2f (jitter = %.1f)' % (\n",
    "        bettacc_mean, bettacc_uncertainty, bettacc_madstd))\n",
    "    logger.info('-' * 64)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "nstep = 0\n",
    "if FLAGS.eval:\n",
    "    if not restored:\n",
    "        logger.error(\"No checkpoint found for evaluation\")\n",
    "        raise ValueError(\"No checkpoint found for evaluation\")\n",
    "    else:        \n",
    "        #nstep = nrecord // FLAGS.test_batch_size \n",
    "        nstep = DATA.test.total_num_batch(FLAGS.test_batch_size) \n",
    "        logger.info(\"total steps: {}\".format(nstep))        \n",
    "        enq_ops = [conf_mtx_op, accuracy_op, better_acc_op, precision_op]\n",
    "        logger.info(\"Executing Evaluation\")        \n",
    "        run_evaluation(nstep, sess, enq_ops)   \n",
    "        logger.info('Evaluation was done')\n",
    "        # sys.exit(0) #the following instructiones will not be  executed\n",
    "        quit()\n",
    "else:    \n",
    "    if FLAGS.epoch_num is not None:\n",
    "        if (nrecord <= 0):\n",
    "            logger.error(\"num_epochs requires nrecord to be specified\")\n",
    "            raise ValueError(\"num_epochs requires nrecord to be specified\")\n",
    "        nstep = math.ceil(np.float32(nrecord * FLAGS.epoch_num / FLAGS.batch_size)) # if it is kwnow the total size: (FLAGS.batch_size * hvd.size()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-13 19:35:46,316 - Xworkers_1mill_0 - INFO - METRICS:  Namespace(allow_summaries=True, batch_layer_type='batch', batch_norm=True, batch_size=8850, batch_type='layer', data_dir='/input_data', decay_rate=1, decay_step=3520000, display_every=100, dropout=True, dropout_keep=0.9, epoch_flag=0, epoch_num=4, eval=False, ftp_dir='processed/c1mill', learning_rate=0.1, log_file=None, logdir=PosixPath('/home/ubuntu/summ_15ep_2wrk'), loss_tolerance=0.0001, lr_decay_epochs=30, lr_decay_policy='time', lr_decay_rate=0.1, lr_poly_power=2.0, max_epoch_size=-1, momentum=0.5, n_hidden=3, name='Xworkers_1mill', nstep_burnin=20, rate_min=0.0015, reg_rate=1e-05, s_hidden=[200, 140, 140], save_interval=450, stratified_flag=False, summary_interval=1800, test_batch_size=1200000, test_dir='chuncks_random_c1millx2_test', test_flag=True, test_period=[330, 342], total_examples=38500000, train_dir='chuncks_random_c1millx2_train', train_period=[121, 323], valid_batch_size=150000, valid_dir='chuncks_random_c1millx2_valid', valid_period=[324, 329], weighted_sampling=False, xla=True)\n",
      "\n",
      "2018-11-13 19:35:46,316 - Xworkers_1mill_0 - INFO - METRICS:  Namespace(allow_summaries=True, batch_layer_type='batch', batch_norm=True, batch_size=8850, batch_type='layer', data_dir='/input_data', decay_rate=1, decay_step=3520000, display_every=100, dropout=True, dropout_keep=0.9, epoch_flag=0, epoch_num=4, eval=False, ftp_dir='processed/c1mill', learning_rate=0.1, log_file=None, logdir=PosixPath('/home/ubuntu/summ_15ep_2wrk'), loss_tolerance=0.0001, lr_decay_epochs=30, lr_decay_policy='time', lr_decay_rate=0.1, lr_poly_power=2.0, max_epoch_size=-1, momentum=0.5, n_hidden=3, name='Xworkers_1mill', nstep_burnin=20, rate_min=0.0015, reg_rate=1e-05, s_hidden=[200, 140, 140], save_interval=450, stratified_flag=False, summary_interval=1800, test_batch_size=1200000, test_dir='chuncks_random_c1millx2_test', test_flag=True, test_period=[330, 342], total_examples=38500000, train_dir='chuncks_random_c1millx2_train', train_period=[121, 323], valid_batch_size=150000, valid_dir='chuncks_random_c1millx2_valid', valid_period=[324, 329], weighted_sampling=False, xla=True)\n",
      "\n",
      "2018-11-13 19:35:46,318 - Xworkers_1mill_0 - INFO - Number of total steps: 17402\n",
      "2018-11-13 19:35:46,318 - Xworkers_1mill_0 - INFO - Number of total steps: 17402\n"
     ]
    }
   ],
   "source": [
    "logger.info('METRICS:  %s\\r\\n' % str(FLAGS))\n",
    "logger.info('Number of total steps: %d' % nstep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# broadcast_global_variables from hvd\n",
    "trainer.sync(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-13 19:35:46,834 - Xworkers_1mill_0 - INFO - Writing summaries to /home/ubuntu/summ_15ep_2wrk\n",
      "2018-11-13 19:35:46,834 - Xworkers_1mill_0 - INFO - Writing summaries to /home/ubuntu/summ_15ep_2wrk\n",
      "2018-11-13 19:35:46,836 - Xworkers_1mill_0 - INFO - Training\n",
      "2018-11-13 19:35:46,836 - Xworkers_1mill_0 - INFO - Training\n",
      "2018-11-13 19:35:46,838 - Xworkers_1mill_0 - INFO -   Step; Epoch; time-per-record(sec);  batchtime/worker(sec);  Loss;   Learning Rate; Accuracy; Recall; Precision; F1score\n",
      "2018-11-13 19:35:46,838 - Xworkers_1mill_0 - INFO -   Step; Epoch; time-per-record(sec);  batchtime/worker(sec);  Loss;   Learning Rate; Accuracy; Recall; Precision; F1score\n"
     ]
    }
   ],
   "source": [
    "# Trying to restore for training:\n",
    "if hvd.rank() == 0 and not restored:\n",
    "    if saver is not None:\n",
    "        save_path = saver.save(sess, checkpoint_file, global_step=0)\n",
    "        print(\"Checkpoint written to\", save_path)\n",
    "\n",
    "logger.info(\"Writing summaries to {}\".format(FLAGS.logdir))\n",
    "logger.info(\"Training\")\n",
    "logger.info(\"  Step; Epoch; time-per-record(sec);  batchtime/worker(sec);  Loss;   Learning Rate; Accuracy; Recall; Precision; F1score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_and_update(sess, local_init, feed_dict):\n",
    "    \"\"\"Reset the local variables and update the necessary update ops.\"\"\"\n",
    "    # sess.run(local_init) # this is necesary in each batch??check out the local variables!\n",
    "        \n",
    "    update_names_list = [\n",
    "        'metrics/auc/{:d}/hist_accumulate/update_op'.format(i)\n",
    "        for i in range(7)\n",
    "    ]\n",
    "\n",
    "    update_names_list.extend([\n",
    "        'metrics/m_measure/' + str(i) + str(j) + '/hist_accumulate/update_op'\n",
    "        for i in range(7) for j in range(7) if i != j\n",
    "    ])\n",
    "\n",
    "    sess.run(update_names_list, feed_dict=feed_dict)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_m_mtx(mtx):\n",
    "    \"\"\"Reshape the python list into a np array.\"\"\"\n",
    "    new_mtx = [0]\n",
    "    for i in range(6):\n",
    "        new_mtx.extend(mtx[i * 7:(i + 1) * 7])\n",
    "        new_mtx.append(0)\n",
    "    temp = np.array(new_mtx).reshape(7, 7)\n",
    "\n",
    "    return temp\n",
    "\n",
    "def print_stats(name, conf_mtx, accuracy, better_acc, precision, f1score_micro, f1score_macro, auc_list, auc_mean, m_list, m_list_mean, lloss, auc_pr, auc_pr_mean, loss):\n",
    "    \"\"\"Print to logger the given stats.\"\"\"        \n",
    "                \n",
    "    m_mtx = np.nan_to_num(reshape_m_mtx(m_list)) \n",
    "    auc_list = np.nan_to_num(auc_list)\n",
    "    conf_mtx = np.array(conf_mtx, dtype=int)\n",
    "            \n",
    "    stdout = 'Loss in ' + name +': {:.5f}\\n'.format(loss)        \n",
    "    stdout = stdout + ' Avg Log_Loss in ' + name +': {:.5f}\\n'.format(lloss)\n",
    "    stdout = stdout +  '{:s}:'.format(name) + ' (Silly) Global-ACC={:.5f}, Recall={:.5f},'.format(accuracy, better_acc) + \\\n",
    "        ' Avg M-Measure={:.4f},'.format(m_list_mean) + \\\n",
    "        ' Avg AUC_AOC={:.4f}'.format(auc_mean) + ' Avg AUC_PR={:.4f}\\n'.format(auc_pr_mean)\n",
    "    stdout = stdout + ' Precision= {:.5f} '.format(precision)\n",
    "    stdout = stdout + ' f1score_micro= {:.5f} '.format(f1score_micro)\n",
    "    stdout = stdout + ' f1score_macro= {:.5f}\\n'.format(f1score_macro)\n",
    "    stdout = stdout + (';').join(['Total Confusion Matrix', 'Total M-Measure Matrix', 'Total AUC_AOC', 'Total AUC_PR\\n'])\n",
    "    for conf_row, row, auc, auc_pr in zip(conf_mtx, m_mtx, auc_list, auc_pr):\n",
    "        for conf_value in conf_row:\n",
    "            stdout = stdout + '{}'.format(conf_value) + ';'\n",
    "        stdout = stdout + ';'\n",
    "        for value in row:\n",
    "            stdout = stdout + '{:.4f}'.format(value) + ';'\n",
    "        stdout = stdout + ';{:.4f}'.format(auc) + ' ;{:.4f}'.format(auc_pr) + '\\n'\n",
    "    stdout = stdout + '---------------------------------------------------------------------'\n",
    "              \n",
    "    logger.info('METRICS each %s (secs):  %s\\r\\n' % (FLAGS.summary_interval, stdout))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for validation set:\n",
    "def batching_dataset(sess, epoch, writer, tag, DATA, FLAGS):\n",
    "    if tag =='valid':\n",
    "        batch_num = DATA.validation.total_num_batch(FLAGS.valid_batch_size) \n",
    "        \n",
    "    # metrics = acc_metrics_init(DATA)\n",
    "    sess.run(local_init)\n",
    "    start_time = datetime.now()\n",
    "    acc_conf_mtx=np.zeros((DATA.train.num_classes, DATA.train.num_classes))        \n",
    "    metrics = [0.0] * 7\n",
    "    for batch_i in range(batch_num):\n",
    "        step = epoch * batch_num + batch_i    # total steps for all epochs        \n",
    "        feed = create_feed_dict(tag, DATA, FLAGS)     \n",
    "        reset_and_update(sess, local_init, feed)\n",
    "        summary, conf_mtx, accuracy, better_acc, precision, lloss, loss = sess.run([summary_ops, conf_mtx_op, accuracy_op, better_acc_op, precision_op, lloss_op, total_loss_op], feed_dict=feed)\n",
    "\n",
    "        if (math.isnan(better_acc)):\n",
    "            better_acc = calculate_better_acc(conf_mtx)\n",
    "\n",
    "        if (math.isnan(precision)):\n",
    "            precision = calculate_better_acc(conf_mtx, axis=0)\n",
    "        \n",
    "        f1score_micro = 2 * precision * better_acc / (precision + better_acc)\n",
    "        f1score_macro = calculate_macro_f1score(np.array(conf_mtx, dtype='float32'))\n",
    "        \n",
    "        acc_conf_mtx = np.add(acc_conf_mtx, conf_mtx)\n",
    "        metrics = np.add(metrics, np.array([accuracy, better_acc, precision, f1score_micro, f1score_macro, lloss, loss]))\n",
    "        writer.add_summary(summary, step)\n",
    "        writer.flush()\n",
    "        del feed\n",
    "    \n",
    "    metrics[:] = [x / batch_num for x in metrics]\n",
    "    valid_time = datetime.now() - start_time\n",
    "    logger.info('%s - Number of batches: %d; batch_size: %d; Total Time: %s' %(tag, batch_num,  FLAGS.valid_batch_size, valid_time))\n",
    "    return acc_conf_mtx, valid_time, metrics\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_train: \n",
      " Empty DataFrame\n",
      "Columns: [step, epoch, batch_time, Loss, LogLoss, Accuracy, Recall, Precision, f1score-Micro, f1score-Macro, M-Measure Mean, AUC_AOC Mean, AUC_PR Mean]\n",
      "Index: []\n",
      "df_valid: \n",
      " Empty DataFrame\n",
      "Columns: [step, epoch, batch_time, Loss, LogLoss, Accuracy, Recall, Precision, f1score-Micro, f1score-Macro]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "if hvd.rank() == 0:\n",
    "    if not FLAGS.eval:\n",
    "        dtype = ['step','epoch','batch_time','Loss','LogLoss','Accuracy','Recall','Precision', 'f1score-Micro', 'f1score-Macro', 'M-Measure Mean','AUC_AOC Mean','AUC_PR Mean']        \n",
    "        train_file = os.path.join(FLAGS.logdir, FLAGS.name + \"_train.csv\")\n",
    "        valid_file = os.path.join(FLAGS.logdir, FLAGS.name + \"_valid.csv\")        \n",
    "        \n",
    "        if not Path(train_file).exists():\n",
    "            df_train = pd.DataFrame(columns=dtype)                \n",
    "            df_train.to_csv(train_file, sep=';', index=False)\n",
    "        else:\n",
    "            df_train = pd.read_csv(train_file, sep=';')\n",
    "\n",
    "        if not Path(valid_file).exists():\n",
    "            df_valid = pd.DataFrame(columns=dtype[:10])\n",
    "            df_valid.to_csv(valid_file, sep=';', index=False)            \n",
    "        else:\n",
    "            df_valid = pd.read_csv(valid_file, sep=';')\n",
    "        \n",
    "        print('df_train: \\n', df_train)\n",
    "        print('df_valid: \\n', df_valid)\n",
    "        \n",
    "    else:  # validation set:\n",
    "        dtype = ['NN_name', 'NN_Number','Total Epochs', 'Execute Epochs', 'Total Training Time', 'Loss','LogLoss','Accuracy','Recall', 'Precision', 'f1score-Micro', 'f1score-Macro', 'M-Measure Mean','AUC_AOC Mean','AUC_PR Mean']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_macro_f1score(conf_mtx):\n",
    "    sum_byrows = conf_mtx.sum(axis=1, keepdims=True)\n",
    "    recall = np.diag(np.divide(conf_mtx, sum_byrows, out=np.zeros_like(conf_mtx), where=sum_byrows!=0)) #, where=sum_byrows!=0\n",
    "    #print('recall: ', recall)\n",
    "\n",
    "    sum_bycols = np.sum(conf_mtx, axis=0, keepdims=True)\n",
    "    #print('sum_bycols: ', sum_bycols)\n",
    "    precision = np.diag(np.divide(conf_mtx, sum_bycols, out=np.zeros_like(conf_mtx), where=sum_byrows!=0)) #where=sum_bycols!=0\n",
    "    #print('precision: ', precision)\n",
    "\n",
    "    sum_rec_prec = recall + precision\n",
    "    #print('sum_rec_prec: ', sum_rec_prec)\n",
    "    f1sc_perclass = np.divide((2 * recall * precision), sum_rec_prec, out=np.zeros_like(recall), where=sum_rec_prec!=0)\n",
    "\n",
    "\n",
    "    #print('f1sc_perclass: ', f1sc_perclass)\n",
    "    np.nan_to_num(f1sc_perclass, copy=False)\n",
    "    #if (recall==np.nan() or precision==np.nan()):\n",
    "    micro_f1sc = np.average(f1sc_perclass)\n",
    "    #print('micro_f1sc: ', micro_f1sc)\n",
    "    \n",
    "    return micro_f1sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-13 20:04:20,014 - Xworkers_1mill_0 - INFO - METRICS each 1800 (secs):  Loss in ---Training in Summary---: 0.12763\n",
      " Avg Log_Loss in ---Training in Summary---: 0.09897\n",
      "---Training in Summary---: (Silly) Global-ACC=0.90780, Recall=0.50614, Avg M-Measure=0.9361, Avg AUC_AOC=0.9373 Avg AUC_PR=0.5529\n",
      " Precision= 0.66503  f1score_micro= 0.57481  f1score_macro= 0.49437\n",
      "Total Confusion Matrix;Total M-Measure Matrix;Total AUC_AOC;Total AUC_PR\n",
      "18;9;0;5;92;4;0;;0.0000;0.9262;0.9786;0.9867;0.7258;0.9694;0.9470;;0.7522 ;0.3306\n",
      "1;146;11;9;94;0;0;;0.9623;0.0000;0.6217;0.9789;0.9142;0.9975;0.9975;;0.9174 ;0.2848\n",
      "0;51;16;20;0;1;0;;0.9965;0.8010;0.0000;0.9334;0.9962;0.9942;0.9982;;0.9879 ;0.3572\n",
      "0;6;12;167;0;17;0;;0.9879;0.9894;0.9276;0.0000;0.9989;0.9268;0.9252;;0.9949 ;0.7807\n",
      "9;402;7;21;7559;3;0;;0.7063;0.9128;0.9957;0.9990;0.0000;0.9990;0.9978;;0.9293 ;0.9812\n",
      "0;0;1;29;2;127;0;;0.9859;0.9986;0.9950;0.9264;0.9997;0.0000;0.6713;;0.9963 ;0.8440\n",
      "0;1;0;2;0;7;1;;0.9850;0.9965;0.9942;0.9597;0.9988;0.7119;0.0000;;0.9828 ;0.2916\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "2018-11-13 20:04:20,014 - Xworkers_1mill_0 - INFO - METRICS each 1800 (secs):  Loss in ---Training in Summary---: 0.12763\n",
      " Avg Log_Loss in ---Training in Summary---: 0.09897\n",
      "---Training in Summary---: (Silly) Global-ACC=0.90780, Recall=0.50614, Avg M-Measure=0.9361, Avg AUC_AOC=0.9373 Avg AUC_PR=0.5529\n",
      " Precision= 0.66503  f1score_micro= 0.57481  f1score_macro= 0.49437\n",
      "Total Confusion Matrix;Total M-Measure Matrix;Total AUC_AOC;Total AUC_PR\n",
      "18;9;0;5;92;4;0;;0.0000;0.9262;0.9786;0.9867;0.7258;0.9694;0.9470;;0.7522 ;0.3306\n",
      "1;146;11;9;94;0;0;;0.9623;0.0000;0.6217;0.9789;0.9142;0.9975;0.9975;;0.9174 ;0.2848\n",
      "0;51;16;20;0;1;0;;0.9965;0.8010;0.0000;0.9334;0.9962;0.9942;0.9982;;0.9879 ;0.3572\n",
      "0;6;12;167;0;17;0;;0.9879;0.9894;0.9276;0.0000;0.9989;0.9268;0.9252;;0.9949 ;0.7807\n",
      "9;402;7;21;7559;3;0;;0.7063;0.9128;0.9957;0.9990;0.0000;0.9990;0.9978;;0.9293 ;0.9812\n",
      "0;0;1;29;2;127;0;;0.9859;0.9986;0.9950;0.9264;0.9997;0.0000;0.6713;;0.9963 ;0.8440\n",
      "0;1;0;2;0;7;1;;0.9850;0.9965;0.9942;0.9597;0.9988;0.7119;0.0000;;0.9828 ;0.2916\n",
      "---------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint written to /home/ubuntu/summ_15ep_2wrk/checkpoint-4351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py:8: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint written to /home/ubuntu/summ_15ep_2wrk/checkpoint-4352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-13 20:04:28,128 - Xworkers_1mill_0 - INFO -   4400;     2; 72576.2;   0.122; 0.12164; 0.09988; 0.92362; 0.55089; 0.65969; 0.60040; 0.55897\n",
      "2018-11-13 20:04:28,128 - Xworkers_1mill_0 - INFO -   4400;     2; 72576.2;   0.122; 0.12164; 0.09988; 0.92362; 0.55089; 0.65969; 0.60040; 0.55897\n",
      "2018-11-13 20:04:40,364 - Xworkers_1mill_0 - INFO -   4500;     2; 70091.6;   0.126; 0.12372; 0.09987; 0.91921; 0.51710; 0.64877; 0.57550; 0.51451\n",
      "2018-11-13 20:04:40,364 - Xworkers_1mill_0 - INFO -   4500;     2; 70091.6;   0.126; 0.12372; 0.09987; 0.91921; 0.51710; 0.64877; 0.57550; 0.51451\n",
      "2018-11-13 20:04:52,672 - Xworkers_1mill_0 - INFO -   4600;     2; 70672.1;   0.125; 0.12351; 0.09987; 0.92090; 0.53966; 0.67935; 0.60150; 0.54635\n",
      "2018-11-13 20:04:52,672 - Xworkers_1mill_0 - INFO -   4600;     2; 70672.1;   0.125; 0.12351; 0.09987; 0.92090; 0.53966; 0.67935; 0.60150; 0.54635\n",
      "2018-11-13 20:05:05,078 - Xworkers_1mill_0 - INFO -   4700;     2; 70283.6;   0.126; 0.12168; 0.09987; 0.91514; 0.52622; 0.50978; 0.51787; 0.50195\n",
      "2018-11-13 20:05:05,078 - Xworkers_1mill_0 - INFO -   4700;     2; 70283.6;   0.126; 0.12168; 0.09987; 0.91514; 0.52622; 0.50978; 0.51787; 0.50195\n",
      "2018-11-13 20:05:17,377 - Xworkers_1mill_0 - INFO -   4800;     2; 73411.1;   0.121; 0.13015; 0.09986; 0.91299; 0.54582; 0.65234; 0.59435; 0.54579\n",
      "2018-11-13 20:05:17,377 - Xworkers_1mill_0 - INFO -   4800;     2; 73411.1;   0.121; 0.13015; 0.09986; 0.91299; 0.54582; 0.65234; 0.59435; 0.54579\n",
      "2018-11-13 20:05:29,618 - Xworkers_1mill_0 - INFO -   4900;     2; 73854.1;   0.120; 0.12884; 0.09986; 0.91119; 0.55114; 0.59891; 0.57403; 0.53519\n",
      "2018-11-13 20:05:29,618 - Xworkers_1mill_0 - INFO -   4900;     2; 73854.1;   0.120; 0.12884; 0.09986; 0.91119; 0.55114; 0.59891; 0.57403; 0.53519\n",
      "2018-11-13 20:05:42,012 - Xworkers_1mill_0 - INFO -   5000;     2; 72331.1;   0.122; 0.12761; 0.09986; 0.92282; 0.51063; 0.52714; 0.51875; 0.50327\n",
      "2018-11-13 20:05:42,012 - Xworkers_1mill_0 - INFO -   5000;     2; 72331.1;   0.122; 0.12761; 0.09986; 0.92282; 0.51063; 0.52714; 0.51875; 0.50327\n",
      "2018-11-13 20:05:54,314 - Xworkers_1mill_0 - INFO -   5100;     2; 72531.4;   0.122; 0.12599; 0.09986; 0.90847; 0.51034; 0.54362; 0.52645; 0.49382\n",
      "2018-11-13 20:05:54,314 - Xworkers_1mill_0 - INFO -   5100;     2; 72531.4;   0.122; 0.12599; 0.09986; 0.90847; 0.51034; 0.54362; 0.52645; 0.49382\n",
      "2018-11-13 20:06:06,631 - Xworkers_1mill_0 - INFO -   5200;     2; 70621.1;   0.125; 0.12011; 0.09985; 0.92294; 0.54896; 0.68570; 0.60976; 0.55947\n",
      "2018-11-13 20:06:06,631 - Xworkers_1mill_0 - INFO -   5200;     2; 70621.1;   0.125; 0.12011; 0.09985; 0.92294; 0.54896; 0.68570; 0.60976; 0.55947\n",
      "2018-11-13 20:06:18,937 - Xworkers_1mill_0 - INFO -   5300;     2; 72879.4;   0.121; 0.11689; 0.09985; 0.91955; 0.53592; 0.67559; 0.59771; 0.55085\n",
      "2018-11-13 20:06:18,937 - Xworkers_1mill_0 - INFO -   5300;     2; 72879.4;   0.121; 0.11689; 0.09985; 0.91955; 0.53592; 0.67559; 0.59771; 0.55085\n",
      "2018-11-13 20:06:31,297 - Xworkers_1mill_0 - INFO -   5400;     2; 73161.1;   0.121; 0.11962; 0.09985; 0.92802; 0.53732; 0.68118; 0.60076; 0.55217\n",
      "2018-11-13 20:06:31,297 - Xworkers_1mill_0 - INFO -   5400;     2; 73161.1;   0.121; 0.11962; 0.09985; 0.92802; 0.53732; 0.68118; 0.60076; 0.55217\n",
      "2018-11-13 20:06:43,620 - Xworkers_1mill_0 - INFO -   5500;     2; 73382.8;   0.121; 0.11654; 0.09984; 0.92271; 0.51779; 0.52702; 0.52237; 0.50503\n",
      "2018-11-13 20:06:43,620 - Xworkers_1mill_0 - INFO -   5500;     2; 73382.8;   0.121; 0.11654; 0.09984; 0.92271; 0.51779; 0.52702; 0.52237; 0.50503\n",
      "2018-11-13 20:06:55,921 - Xworkers_1mill_0 - INFO -   5600;     2; 71041.5;   0.125; 0.12446; 0.09984; 0.91469; 0.51113; 0.64334; 0.56966; 0.50602\n",
      "2018-11-13 20:06:55,921 - Xworkers_1mill_0 - INFO -   5600;     2; 71041.5;   0.125; 0.12446; 0.09984; 0.91469; 0.51113; 0.64334; 0.56966; 0.50602\n",
      "2018-11-13 20:07:08,218 - Xworkers_1mill_0 - INFO -   5700;     2; 71166.8;   0.124; 0.11439; 0.09984; 0.92531; 0.50369; 0.52640; 0.51479; 0.49185\n",
      "2018-11-13 20:07:08,218 - Xworkers_1mill_0 - INFO -   5700;     2; 71166.8;   0.124; 0.11439; 0.09984; 0.92531; 0.50369; 0.52640; 0.51479; 0.49185\n",
      "2018-11-13 20:07:20,574 - Xworkers_1mill_0 - INFO -   5800;     2; 72037.1;   0.123; 0.12807; 0.09984; 0.91028; 0.54972; 0.64591; 0.59394; 0.54922\n",
      "2018-11-13 20:07:20,574 - Xworkers_1mill_0 - INFO -   5800;     2; 72037.1;   0.123; 0.12807; 0.09984; 0.91028; 0.54972; 0.64591; 0.59394; 0.54922\n",
      "2018-11-13 20:07:32,993 - Xworkers_1mill_0 - INFO -   5900;     2; 73492.2;   0.120; 0.12256; 0.09983; 0.92407; 0.58217; 0.68913; 0.63115; 0.59738\n",
      "2018-11-13 20:07:32,993 - Xworkers_1mill_0 - INFO -   5900;     2; 73492.2;   0.120; 0.12256; 0.09983; 0.92407; 0.58217; 0.68913; 0.63115; 0.59738\n",
      "2018-11-13 20:07:45,412 - Xworkers_1mill_0 - INFO -   6000;     2; 71195.2;   0.124; 0.12224; 0.09983; 0.92113; 0.50400; 0.64789; 0.56696; 0.50738\n",
      "2018-11-13 20:07:45,412 - Xworkers_1mill_0 - INFO -   6000;     2; 71195.2;   0.124; 0.12224; 0.09983; 0.92113; 0.50400; 0.64789; 0.56696; 0.50738\n",
      "2018-11-13 20:07:57,812 - Xworkers_1mill_0 - INFO -   6100;     2; 72908.7;   0.121; 0.12140; 0.09983; 0.92124; 0.51465; 0.57763; 0.54433; 0.50353\n",
      "2018-11-13 20:07:57,812 - Xworkers_1mill_0 - INFO -   6100;     2; 72908.7;   0.121; 0.12140; 0.09983; 0.92124; 0.51465; 0.57763; 0.54433; 0.50353\n",
      "2018-11-13 20:08:10,148 - Xworkers_1mill_0 - INFO -   6200;     2; 71093.0;   0.124; 0.11369; 0.09982; 0.92960; 0.58690; 0.69877; 0.63797; 0.60478\n",
      "2018-11-13 20:08:10,148 - Xworkers_1mill_0 - INFO -   6200;     2; 71093.0;   0.124; 0.11369; 0.09982; 0.92960; 0.58690; 0.69877; 0.63797; 0.60478\n",
      "2018-11-13 20:08:22,519 - Xworkers_1mill_0 - INFO -   6300;     2; 72818.6;   0.122; 0.11379; 0.09982; 0.92734; 0.56291; 0.66820; 0.61105; 0.57831\n",
      "2018-11-13 20:08:22,519 - Xworkers_1mill_0 - INFO -   6300;     2; 72818.6;   0.122; 0.11379; 0.09982; 0.92734; 0.56291; 0.66820; 0.61105; 0.57831\n",
      "2018-11-13 20:08:34,950 - Xworkers_1mill_0 - INFO -   6400;     2; 69166.0;   0.128; 0.11355; 0.09982; 0.92814; 0.54602; 0.69901; 0.61312; 0.55547\n",
      "2018-11-13 20:08:34,950 - Xworkers_1mill_0 - INFO -   6400;     2; 69166.0;   0.128; 0.11355; 0.09982; 0.92814; 0.54602; 0.69901; 0.61312; 0.55547\n",
      "2018-11-13 20:08:47,327 - Xworkers_1mill_0 - INFO -   6500;     2; 71336.0;   0.124; 0.12422; 0.09982; 0.91887; 0.54201; 0.66946; 0.59903; 0.54254\n",
      "2018-11-13 20:08:47,327 - Xworkers_1mill_0 - INFO -   6500;     2; 71336.0;   0.124; 0.12422; 0.09982; 0.91887; 0.54201; 0.66946; 0.59903; 0.54254\n",
      "2018-11-13 20:08:59,722 - Xworkers_1mill_0 - INFO -   6600;     2; 73826.6;   0.120; 0.11972; 0.09981; 0.92599; 0.58408; 0.64394; 0.61255; 0.59048\n",
      "2018-11-13 20:08:59,722 - Xworkers_1mill_0 - INFO -   6600;     2; 73826.6;   0.120; 0.11972; 0.09981; 0.92599; 0.58408; 0.64394; 0.61255; 0.59048\n",
      "2018-11-13 20:09:12,079 - Xworkers_1mill_0 - INFO -   6700;     2; 71320.7;   0.124; 0.12172; 0.09981; 0.90384; 0.55127; 0.61597; 0.58182; 0.52526\n",
      "2018-11-13 20:09:12,079 - Xworkers_1mill_0 - INFO -   6700;     2; 71320.7;   0.124; 0.12172; 0.09981; 0.90384; 0.55127; 0.61597; 0.58182; 0.52526\n",
      "2018-11-13 20:09:24,512 - Xworkers_1mill_0 - INFO -   6800;     2; 71088.4;   0.124; 0.11579; 0.09981; 0.92520; 0.58506; 0.69604; 0.63574; 0.60585\n",
      "2018-11-13 20:09:24,512 - Xworkers_1mill_0 - INFO -   6800;     2; 71088.4;   0.124; 0.11579; 0.09981; 0.92520; 0.58506; 0.69604; 0.63574; 0.60585\n",
      "2018-11-13 20:09:36,880 - Xworkers_1mill_0 - INFO -   6900;     2; 70976.4;   0.125; 0.11686; 0.09980; 0.92203; 0.57431; 0.66210; 0.61509; 0.58393\n",
      "2018-11-13 20:09:36,880 - Xworkers_1mill_0 - INFO -   6900;     2; 70976.4;   0.125; 0.11686; 0.09980; 0.92203; 0.57431; 0.66210; 0.61509; 0.58393\n",
      "2018-11-13 20:09:49,231 - Xworkers_1mill_0 - INFO -   7000;     2; 73275.5;   0.121; 0.12370; 0.09980; 0.92271; 0.55081; 0.65226; 0.59726; 0.55716\n",
      "2018-11-13 20:09:49,231 - Xworkers_1mill_0 - INFO -   7000;     2; 73275.5;   0.121; 0.12370; 0.09980; 0.92271; 0.55081; 0.65226; 0.59726; 0.55716\n",
      "2018-11-13 20:10:01,636 - Xworkers_1mill_0 - INFO -   7100;     2; 71240.9;   0.124; 0.12483; 0.09980; 0.92124; 0.58366; 0.66023; 0.61959; 0.59512\n",
      "2018-11-13 20:10:01,636 - Xworkers_1mill_0 - INFO -   7100;     2; 71240.9;   0.124; 0.12483; 0.09980; 0.92124; 0.58366; 0.66023; 0.61959; 0.59512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-13 20:10:14,011 - Xworkers_1mill_0 - INFO -   7200;     2; 73060.3;   0.121; 0.12450; 0.09980; 0.92316; 0.50925; 0.52431; 0.51667; 0.50076\n",
      "2018-11-13 20:10:14,011 - Xworkers_1mill_0 - INFO -   7200;     2; 73060.3;   0.121; 0.12450; 0.09980; 0.92316; 0.50925; 0.52431; 0.51667; 0.50076\n",
      "2018-11-13 20:10:26,318 - Xworkers_1mill_0 - INFO -   7300;     2; 71999.0;   0.123; 0.11719; 0.09979; 0.92927; 0.57274; 0.67399; 0.61925; 0.58653\n",
      "2018-11-13 20:10:26,318 - Xworkers_1mill_0 - INFO -   7300;     2; 71999.0;   0.123; 0.11719; 0.09979; 0.92927; 0.57274; 0.67399; 0.61925; 0.58653\n",
      "2018-11-13 20:10:38,758 - Xworkers_1mill_0 - INFO -   7400;     2; 72786.9;   0.122; 0.11699; 0.09979; 0.92113; 0.60384; 0.69035; 0.64420; 0.61347\n",
      "2018-11-13 20:10:38,758 - Xworkers_1mill_0 - INFO -   7400;     2; 72786.9;   0.122; 0.11699; 0.09979; 0.92113; 0.60384; 0.69035; 0.64420; 0.61347\n",
      "2018-11-13 20:10:51,165 - Xworkers_1mill_0 - INFO -   7500;     2; 73117.7;   0.121; 0.10994; 0.09979; 0.93130; 0.64540; 0.68628; 0.66522; 0.63897\n",
      "2018-11-13 20:10:51,165 - Xworkers_1mill_0 - INFO -   7500;     2; 73117.7;   0.121; 0.10994; 0.09979; 0.93130; 0.64540; 0.68628; 0.66522; 0.63897\n",
      "2018-11-13 20:11:03,540 - Xworkers_1mill_0 - INFO -   7600;     2; 71977.9;   0.123; 0.11419; 0.09978; 0.92339; 0.52405; 0.53568; 0.52980; 0.50380\n",
      "2018-11-13 20:11:03,540 - Xworkers_1mill_0 - INFO -   7600;     2; 71977.9;   0.123; 0.11419; 0.09978; 0.92339; 0.52405; 0.53568; 0.52980; 0.50380\n",
      "2018-11-13 20:11:15,902 - Xworkers_1mill_0 - INFO -   7700;     2; 70204.1;   0.126; 0.11917; 0.09978; 0.92712; 0.56119; 0.70928; 0.62660; 0.57929\n",
      "2018-11-13 20:11:15,902 - Xworkers_1mill_0 - INFO -   7700;     2; 70204.1;   0.126; 0.11917; 0.09978; 0.92712; 0.56119; 0.70928; 0.62660; 0.57929\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint written to /home/ubuntu/summ_15ep_2wrk/checkpoint-7710\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-13 20:11:29,323 - Xworkers_1mill_0 - INFO -   7800;     2; 73889.4;   0.120; 0.12018; 0.09978; 0.92249; 0.54269; 0.66127; 0.59614; 0.54643\n",
      "2018-11-13 20:11:29,323 - Xworkers_1mill_0 - INFO -   7800;     2; 73889.4;   0.120; 0.12018; 0.09978; 0.92249; 0.54269; 0.66127; 0.59614; 0.54643\n",
      "2018-11-13 20:11:41,837 - Xworkers_1mill_0 - INFO -   7900;     2; 73616.5;   0.120; 0.11912; 0.09978; 0.91751; 0.56494; 0.62945; 0.59545; 0.55506\n",
      "2018-11-13 20:11:41,837 - Xworkers_1mill_0 - INFO -   7900;     2; 73616.5;   0.120; 0.11912; 0.09978; 0.91751; 0.56494; 0.62945; 0.59545; 0.55506\n",
      "2018-11-13 20:11:54,211 - Xworkers_1mill_0 - INFO -   8000;     2; 71795.1;   0.123; 0.11322; 0.09977; 0.92102; 0.52768; 0.52279; 0.52522; 0.50699\n",
      "2018-11-13 20:11:54,211 - Xworkers_1mill_0 - INFO -   8000;     2; 71795.1;   0.123; 0.11322; 0.09977; 0.92102; 0.52768; 0.52279; 0.52522; 0.50699\n",
      "2018-11-13 20:12:06,643 - Xworkers_1mill_0 - INFO -   8100;     2; 71686.2;   0.123; 0.12736; 0.09977; 0.91469; 0.52835; 0.62742; 0.57364; 0.53539\n",
      "2018-11-13 20:12:06,643 - Xworkers_1mill_0 - INFO -   8100;     2; 71686.2;   0.123; 0.12736; 0.09977; 0.91469; 0.52835; 0.62742; 0.57364; 0.53539\n",
      "2018-11-13 20:12:19,035 - Xworkers_1mill_0 - INFO -   8200;     2; 70006.3;   0.126; 0.11990; 0.09977; 0.91548; 0.53524; 0.64375; 0.58450; 0.53751\n",
      "2018-11-13 20:12:19,035 - Xworkers_1mill_0 - INFO -   8200;     2; 70006.3;   0.126; 0.11990; 0.09977; 0.91548; 0.53524; 0.64375; 0.58450; 0.53751\n",
      "2018-11-13 20:12:31,556 - Xworkers_1mill_0 - INFO -   8300;     2; 72186.9;   0.123; 0.12028; 0.09976; 0.92452; 0.60425; 0.68094; 0.64030; 0.61893\n",
      "2018-11-13 20:12:31,556 - Xworkers_1mill_0 - INFO -   8300;     2; 72186.9;   0.123; 0.12028; 0.09976; 0.92452; 0.60425; 0.68094; 0.64030; 0.61893\n",
      "2018-11-13 20:12:43,990 - Xworkers_1mill_0 - INFO -   8400;     2; 72996.9;   0.121; 0.12448; 0.09976; 0.91390; 0.53248; 0.65016; 0.58547; 0.54074\n",
      "2018-11-13 20:12:43,990 - Xworkers_1mill_0 - INFO -   8400;     2; 72996.9;   0.121; 0.12448; 0.09976; 0.91390; 0.53248; 0.65016; 0.58547; 0.54074\n",
      "2018-11-13 20:12:56,329 - Xworkers_1mill_0 - INFO -   8500;     2; 71804.2;   0.123; 0.11412; 0.09976; 0.92011; 0.55795; 0.66840; 0.60820; 0.56038\n",
      "2018-11-13 20:12:56,329 - Xworkers_1mill_0 - INFO -   8500;     2; 71804.2;   0.123; 0.11412; 0.09976; 0.92011; 0.55795; 0.66840; 0.60820; 0.56038\n",
      "2018-11-13 20:13:08,771 - Xworkers_1mill_0 - INFO -   8600;     2; 72581.0;   0.122; 0.11346; 0.09976; 0.91774; 0.58875; 0.68010; 0.63114; 0.59526\n",
      "2018-11-13 20:13:08,771 - Xworkers_1mill_0 - INFO -   8600;     2; 72581.0;   0.122; 0.11346; 0.09976; 0.91774; 0.58875; 0.68010; 0.63114; 0.59526\n",
      "2018-11-13 20:13:21,150 - Xworkers_1mill_0 - INFO -   8700;     2; 64166.9;   0.138; 0.11387; 0.09975; 0.92045; 0.54676; 0.63835; 0.58902; 0.55614\n",
      "2018-11-13 20:13:21,150 - Xworkers_1mill_0 - INFO -   8700;     2; 64166.9;   0.138; 0.11387; 0.09975; 0.92045; 0.54676; 0.63835; 0.58902; 0.55614\n",
      "2018-11-13 20:15:04,526 - Xworkers_1mill_0 - INFO - valid - Number of batches: 14; batch_size: 150000; Total Time: 0:01:43.372359\n",
      "2018-11-13 20:15:04,526 - Xworkers_1mill_0 - INFO - valid - Number of batches: 14; batch_size: 150000; Total Time: 0:01:43.372359\n",
      "2018-11-13 20:15:04,528 - Xworkers_1mill_0 - INFO - ---Validation--- Training Step: 8700; Training Epoch: 2; \n",
      " Confusion Matrix:\n",
      " [[  13142     496      42    1255   32011    1037       0]\n",
      " [      1   39276    2590    1244   14823      84       0]\n",
      " [      0   11401    7385    3636       0      55       0]\n",
      " [      0       0    3807   61012       6    5497       0]\n",
      " [     94   72502    1277    3259 1730739     753       0]\n",
      " [      0       9     268    6725      10   66030       0]\n",
      " [      0       0       0     195       1    1533     716]]\n",
      "2018-11-13 20:15:04,528 - Xworkers_1mill_0 - INFO - ---Validation--- Training Step: 8700; Training Epoch: 2; \n",
      " Confusion Matrix:\n",
      " [[  13142     496      42    1255   32011    1037       0]\n",
      " [      1   39276    2590    1244   14823      84       0]\n",
      " [      0   11401    7385    3636       0      55       0]\n",
      " [      0       0    3807   61012       6    5497       0]\n",
      " [     94   72502    1277    3259 1730739     753       0]\n",
      " [      0       9     268    6725      10   66030       0]\n",
      " [      0       0       0     195       1    1533     716]]\n",
      "2018-11-13 20:15:04,533 - Xworkers_1mill_0 - INFO - (Training Step, Training Epoch, loss, accuracy, recall, precision, f1score_micro, f1score_macro) in Validation:   8700;     2; 0.09508; 0.92100; 0.61346; 0.77631; 0.68532; 0.62548\n",
      "2018-11-13 20:15:04,533 - Xworkers_1mill_0 - INFO - (Training Step, Training Epoch, loss, accuracy, recall, precision, f1score_micro, f1score_macro) in Validation:   8700;     2; 0.09508; 0.92100; 0.61346; 0.77631; 0.68532; 0.62548\n",
      "2018-11-13 20:15:16,577 - Xworkers_1mill_0 - INFO -   8800;     3; 73476.6;   0.120; 0.12369; 0.09975; 0.92169; 0.54980; 0.64653; 0.59425; 0.56383\n",
      "2018-11-13 20:15:16,577 - Xworkers_1mill_0 - INFO -   8800;     3; 73476.6;   0.120; 0.12369; 0.09975; 0.92169; 0.54980; 0.64653; 0.59425; 0.56383\n",
      "2018-11-13 20:15:28,586 - Xworkers_1mill_0 - INFO -   8900;     3; 74282.4;   0.119; 0.12773; 0.09975; 0.91243; 0.53909; 0.65892; 0.59301; 0.54726\n",
      "2018-11-13 20:15:28,586 - Xworkers_1mill_0 - INFO -   8900;     3; 74282.4;   0.119; 0.12773; 0.09975; 0.91243; 0.53909; 0.65892; 0.59301; 0.54726\n",
      "2018-11-13 20:15:40,616 - Xworkers_1mill_0 - INFO -   9000;     3; 73878.2;   0.120; 0.11349; 0.09974; 0.91333; 0.61018; 0.63988; 0.62468; 0.59249\n",
      "2018-11-13 20:15:40,616 - Xworkers_1mill_0 - INFO -   9000;     3; 73878.2;   0.120; 0.11349; 0.09974; 0.91333; 0.61018; 0.63988; 0.62468; 0.59249\n",
      "2018-11-13 20:15:52,890 - Xworkers_1mill_0 - INFO -   9100;     3; 72269.7;   0.122; 0.11660; 0.09974; 0.91763; 0.57580; 0.65876; 0.61449; 0.58271\n",
      "2018-11-13 20:15:52,890 - Xworkers_1mill_0 - INFO -   9100;     3; 72269.7;   0.122; 0.11660; 0.09974; 0.91763; 0.57580; 0.65876; 0.61449; 0.58271\n",
      "2018-11-13 20:16:05,306 - Xworkers_1mill_0 - INFO -   9200;     3; 72466.4;   0.122; 0.11446; 0.09974; 0.91559; 0.55549; 0.65915; 0.60290; 0.55288\n",
      "2018-11-13 20:16:05,306 - Xworkers_1mill_0 - INFO -   9200;     3; 72466.4;   0.122; 0.11446; 0.09974; 0.91559; 0.55549; 0.65915; 0.60290; 0.55288\n",
      "2018-11-13 20:16:17,622 - Xworkers_1mill_0 - INFO -   9300;     3; 73977.3;   0.120; 0.11408; 0.09974; 0.92339; 0.53337; 0.59308; 0.56165; 0.52974\n",
      "2018-11-13 20:16:17,622 - Xworkers_1mill_0 - INFO -   9300;     3; 73977.3;   0.120; 0.11408; 0.09974; 0.92339; 0.53337; 0.59308; 0.56165; 0.52974\n",
      "2018-11-13 20:16:29,908 - Xworkers_1mill_0 - INFO -   9400;     3; 74291.3;   0.119; 0.12029; 0.09973; 0.91548; 0.57916; 0.63209; 0.60447; 0.57980\n",
      "2018-11-13 20:16:29,908 - Xworkers_1mill_0 - INFO -   9400;     3; 74291.3;   0.119; 0.12029; 0.09973; 0.91548; 0.57916; 0.63209; 0.60447; 0.57980\n",
      "2018-11-13 20:16:42,120 - Xworkers_1mill_0 - INFO -   9500;     3; 71516.7;   0.124; 0.11843; 0.09973; 0.92452; 0.56372; 0.65372; 0.60539; 0.56687\n",
      "2018-11-13 20:16:42,120 - Xworkers_1mill_0 - INFO -   9500;     3; 71516.7;   0.124; 0.11843; 0.09973; 0.92452; 0.56372; 0.65372; 0.60539; 0.56687\n",
      "2018-11-13 20:16:54,407 - Xworkers_1mill_0 - INFO -   9600;     3; 74666.0;   0.119; 0.12096; 0.09973; 0.92023; 0.56156; 0.66308; 0.60811; 0.56553\n",
      "2018-11-13 20:16:54,407 - Xworkers_1mill_0 - INFO -   9600;     3; 74666.0;   0.119; 0.12096; 0.09973; 0.92023; 0.56156; 0.66308; 0.60811; 0.56553\n",
      "2018-11-13 20:17:06,781 - Xworkers_1mill_0 - INFO -   9700;     3; 72876.8;   0.121; 0.11788; 0.09973; 0.92441; 0.59048; 0.69877; 0.64008; 0.60119\n",
      "2018-11-13 20:17:06,781 - Xworkers_1mill_0 - INFO -   9700;     3; 72876.8;   0.121; 0.11788; 0.09973; 0.92441; 0.59048; 0.69877; 0.64008; 0.60119\n",
      "2018-11-13 20:17:19,118 - Xworkers_1mill_0 - INFO -   9800;     3; 71496.2;   0.124; 0.11284; 0.09972; 0.92282; 0.58644; 0.65183; 0.61741; 0.59048\n",
      "2018-11-13 20:17:19,118 - Xworkers_1mill_0 - INFO -   9800;     3; 71496.2;   0.124; 0.11284; 0.09972; 0.92282; 0.58644; 0.65183; 0.61741; 0.59048\n",
      "2018-11-13 20:17:31,424 - Xworkers_1mill_0 - INFO -   9900;     3; 70712.3;   0.125; 0.11586; 0.09972; 0.92136; 0.53816; 0.61795; 0.57530; 0.53313\n",
      "2018-11-13 20:17:31,424 - Xworkers_1mill_0 - INFO -   9900;     3; 70712.3;   0.125; 0.11586; 0.09972; 0.92136; 0.53816; 0.61795; 0.57530; 0.53313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-13 20:17:43,745 - Xworkers_1mill_0 - INFO -  10000;     3; 70796.4;   0.125; 0.11395; 0.09972; 0.92260; 0.55188; 0.66425; 0.60287; 0.54575\n",
      "2018-11-13 20:17:43,745 - Xworkers_1mill_0 - INFO -  10000;     3; 70796.4;   0.125; 0.11395; 0.09972; 0.92260; 0.55188; 0.66425; 0.60287; 0.54575\n",
      "2018-11-13 20:17:56,118 - Xworkers_1mill_0 - INFO -  10100;     3; 72709.7;   0.122; 0.11988; 0.09971; 0.92576; 0.59365; 0.71249; 0.64766; 0.61389\n",
      "2018-11-13 20:17:56,118 - Xworkers_1mill_0 - INFO -  10100;     3; 72709.7;   0.122; 0.11988; 0.09971; 0.92576; 0.59365; 0.71249; 0.64766; 0.61389\n",
      "2018-11-13 20:18:08,433 - Xworkers_1mill_0 - INFO -  10200;     3; 70410.5;   0.126; 0.11596; 0.09971; 0.91977; 0.57150; 0.68028; 0.62116; 0.58463\n",
      "2018-11-13 20:18:08,433 - Xworkers_1mill_0 - INFO -  10200;     3; 70410.5;   0.126; 0.11596; 0.09971; 0.91977; 0.57150; 0.68028; 0.62116; 0.58463\n",
      "2018-11-13 20:18:20,764 - Xworkers_1mill_0 - INFO -  10300;     3; 72115.5;   0.123; 0.11773; 0.09971; 0.91921; 0.53317; 0.50796; 0.52026; 0.50928\n",
      "2018-11-13 20:18:20,764 - Xworkers_1mill_0 - INFO -  10300;     3; 72115.5;   0.123; 0.11773; 0.09971; 0.91921; 0.53317; 0.50796; 0.52026; 0.50928\n",
      "2018-11-13 20:18:33,125 - Xworkers_1mill_0 - INFO -  10400;     3; 70536.0;   0.125; 0.11538; 0.09971; 0.92068; 0.55884; 0.67947; 0.61328; 0.56917\n",
      "2018-11-13 20:18:33,125 - Xworkers_1mill_0 - INFO -  10400;     3; 70536.0;   0.125; 0.11538; 0.09971; 0.92068; 0.55884; 0.67947; 0.61328; 0.56917\n",
      "2018-11-13 20:18:45,583 - Xworkers_1mill_0 - INFO -  10500;     3; 71136.1;   0.124; 0.12013; 0.09970; 0.92090; 0.52175; 0.67369; 0.58806; 0.50909\n",
      "2018-11-13 20:18:45,583 - Xworkers_1mill_0 - INFO -  10500;     3; 71136.1;   0.124; 0.12013; 0.09970; 0.92090; 0.52175; 0.67369; 0.58806; 0.50909\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint written to /home/ubuntu/summ_15ep_2wrk/checkpoint-10513\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-13 20:18:58,924 - Xworkers_1mill_0 - INFO -  10600;     3; 72224.7;   0.123; 0.11742; 0.09970; 0.92373; 0.50872; 0.54520; 0.52633; 0.50143\n",
      "2018-11-13 20:18:58,924 - Xworkers_1mill_0 - INFO -  10600;     3; 72224.7;   0.123; 0.11742; 0.09970; 0.92373; 0.50872; 0.54520; 0.52633; 0.50143\n",
      "2018-11-13 20:19:11,340 - Xworkers_1mill_0 - INFO -  10700;     3; 71761.0;   0.123; 0.12641; 0.09970; 0.91853; 0.52818; 0.67278; 0.59177; 0.52324\n",
      "2018-11-13 20:19:11,340 - Xworkers_1mill_0 - INFO -  10700;     3; 71761.0;   0.123; 0.12641; 0.09970; 0.91853; 0.52818; 0.67278; 0.59177; 0.52324\n",
      "2018-11-13 20:19:23,748 - Xworkers_1mill_0 - INFO -  10800;     3; 70267.7;   0.126; 0.12594; 0.09969; 0.90983; 0.55686; 0.67524; 0.61036; 0.55467\n",
      "2018-11-13 20:19:23,748 - Xworkers_1mill_0 - INFO -  10800;     3; 70267.7;   0.126; 0.12594; 0.09969; 0.90983; 0.55686; 0.67524; 0.61036; 0.55467\n",
      "2018-11-13 20:19:36,158 - Xworkers_1mill_0 - INFO -  10900;     3; 70682.4;   0.125; 0.11364; 0.09969; 0.92384; 0.56504; 0.67312; 0.61436; 0.57471\n",
      "2018-11-13 20:19:36,158 - Xworkers_1mill_0 - INFO -  10900;     3; 70682.4;   0.125; 0.11364; 0.09969; 0.92384; 0.56504; 0.67312; 0.61436; 0.57471\n",
      "2018-11-13 20:19:48,538 - Xworkers_1mill_0 - INFO -  11000;     3; 72231.3;   0.123; 0.12829; 0.09969; 0.92079; 0.56898; 0.67730; 0.61843; 0.58641\n",
      "2018-11-13 20:19:48,538 - Xworkers_1mill_0 - INFO -  11000;     3; 72231.3;   0.123; 0.12829; 0.09969; 0.92079; 0.56898; 0.67730; 0.61843; 0.58641\n",
      "2018-11-13 20:20:00,981 - Xworkers_1mill_0 - INFO -  11100;     3; 70918.4;   0.125; 0.11707; 0.09969; 0.92169; 0.57196; 0.66542; 0.61516; 0.57814\n",
      "2018-11-13 20:20:00,981 - Xworkers_1mill_0 - INFO -  11100;     3; 70918.4;   0.125; 0.11707; 0.09969; 0.92169; 0.57196; 0.66542; 0.61516; 0.57814\n",
      "2018-11-13 20:20:13,455 - Xworkers_1mill_0 - INFO -  11200;     3; 72625.0;   0.122; 0.11595; 0.09968; 0.92045; 0.59129; 0.65374; 0.62095; 0.60072\n",
      "2018-11-13 20:20:13,455 - Xworkers_1mill_0 - INFO -  11200;     3; 72625.0;   0.122; 0.11595; 0.09968; 0.92045; 0.59129; 0.65374; 0.62095; 0.60072\n",
      "2018-11-13 20:20:25,921 - Xworkers_1mill_0 - INFO -  11300;     3; 72090.5;   0.123; 0.11726; 0.09968; 0.92068; 0.55724; 0.63945; 0.59552; 0.55389\n",
      "2018-11-13 20:20:25,921 - Xworkers_1mill_0 - INFO -  11300;     3; 72090.5;   0.123; 0.11726; 0.09968; 0.92068; 0.55724; 0.63945; 0.59552; 0.55389\n",
      "2018-11-13 20:20:38,340 - Xworkers_1mill_0 - INFO -  11400;     3; 73630.6;   0.120; 0.12061; 0.09968; 0.90316; 0.52383; 0.58215; 0.55145; 0.48709\n",
      "2018-11-13 20:20:38,340 - Xworkers_1mill_0 - INFO -  11400;     3; 73630.6;   0.120; 0.12061; 0.09968; 0.90316; 0.52383; 0.58215; 0.55145; 0.48709\n",
      "2018-11-13 20:20:50,757 - Xworkers_1mill_0 - INFO -  11500;     3; 71448.5;   0.124; 0.12573; 0.09967; 0.92136; 0.52847; 0.68486; 0.59659; 0.53582\n",
      "2018-11-13 20:20:50,757 - Xworkers_1mill_0 - INFO -  11500;     3; 71448.5;   0.124; 0.12573; 0.09967; 0.92136; 0.52847; 0.68486; 0.59659; 0.53582\n",
      "2018-11-13 20:21:03,117 - Xworkers_1mill_0 - INFO -  11600;     3; 71804.3;   0.123; 0.12353; 0.09967; 0.91887; 0.57731; 0.69468; 0.63058; 0.59168\n",
      "2018-11-13 20:21:03,117 - Xworkers_1mill_0 - INFO -  11600;     3; 71804.3;   0.123; 0.12353; 0.09967; 0.91887; 0.57731; 0.69468; 0.63058; 0.59168\n",
      "2018-11-13 20:21:15,406 - Xworkers_1mill_0 - INFO -  11700;     3; 72281.9;   0.122; 0.11857; 0.09967; 0.91141; 0.54303; 0.58415; 0.56284; 0.53007\n",
      "2018-11-13 20:21:15,406 - Xworkers_1mill_0 - INFO -  11700;     3; 72281.9;   0.122; 0.11857; 0.09967; 0.91141; 0.54303; 0.58415; 0.56284; 0.53007\n",
      "2018-11-13 20:21:27,671 - Xworkers_1mill_0 - INFO -  11800;     3; 72132.9;   0.123; 0.11420; 0.09967; 0.92102; 0.58148; 0.65570; 0.61636; 0.58462\n",
      "2018-11-13 20:21:27,671 - Xworkers_1mill_0 - INFO -  11800;     3; 72132.9;   0.123; 0.11420; 0.09967; 0.92102; 0.58148; 0.65570; 0.61636; 0.58462\n",
      "2018-11-13 20:21:39,936 - Xworkers_1mill_0 - INFO -  11900;     3; 72412.4;   0.122; 0.11312; 0.09966; 0.92452; 0.59538; 0.64843; 0.62078; 0.60208\n",
      "2018-11-13 20:21:39,936 - Xworkers_1mill_0 - INFO -  11900;     3; 72412.4;   0.122; 0.11312; 0.09966; 0.92452; 0.59538; 0.64843; 0.62078; 0.60208\n",
      "2018-11-13 20:21:52,249 - Xworkers_1mill_0 - INFO -  12000;     3; 72817.9;   0.122; 0.12033; 0.09966; 0.92068; 0.54683; 0.60339; 0.57372; 0.54682\n",
      "2018-11-13 20:21:52,249 - Xworkers_1mill_0 - INFO -  12000;     3; 72817.9;   0.122; 0.12033; 0.09966; 0.92068; 0.54683; 0.60339; 0.57372; 0.54682\n",
      "2018-11-13 20:22:04,602 - Xworkers_1mill_0 - INFO -  12100;     3; 72977.1;   0.121; 0.10931; 0.09966; 0.92723; 0.55283; 0.66480; 0.60367; 0.54917\n",
      "2018-11-13 20:22:04,602 - Xworkers_1mill_0 - INFO -  12100;     3; 72977.1;   0.121; 0.10931; 0.09966; 0.92723; 0.55283; 0.66480; 0.60367; 0.54917\n",
      "2018-11-13 20:22:16,985 - Xworkers_1mill_0 - INFO -  12200;     3; 58684.2;   0.151; 0.11434; 0.09965; 0.92531; 0.55935; 0.65684; 0.60419; 0.56811\n",
      "2018-11-13 20:22:16,985 - Xworkers_1mill_0 - INFO -  12200;     3; 58684.2;   0.151; 0.11434; 0.09965; 0.92531; 0.55935; 0.65684; 0.60419; 0.56811\n",
      "2018-11-13 20:22:29,358 - Xworkers_1mill_0 - INFO -  12300;     3; 73604.0;   0.120; 0.11386; 0.09965; 0.92486; 0.54961; 0.60435; 0.57568; 0.55163\n",
      "2018-11-13 20:22:29,358 - Xworkers_1mill_0 - INFO -  12300;     3; 73604.0;   0.120; 0.11386; 0.09965; 0.92486; 0.54961; 0.60435; 0.57568; 0.55163\n",
      "2018-11-13 20:22:41,666 - Xworkers_1mill_0 - INFO -  12400;     3; 72124.6;   0.123; 0.12097; 0.09965; 0.92000; 0.56465; 0.66404; 0.61032; 0.57644\n",
      "2018-11-13 20:22:41,666 - Xworkers_1mill_0 - INFO -  12400;     3; 72124.6;   0.123; 0.12097; 0.09965; 0.92000; 0.56465; 0.66404; 0.61032; 0.57644\n",
      "2018-11-13 20:22:54,017 - Xworkers_1mill_0 - INFO -  12500;     3; 72433.8;   0.122; 0.12590; 0.09965; 0.90768; 0.52974; 0.64250; 0.58070; 0.51853\n",
      "2018-11-13 20:22:54,017 - Xworkers_1mill_0 - INFO -  12500;     3; 72433.8;   0.122; 0.12590; 0.09965; 0.90768; 0.52974; 0.64250; 0.58070; 0.51853\n",
      "2018-11-13 20:23:06,367 - Xworkers_1mill_0 - INFO -  12600;     3; 72766.5;   0.122; 0.12099; 0.09964; 0.91910; 0.57032; 0.67226; 0.61711; 0.57435\n",
      "2018-11-13 20:23:06,367 - Xworkers_1mill_0 - INFO -  12600;     3; 72766.5;   0.122; 0.12099; 0.09964; 0.91910; 0.57032; 0.67226; 0.61711; 0.57435\n",
      "2018-11-13 20:23:18,686 - Xworkers_1mill_0 - INFO -  12700;     3; 73570.1;   0.120; 0.11331; 0.09964; 0.92068; 0.55392; 0.65983; 0.60226; 0.55530\n",
      "2018-11-13 20:23:18,686 - Xworkers_1mill_0 - INFO -  12700;     3; 73570.1;   0.120; 0.11331; 0.09964; 0.92068; 0.55392; 0.65983; 0.60226; 0.55530\n",
      "2018-11-13 20:23:31,047 - Xworkers_1mill_0 - INFO -  12800;     3; 73027.9;   0.121; 0.11798; 0.09964; 0.91492; 0.56573; 0.59567; 0.58032; 0.56093\n",
      "2018-11-13 20:23:31,047 - Xworkers_1mill_0 - INFO -  12800;     3; 73027.9;   0.121; 0.11798; 0.09964; 0.91492; 0.56573; 0.59567; 0.58032; 0.56093\n",
      "2018-11-13 20:23:43,336 - Xworkers_1mill_0 - INFO -  12900;     3; 73081.1;   0.121; 0.11714; 0.09963; 0.92689; 0.57254; 0.67651; 0.62020; 0.58061\n",
      "2018-11-13 20:23:43,336 - Xworkers_1mill_0 - INFO -  12900;     3; 73081.1;   0.121; 0.11714; 0.09963; 0.92689; 0.57254; 0.67651; 0.62020; 0.58061\n",
      "2018-11-13 20:23:55,673 - Xworkers_1mill_0 - INFO -  13000;     3; 71089.9;   0.124; 0.12513; 0.09963; 0.91774; 0.55346; 0.66668; 0.60482; 0.55950\n",
      "2018-11-13 20:23:55,673 - Xworkers_1mill_0 - INFO -  13000;     3; 71089.9;   0.124; 0.12513; 0.09963; 0.91774; 0.55346; 0.66668; 0.60482; 0.55950\n",
      "2018-11-13 20:25:45,732 - Xworkers_1mill_0 - INFO - valid - Number of batches: 14; batch_size: 150000; Total Time: 0:01:43.879334\n",
      "2018-11-13 20:25:45,732 - Xworkers_1mill_0 - INFO - valid - Number of batches: 14; batch_size: 150000; Total Time: 0:01:43.879334\n",
      "2018-11-13 20:25:45,734 - Xworkers_1mill_0 - INFO - ---Validation--- Training Step: 13050; Training Epoch: 3; \n",
      " Confusion Matrix:\n",
      " [[  13148     488      40    1255   32019    1033       0]\n",
      " [      1   39101    2545    1226   15061      84       0]\n",
      " [      0   11406    7492    3524       0      55       0]\n",
      " [      0       3    3731   61092       6    5490       0]\n",
      " [    126   66061    1266    3238 1737178     755       0]\n",
      " [      0       9     282    6711      10   66030       0]\n",
      " [      1       0       0     195       1    1481     767]]\n",
      "2018-11-13 20:25:45,734 - Xworkers_1mill_0 - INFO - ---Validation--- Training Step: 13050; Training Epoch: 3; \n",
      " Confusion Matrix:\n",
      " [[  13148     488      40    1255   32019    1033       0]\n",
      " [      1   39101    2545    1226   15061      84       0]\n",
      " [      0   11406    7492    3524       0      55       0]\n",
      " [      0       3    3731   61092       6    5490       0]\n",
      " [    126   66061    1266    3238 1737178     755       0]\n",
      " [      0       9     282    6711      10   66030       0]\n",
      " [      1       0       0     195       1    1481     767]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-13 20:25:45,739 - Xworkers_1mill_0 - INFO - (Training Step, Training Epoch, loss, accuracy, recall, precision, f1score_micro, f1score_macro) in Validation:  13050;     3; 0.09127; 0.92412; 0.61736; 0.77964; 0.68906; 0.63241\n",
      "2018-11-13 20:25:45,739 - Xworkers_1mill_0 - INFO - (Training Step, Training Epoch, loss, accuracy, recall, precision, f1score_micro, f1score_macro) in Validation:  13050;     3; 0.09127; 0.92412; 0.61736; 0.77964; 0.68906; 0.63241\n",
      "2018-11-13 20:25:51,831 - Xworkers_1mill_0 - INFO -  13100;     4; 73712.4;   0.120; 0.11613; 0.09963; 0.91582; 0.58598; 0.68333; 0.63092; 0.59614\n",
      "2018-11-13 20:25:51,831 - Xworkers_1mill_0 - INFO -  13100;     4; 73712.4;   0.120; 0.11613; 0.09963; 0.91582; 0.58598; 0.68333; 0.63092; 0.59614\n",
      "2018-11-13 20:26:03,979 - Xworkers_1mill_0 - INFO -  13200;     4; 74901.8;   0.118; 0.12018; 0.09963; 0.91028; 0.62865; 0.66216; 0.64497; 0.62661\n",
      "2018-11-13 20:26:03,979 - Xworkers_1mill_0 - INFO -  13200;     4; 74901.8;   0.118; 0.12018; 0.09963; 0.91028; 0.62865; 0.66216; 0.64497; 0.62661\n",
      "2018-11-13 20:26:16,037 - Xworkers_1mill_0 - INFO -  13300;     4; 73744.9;   0.120; 0.10920; 0.09962; 0.91254; 0.57600; 0.64214; 0.60728; 0.56806\n",
      "2018-11-13 20:26:16,037 - Xworkers_1mill_0 - INFO -  13300;     4; 73744.9;   0.120; 0.10920; 0.09962; 0.91254; 0.57600; 0.64214; 0.60728; 0.56806\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint written to /home/ubuntu/summ_15ep_2wrk/checkpoint-13309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-13 20:26:29,290 - Xworkers_1mill_0 - INFO -  13400;     4; 73468.2;   0.120; 0.11635; 0.09962; 0.91751; 0.56119; 0.64643; 0.60080; 0.56452\n",
      "2018-11-13 20:26:29,290 - Xworkers_1mill_0 - INFO -  13400;     4; 73468.2;   0.120; 0.11635; 0.09962; 0.91751; 0.56119; 0.64643; 0.60080; 0.56452\n",
      "2018-11-13 20:26:41,529 - Xworkers_1mill_0 - INFO -  13500;     4; 70779.9;   0.125; 0.11393; 0.09962; 0.92237; 0.61535; 0.66716; 0.64021; 0.61479\n",
      "2018-11-13 20:26:41,529 - Xworkers_1mill_0 - INFO -  13500;     4; 70779.9;   0.125; 0.11393; 0.09962; 0.92237; 0.61535; 0.66716; 0.64021; 0.61479\n",
      "2018-11-13 20:26:53,820 - Xworkers_1mill_0 - INFO -  13600;     4; 72506.3;   0.122; 0.12022; 0.09962; 0.91277; 0.53145; 0.64465; 0.58260; 0.53546\n",
      "2018-11-13 20:26:53,820 - Xworkers_1mill_0 - INFO -  13600;     4; 72506.3;   0.122; 0.12022; 0.09962; 0.91277; 0.53145; 0.64465; 0.58260; 0.53546\n",
      "2018-11-13 20:27:06,133 - Xworkers_1mill_0 - INFO -  13700;     4; 71729.0;   0.123; 0.12038; 0.09961; 0.92475; 0.57270; 0.67428; 0.61935; 0.58962\n",
      "2018-11-13 20:27:06,133 - Xworkers_1mill_0 - INFO -  13700;     4; 71729.0;   0.123; 0.12038; 0.09961; 0.92475; 0.57270; 0.67428; 0.61935; 0.58962\n",
      "2018-11-13 20:27:18,411 - Xworkers_1mill_0 - INFO -  13800;     4; 72953.7;   0.121; 0.11580; 0.09961; 0.91605; 0.56961; 0.65924; 0.61116; 0.57060\n",
      "2018-11-13 20:27:18,411 - Xworkers_1mill_0 - INFO -  13800;     4; 72953.7;   0.121; 0.11580; 0.09961; 0.91605; 0.56961; 0.65924; 0.61116; 0.57060\n",
      "2018-11-13 20:27:30,734 - Xworkers_1mill_0 - INFO -  13900;     4; 74613.6;   0.119; 0.11372; 0.09961; 0.92169; 0.56437; 0.61639; 0.58923; 0.56308\n",
      "2018-11-13 20:27:30,734 - Xworkers_1mill_0 - INFO -  13900;     4; 74613.6;   0.119; 0.11372; 0.09961; 0.92169; 0.56437; 0.61639; 0.58923; 0.56308\n",
      "2018-11-13 20:27:43,042 - Xworkers_1mill_0 - INFO -  14000;     4; 72154.9;   0.123; 0.11083; 0.09960; 0.92655; 0.59958; 0.68464; 0.63929; 0.61160\n",
      "2018-11-13 20:27:43,042 - Xworkers_1mill_0 - INFO -  14000;     4; 72154.9;   0.123; 0.11083; 0.09960; 0.92655; 0.59958; 0.68464; 0.63929; 0.61160\n",
      "2018-11-13 20:27:55,267 - Xworkers_1mill_0 - INFO -  14100;     4; 72616.8;   0.122; 0.11578; 0.09960; 0.92249; 0.50157; 0.50765; 0.50459; 0.49013\n",
      "2018-11-13 20:27:55,267 - Xworkers_1mill_0 - INFO -  14100;     4; 72616.8;   0.122; 0.11578; 0.09960; 0.92249; 0.50157; 0.50765; 0.50459; 0.49013\n",
      "2018-11-13 20:28:07,534 - Xworkers_1mill_0 - INFO -  14200;     4; 75322.5;   0.117; 0.11740; 0.09960; 0.91763; 0.56102; 0.64319; 0.59930; 0.56663\n",
      "2018-11-13 20:28:07,534 - Xworkers_1mill_0 - INFO -  14200;     4; 75322.5;   0.117; 0.11740; 0.09960; 0.91763; 0.56102; 0.64319; 0.59930; 0.56663\n",
      "2018-11-13 20:28:19,841 - Xworkers_1mill_0 - INFO -  14300;     4; 73544.8;   0.120; 0.11126; 0.09960; 0.92588; 0.63576; 0.66373; 0.64944; 0.63150\n",
      "2018-11-13 20:28:19,841 - Xworkers_1mill_0 - INFO -  14300;     4; 73544.8;   0.120; 0.11126; 0.09960; 0.92588; 0.63576; 0.66373; 0.64944; 0.63150\n",
      "2018-11-13 20:28:32,079 - Xworkers_1mill_0 - INFO -  14400;     4; 73847.6;   0.120; 0.12176; 0.09959; 0.91582; 0.57932; 0.64814; 0.61180; 0.58191\n",
      "2018-11-13 20:28:32,079 - Xworkers_1mill_0 - INFO -  14400;     4; 73847.6;   0.120; 0.12176; 0.09959; 0.91582; 0.57932; 0.64814; 0.61180; 0.58191\n",
      "2018-11-13 20:28:44,370 - Xworkers_1mill_0 - INFO -  14500;     4; 66802.8;   0.132; 0.12083; 0.09959; 0.91356; 0.58739; 0.64397; 0.61438; 0.59007\n",
      "2018-11-13 20:28:44,370 - Xworkers_1mill_0 - INFO -  14500;     4; 66802.8;   0.132; 0.12083; 0.09959; 0.91356; 0.58739; 0.64397; 0.61438; 0.59007\n",
      "2018-11-13 20:28:56,589 - Xworkers_1mill_0 - INFO -  14600;     4; 73757.1;   0.120; 0.11512; 0.09959; 0.92192; 0.54363; 0.70279; 0.61305; 0.54254\n",
      "2018-11-13 20:28:56,589 - Xworkers_1mill_0 - INFO -  14600;     4; 73757.1;   0.120; 0.11512; 0.09959; 0.92192; 0.54363; 0.70279; 0.61305; 0.54254\n",
      "2018-11-13 20:29:08,942 - Xworkers_1mill_0 - INFO -  14700;     4; 71151.1;   0.124; 0.11497; 0.09958; 0.92226; 0.58667; 0.66581; 0.62374; 0.59500\n",
      "2018-11-13 20:29:08,942 - Xworkers_1mill_0 - INFO -  14700;     4; 71151.1;   0.124; 0.11497; 0.09958; 0.92226; 0.58667; 0.66581; 0.62374; 0.59500\n",
      "2018-11-13 20:29:21,294 - Xworkers_1mill_0 - INFO -  14800;     4; 71833.9;   0.123; 0.12492; 0.09958; 0.91469; 0.59307; 0.62379; 0.60805; 0.58929\n",
      "2018-11-13 20:29:21,294 - Xworkers_1mill_0 - INFO -  14800;     4; 71833.9;   0.123; 0.12492; 0.09958; 0.91469; 0.59307; 0.62379; 0.60805; 0.58929\n",
      "2018-11-13 20:29:33,685 - Xworkers_1mill_0 - INFO -  14900;     4; 72649.5;   0.122; 0.11457; 0.09958; 0.91989; 0.57541; 0.63455; 0.60353; 0.57589\n",
      "2018-11-13 20:29:33,685 - Xworkers_1mill_0 - INFO -  14900;     4; 72649.5;   0.122; 0.11457; 0.09958; 0.91989; 0.57541; 0.63455; 0.60353; 0.57589\n",
      "2018-11-13 20:29:46,048 - Xworkers_1mill_0 - INFO -  15000;     4; 71575.4;   0.124; 0.11723; 0.09958; 0.91876; 0.55113; 0.60560; 0.57708; 0.54596\n",
      "2018-11-13 20:29:46,048 - Xworkers_1mill_0 - INFO -  15000;     4; 71575.4;   0.124; 0.11723; 0.09958; 0.91876; 0.55113; 0.60560; 0.57708; 0.54596\n",
      "2018-11-13 20:29:58,393 - Xworkers_1mill_0 - INFO -  15100;     4; 73940.5;   0.120; 0.11713; 0.09957; 0.92192; 0.56853; 0.68595; 0.62174; 0.57918\n",
      "2018-11-13 20:29:58,393 - Xworkers_1mill_0 - INFO -  15100;     4; 73940.5;   0.120; 0.11713; 0.09957; 0.92192; 0.56853; 0.68595; 0.62174; 0.57918\n",
      "2018-11-13 20:30:10,718 - Xworkers_1mill_0 - INFO -  15200;     4; 71324.4;   0.124; 0.11876; 0.09957; 0.91672; 0.54309; 0.66144; 0.59645; 0.53208\n",
      "2018-11-13 20:30:10,718 - Xworkers_1mill_0 - INFO -  15200;     4; 71324.4;   0.124; 0.11876; 0.09957; 0.91672; 0.54309; 0.66144; 0.59645; 0.53208\n",
      "2018-11-13 20:30:23,152 - Xworkers_1mill_0 - INFO -  15300;     4; 70354.0;   0.126; 0.11948; 0.09957; 0.91774; 0.55262; 0.64957; 0.59719; 0.55666\n",
      "2018-11-13 20:30:23,152 - Xworkers_1mill_0 - INFO -  15300;     4; 70354.0;   0.126; 0.11948; 0.09957; 0.91774; 0.55262; 0.64957; 0.59719; 0.55666\n",
      "2018-11-13 20:30:35,506 - Xworkers_1mill_0 - INFO -  15400;     4; 73051.4;   0.121; 0.12403; 0.09956; 0.91548; 0.51368; 0.63898; 0.56952; 0.50615\n",
      "2018-11-13 20:30:35,506 - Xworkers_1mill_0 - INFO -  15400;     4; 73051.4;   0.121; 0.12403; 0.09956; 0.91548; 0.51368; 0.63898; 0.56952; 0.50615\n",
      "2018-11-13 20:30:47,830 - Xworkers_1mill_0 - INFO -  15500;     4; 72985.1;   0.121; 0.11752; 0.09956; 0.91548; 0.59691; 0.64340; 0.61929; 0.59219\n",
      "2018-11-13 20:30:47,830 - Xworkers_1mill_0 - INFO -  15500;     4; 72985.1;   0.121; 0.11752; 0.09956; 0.91548; 0.59691; 0.64340; 0.61929; 0.59219\n",
      "2018-11-13 20:31:00,157 - Xworkers_1mill_0 - INFO -  15600;     4; 69029.5;   0.128; 0.11350; 0.09956; 0.91412; 0.61911; 0.66732; 0.64231; 0.61719\n",
      "2018-11-13 20:31:00,157 - Xworkers_1mill_0 - INFO -  15600;     4; 69029.5;   0.128; 0.11350; 0.09956; 0.91412; 0.61911; 0.66732; 0.64231; 0.61719\n",
      "2018-11-13 20:31:12,522 - Xworkers_1mill_0 - INFO -  15700;     4; 72868.0;   0.121; 0.11827; 0.09956; 0.91040; 0.56815; 0.59339; 0.58049; 0.55982\n",
      "2018-11-13 20:31:12,522 - Xworkers_1mill_0 - INFO -  15700;     4; 72868.0;   0.121; 0.11827; 0.09956; 0.91040; 0.56815; 0.59339; 0.58049; 0.55982\n",
      "2018-11-13 20:31:24,947 - Xworkers_1mill_0 - INFO -  15800;     4; 73005.2;   0.121; 0.11747; 0.09955; 0.91808; 0.55025; 0.67517; 0.60635; 0.55095\n",
      "2018-11-13 20:31:24,947 - Xworkers_1mill_0 - INFO -  15800;     4; 73005.2;   0.121; 0.11747; 0.09955; 0.91808; 0.55025; 0.67517; 0.60635; 0.55095\n",
      "2018-11-13 20:31:37,287 - Xworkers_1mill_0 - INFO -  15900;     4; 73689.9;   0.120; 0.12090; 0.09955; 0.90983; 0.54072; 0.64824; 0.58962; 0.53077\n",
      "2018-11-13 20:31:37,287 - Xworkers_1mill_0 - INFO -  15900;     4; 73689.9;   0.120; 0.12090; 0.09955; 0.90983; 0.54072; 0.64824; 0.58962; 0.53077\n",
      "2018-11-13 20:31:49,692 - Xworkers_1mill_0 - INFO -  16000;     4; 72914.2;   0.121; 0.11461; 0.09955; 0.91932; 0.62555; 0.65770; 0.64122; 0.62396\n",
      "2018-11-13 20:31:49,692 - Xworkers_1mill_0 - INFO -  16000;     4; 72914.2;   0.121; 0.11461; 0.09955; 0.91932; 0.62555; 0.65770; 0.64122; 0.62396\n",
      "2018-11-13 20:32:02,048 - Xworkers_1mill_0 - INFO -  16100;     4; 72196.0;   0.123; 0.11718; 0.09954; 0.92294; 0.57900; 0.68010; 0.62549; 0.59140\n",
      "2018-11-13 20:32:02,048 - Xworkers_1mill_0 - INFO -  16100;     4; 72196.0;   0.123; 0.11718; 0.09954; 0.92294; 0.57900; 0.68010; 0.62549; 0.59140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-13 20:32:14,458 - Xworkers_1mill_0 - INFO -  16200;     4; 72371.4;   0.122; 0.11098; 0.09954; 0.91966; 0.57152; 0.63037; 0.59950; 0.57506\n",
      "2018-11-13 20:32:14,458 - Xworkers_1mill_0 - INFO -  16200;     4; 72371.4;   0.122; 0.11098; 0.09954; 0.91966; 0.57152; 0.63037; 0.59950; 0.57506\n",
      "2018-11-13 20:32:26,812 - Xworkers_1mill_0 - INFO -  16300;     4; 74081.2;   0.119; 0.11901; 0.09954; 0.92023; 0.55548; 0.63989; 0.59470; 0.55695\n",
      "2018-11-13 20:32:26,812 - Xworkers_1mill_0 - INFO -  16300;     4; 74081.2;   0.119; 0.11901; 0.09954; 0.92023; 0.55548; 0.63989; 0.59470; 0.55695\n",
      "2018-11-13 20:32:39,178 - Xworkers_1mill_0 - INFO -  16400;     4; 71213.5;   0.124; 0.11381; 0.09954; 0.91876; 0.59443; 0.66998; 0.62994; 0.60460\n",
      "2018-11-13 20:32:39,178 - Xworkers_1mill_0 - INFO -  16400;     4; 71213.5;   0.124; 0.11381; 0.09954; 0.91876; 0.59443; 0.66998; 0.62994; 0.60460\n",
      "2018-11-13 20:32:51,570 - Xworkers_1mill_0 - INFO -  16500;     4; 73486.2;   0.120; 0.11355; 0.09953; 0.92418; 0.55635; 0.66542; 0.60601; 0.56258\n",
      "2018-11-13 20:32:51,570 - Xworkers_1mill_0 - INFO -  16500;     4; 73486.2;   0.120; 0.11355; 0.09953; 0.92418; 0.55635; 0.66542; 0.60601; 0.56258\n",
      "2018-11-13 20:33:03,957 - Xworkers_1mill_0 - INFO -  16600;     4; 70463.2;   0.126; 0.11371; 0.09953; 0.92384; 0.57263; 0.65855; 0.61259; 0.57971\n",
      "2018-11-13 20:33:03,957 - Xworkers_1mill_0 - INFO -  16600;     4; 70463.2;   0.126; 0.11371; 0.09953; 0.92384; 0.57263; 0.65855; 0.61259; 0.57971\n",
      "2018-11-13 20:33:16,378 - Xworkers_1mill_0 - INFO -  16700;     4; 72705.7;   0.122; 0.11047; 0.09953; 0.91977; 0.55865; 0.61870; 0.58714; 0.56010\n",
      "2018-11-13 20:33:16,378 - Xworkers_1mill_0 - INFO -  16700;     4; 72705.7;   0.122; 0.11047; 0.09953; 0.91977; 0.55865; 0.61870; 0.58714; 0.56010\n",
      "2018-11-13 20:33:28,771 - Xworkers_1mill_0 - INFO -  16800;     4; 71153.0;   0.124; 0.11528; 0.09953; 0.92282; 0.55968; 0.68526; 0.61613; 0.56513\n",
      "2018-11-13 20:33:28,771 - Xworkers_1mill_0 - INFO -  16800;     4; 71153.0;   0.124; 0.11528; 0.09953; 0.92282; 0.55968; 0.68526; 0.61613; 0.56513\n",
      "2018-11-13 20:33:41,086 - Xworkers_1mill_0 - INFO -  16900;     4; 71138.8;   0.124; 0.12046; 0.09952; 0.90531; 0.60218; 0.64106; 0.62102; 0.59450\n",
      "2018-11-13 20:33:41,086 - Xworkers_1mill_0 - INFO -  16900;     4; 71138.8;   0.124; 0.12046; 0.09952; 0.90531; 0.60218; 0.64106; 0.62102; 0.59450\n",
      "2018-11-13 20:33:47,746 - Xworkers_1mill_0 - INFO - METRICS each 1800 (secs):  Loss in ---Training in Summary---: 0.10982\n",
      " Avg Log_Loss in ---Training in Summary---: 0.08968\n",
      "---Training in Summary---: (Silly) Global-ACC=0.92226, Recall=0.56798, Avg M-Measure=0.9268, Avg AUC_AOC=0.9357 Avg AUC_PR=0.5380\n",
      " Precision= 0.67153  f1score_micro= 0.61543  f1score_macro= 0.56757\n",
      "Total Confusion Matrix;Total M-Measure Matrix;Total AUC_AOC;Total AUC_PR\n",
      "30;4;0;0;94;7;0;;0.0000;0.9046;0.9605;0.9762;0.7944;0.9586;0.9268;;0.8062 ;0.2617\n",
      "0;142;14;6;70;0;0;;0.9151;0.0000;0.5528;0.9550;0.8927;0.9851;0.9810;;0.8927 ;0.2588\n",
      "0;47;26;10;1;0;0;;0.9853;0.7965;0.0000;0.8904;0.9937;0.9821;0.9909;;0.9843 ;0.3014\n",
      "0;3;12;154;2;16;0;;0.9895;0.9857;0.9251;0.0000;0.9988;0.8987;0.9438;;0.9954 ;0.7781\n",
      "32;311;11;13;7684;3;0;;0.7853;0.8938;0.9934;0.9984;0.0000;0.9985;0.9970;;0.9303 ;0.9880\n",
      "0;0;2;19;4;124;0;;0.9849;0.9941;0.9843;0.9064;0.9984;0.0000;0.7093;;0.9950 ;0.7894\n",
      "0;0;0;1;1;5;2;;0.9568;0.9611;0.9534;0.9181;0.9661;0.7422;0.0000;;0.9457 ;0.3886\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "2018-11-13 20:33:47,746 - Xworkers_1mill_0 - INFO - METRICS each 1800 (secs):  Loss in ---Training in Summary---: 0.10982\n",
      " Avg Log_Loss in ---Training in Summary---: 0.08968\n",
      "---Training in Summary---: (Silly) Global-ACC=0.92226, Recall=0.56798, Avg M-Measure=0.9268, Avg AUC_AOC=0.9357 Avg AUC_PR=0.5380\n",
      " Precision= 0.67153  f1score_micro= 0.61543  f1score_macro= 0.56757\n",
      "Total Confusion Matrix;Total M-Measure Matrix;Total AUC_AOC;Total AUC_PR\n",
      "30;4;0;0;94;7;0;;0.0000;0.9046;0.9605;0.9762;0.7944;0.9586;0.9268;;0.8062 ;0.2617\n",
      "0;142;14;6;70;0;0;;0.9151;0.0000;0.5528;0.9550;0.8927;0.9851;0.9810;;0.8927 ;0.2588\n",
      "0;47;26;10;1;0;0;;0.9853;0.7965;0.0000;0.8904;0.9937;0.9821;0.9909;;0.9843 ;0.3014\n",
      "0;3;12;154;2;16;0;;0.9895;0.9857;0.9251;0.0000;0.9988;0.8987;0.9438;;0.9954 ;0.7781\n",
      "32;311;11;13;7684;3;0;;0.7853;0.8938;0.9934;0.9984;0.0000;0.9985;0.9970;;0.9303 ;0.9880\n",
      "0;0;2;19;4;124;0;;0.9849;0.9941;0.9843;0.9064;0.9984;0.0000;0.7093;;0.9950 ;0.7894\n",
      "0;0;0;1;1;5;2;;0.9568;0.9611;0.9534;0.9181;0.9661;0.7422;0.0000;;0.9457 ;0.3886\n",
      "---------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint written to /home/ubuntu/summ_15ep_2wrk/checkpoint-16949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-13 20:33:55,078 - Xworkers_1mill_0 - INFO -  17000;     4; 71339.4;   0.124; 0.12071; 0.09952; 0.91605; 0.54283; 0.64624; 0.59004; 0.54185\n",
      "2018-11-13 20:33:55,078 - Xworkers_1mill_0 - INFO -  17000;     4; 71339.4;   0.124; 0.12071; 0.09952; 0.91605; 0.54283; 0.64624; 0.59004; 0.54185\n",
      "2018-11-13 20:34:07,438 - Xworkers_1mill_0 - INFO -  17100;     4; 72421.7;   0.122; 0.11695; 0.09952; 0.91661; 0.54899; 0.62520; 0.58462; 0.54176\n",
      "2018-11-13 20:34:07,438 - Xworkers_1mill_0 - INFO -  17100;     4; 72421.7;   0.122; 0.11695; 0.09952; 0.91661; 0.54899; 0.62520; 0.58462; 0.54176\n",
      "2018-11-13 20:34:19,764 - Xworkers_1mill_0 - INFO -  17200;     4; 71914.9;   0.123; 0.11113; 0.09951; 0.91220; 0.61508; 0.60631; 0.61066; 0.59048\n",
      "2018-11-13 20:34:19,764 - Xworkers_1mill_0 - INFO -  17200;     4; 71914.9;   0.123; 0.11113; 0.09951; 0.91220; 0.61508; 0.60631; 0.61066; 0.59048\n",
      "2018-11-13 20:34:32,091 - Xworkers_1mill_0 - INFO -  17300;     4; 72753.7;   0.122; 0.11876; 0.09951; 0.92023; 0.57317; 0.63627; 0.60307; 0.57420\n",
      "2018-11-13 20:34:32,091 - Xworkers_1mill_0 - INFO -  17300;     4; 72753.7;   0.122; 0.11876; 0.09951; 0.92023; 0.57317; 0.63627; 0.60307; 0.57420\n",
      "2018-11-13 20:34:44,506 - Xworkers_1mill_0 - INFO -  17400;     4; 70500.9;   0.126; 0.11362; 0.09951; 0.91977; 0.57653; 0.67483; 0.62182; 0.58498\n",
      "2018-11-13 20:34:44,506 - Xworkers_1mill_0 - INFO -  17400;     4; 70500.9;   0.126; 0.11362; 0.09951; 0.91977; 0.57653; 0.67483; 0.62182; 0.58498\n",
      "2018-11-13 20:36:28,896 - Xworkers_1mill_0 - INFO - valid - Number of batches: 14; batch_size: 150000; Total Time: 0:01:44.385045\n",
      "2018-11-13 20:36:28,896 - Xworkers_1mill_0 - INFO - valid - Number of batches: 14; batch_size: 150000; Total Time: 0:01:44.385045\n",
      "2018-11-13 20:36:28,898 - Xworkers_1mill_0 - INFO - ---Validation--- Training Step: 17400; Training Epoch: 4; \n",
      " Confusion Matrix:\n",
      " [[  13156     522      45    1256   31971    1033       0]\n",
      " [      1   40033    2652    1166   14082      84       0]\n",
      " [      0   11264    7817    3340       0      56       0]\n",
      " [      0       0    3880   60946       6    5490       0]\n",
      " [    142   76656    1396    3232 1726443     755       0]\n",
      " [      0       9     293    6700      10   66030       0]\n",
      " [      1       0       0     196       1    1464     783]]\n",
      "2018-11-13 20:36:28,898 - Xworkers_1mill_0 - INFO - ---Validation--- Training Step: 17400; Training Epoch: 4; \n",
      " Confusion Matrix:\n",
      " [[  13156     522      45    1256   31971    1033       0]\n",
      " [      1   40033    2652    1166   14082      84       0]\n",
      " [      0   11264    7817    3340       0      56       0]\n",
      " [      0       0    3880   60946       6    5490       0]\n",
      " [    142   76656    1396    3232 1726443     755       0]\n",
      " [      0       9     293    6700      10   66030       0]\n",
      " [      1       0       0     196       1    1464     783]]\n",
      "2018-11-13 20:36:28,903 - Xworkers_1mill_0 - INFO - (Training Step, Training Epoch, loss, accuracy, recall, precision, f1score_micro, f1score_macro) in Validation:  17400;     4; 0.09278; 0.91951; 0.62157; 0.77643; 0.69040; 0.63209\n",
      "2018-11-13 20:36:28,903 - Xworkers_1mill_0 - INFO - (Training Step, Training Epoch, loss, accuracy, recall, precision, f1score_micro, f1score_macro) in Validation:  17400;     4; 0.09278; 0.91951; 0.62157; 0.77643; 0.69040; 0.63209\n",
      "2018-11-13 20:36:29,736 - Xworkers_1mill_0 - INFO - METRICS each 1800 (secs):  Loss in ---Training in Summary---: 0.11038\n",
      " Avg Log_Loss in ---Training in Summary---: 0.08826\n",
      "---Training in Summary---: (Silly) Global-ACC=0.91831, Recall=0.56919, Avg M-Measure=0.9343, Avg AUC_AOC=0.9371 Avg AUC_PR=0.5494\n",
      " Precision= 0.66643  f1score_micro= 0.61398  f1score_macro= 0.57521\n",
      "Total Confusion Matrix;Total M-Measure Matrix;Total AUC_AOC;Total AUC_PR\n",
      "32;7;0;1;100;1;0;;0.0000;0.9203;0.9612;0.9790;0.8016;0.9601;0.9718;;0.8125 ;0.2440\n",
      "1;145;9;7;86;0;0;;0.9177;0.0000;0.5665;0.9580;0.9072;0.9877;0.9956;;0.9069 ;0.2900\n",
      "0;47;17;23;1;0;0;;0.9842;0.7962;0.0000;0.8898;0.9938;0.9891;0.9975;;0.9856 ;0.2230\n",
      "1;2;8;161;1;7;0;;0.9910;0.9875;0.9265;0.0000;0.9987;0.9106;0.9558;;0.9960 ;0.7979\n",
      "35;314;9;20;7655;2;0;;0.7962;0.8993;0.9936;0.9987;0.0000;0.9986;0.9991;;0.9322 ;0.9887\n",
      "0;4;0;27;3;113;0;;0.9875;0.9935;0.9842;0.9191;0.9964;0.0000;0.7092;;0.9941 ;0.7995\n",
      "0;0;0;2;0;5;4;;0.9739;0.9738;0.9649;0.9231;0.9780;0.8055;0.0000;;0.9321 ;0.5026\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "2018-11-13 20:36:29,736 - Xworkers_1mill_0 - INFO - METRICS each 1800 (secs):  Loss in ---Training in Summary---: 0.11038\n",
      " Avg Log_Loss in ---Training in Summary---: 0.08826\n",
      "---Training in Summary---: (Silly) Global-ACC=0.91831, Recall=0.56919, Avg M-Measure=0.9343, Avg AUC_AOC=0.9371 Avg AUC_PR=0.5494\n",
      " Precision= 0.66643  f1score_micro= 0.61398  f1score_macro= 0.57521\n",
      "Total Confusion Matrix;Total M-Measure Matrix;Total AUC_AOC;Total AUC_PR\n",
      "32;7;0;1;100;1;0;;0.0000;0.9203;0.9612;0.9790;0.8016;0.9601;0.9718;;0.8125 ;0.2440\n",
      "1;145;9;7;86;0;0;;0.9177;0.0000;0.5665;0.9580;0.9072;0.9877;0.9956;;0.9069 ;0.2900\n",
      "0;47;17;23;1;0;0;;0.9842;0.7962;0.0000;0.8898;0.9938;0.9891;0.9975;;0.9856 ;0.2230\n",
      "1;2;8;161;1;7;0;;0.9910;0.9875;0.9265;0.0000;0.9987;0.9106;0.9558;;0.9960 ;0.7979\n",
      "35;314;9;20;7655;2;0;;0.7962;0.8993;0.9936;0.9987;0.0000;0.9986;0.9991;;0.9322 ;0.9887\n",
      "0;4;0;27;3;113;0;;0.9875;0.9935;0.9842;0.9191;0.9964;0.0000;0.7092;;0.9941 ;0.7995\n",
      "0;0;0;2;0;5;4;;0.9739;0.9738;0.9649;0.9231;0.9780;0.8055;0.0000;;0.9321 ;0.5026\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "2018-11-13 20:38:13,447 - Xworkers_1mill_0 - INFO - valid - Number of batches: 14; batch_size: 150000; Total Time: 0:01:43.704545\n",
      "2018-11-13 20:38:13,447 - Xworkers_1mill_0 - INFO - valid - Number of batches: 14; batch_size: 150000; Total Time: 0:01:43.704545\n",
      "2018-11-13 20:38:13,449 - Xworkers_1mill_0 - INFO - ---Validation--- Training Step: 17402; Training Epoch: 4; \n",
      " Confusion Matrix:\n",
      " [[  13151     505      47    1254   31993    1033       0]\n",
      " [      0   39789    2612    1136   14397      84       0]\n",
      " [      0   11349    7811    3262       0      55       0]\n",
      " [      0       1    3927   60898       6    5490       0]\n",
      " [    111   72962    1345    3222 1730230     754       0]\n",
      " [      0       9     295    6698      10   66030       0]\n",
      " [      1       0       0     196       1    1464     783]]\n",
      "2018-11-13 20:38:13,449 - Xworkers_1mill_0 - INFO - ---Validation--- Training Step: 17402; Training Epoch: 4; \n",
      " Confusion Matrix:\n",
      " [[  13151     505      47    1254   31993    1033       0]\n",
      " [      0   39789    2612    1136   14397      84       0]\n",
      " [      0   11349    7811    3262       0      55       0]\n",
      " [      0       1    3927   60898       6    5490       0]\n",
      " [    111   72962    1345    3222 1730230     754       0]\n",
      " [      0       9     295    6698      10   66030       0]\n",
      " [      1       0       0     196       1    1464     783]]\n",
      "2018-11-13 20:38:13,455 - Xworkers_1mill_0 - INFO - (Training Step, Training Epoch, loss, accuracy, recall, precision, f1score_micro, f1score_macro) in Validation:  17402;     4; 0.09150; 0.92119; 0.62112; 0.77815; 0.69081; 0.63323\n",
      "2018-11-13 20:38:13,455 - Xworkers_1mill_0 - INFO - (Training Step, Training Epoch, loss, accuracy, recall, precision, f1score_micro, f1score_macro) in Validation:  17402;     4; 0.09150; 0.92119; 0.62112; 0.77815; 0.69081; 0.63323\n",
      "2018-11-13 20:38:14,485 - Xworkers_1mill_0 - INFO - Time used in total: 3888.4 seconds\n",
      "2018-11-13 20:38:14,485 - Xworkers_1mill_0 - INFO - Time used in total: 3888.4 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint written to /home/ubuntu/summ_15ep_2wrk/checkpoint-17402\n"
     ]
    }
   ],
   "source": [
    "ops_to_run = [learning_rate_op, train_ops]\n",
    "ops_stats = [conf_mtx_op, accuracy_op, better_acc_op, precision_op, auc_list_op, auc_mean_op, m_list_op, m_list_mean_op, \n",
    "             lloss_op, auc_pr_op, auc_pr_mean_op, total_loss_op]\n",
    "                    \n",
    "oom = False\n",
    "step0 = int(sess.run(trainer.global_step))\n",
    "for step in range(step0, nstep):    \n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        epoch = step*FLAGS.batch_size // nrecord #*hvd.size()\n",
    "        batch_dict= create_feed_dict('batch', DATA, FLAGS)        \n",
    "        \n",
    "        if (hvd.rank() == 0 and summary_ops is not None and\n",
    "            (step == 0 or step+1 == nstep or\n",
    "             time.time() - last_summary_time > FLAGS.summary_interval)):\n",
    "            \n",
    "            if step != 0:\n",
    "                last_summary_time += FLAGS.summary_interval                        \n",
    "                \n",
    "            reset_and_update(sess, local_init, batch_dict)\n",
    "            summary, conf_mtx, accuracy, better_acc, precision, auc_list, auc_mean, m_list, m_list_mean, lloss, auc_pr, auc_pr_mean, loss, lr, _ = sess.run([summary_ops] + ops_stats + ops_to_run, feed_dict=batch_dict)                        \n",
    "            train_writer.add_summary(summary, step)            \n",
    "            train_writer.flush()\n",
    "            \n",
    "            if (math.isnan(better_acc)):\n",
    "                better_acc = calculate_better_acc(conf_mtx)        \n",
    "            if (math.isnan(precision)):\n",
    "                precision = calculate_better_acc(conf_mtx, axis=0)                         \n",
    "            f1score_micro = 2 * precision * better_acc / (precision + better_acc)\n",
    "            f1score_macro = calculate_macro_f1score(np.array(conf_mtx, dtype='float32'))\n",
    "            \n",
    "            elapsed = time.time() - start_time            \n",
    "            #this not necessarily matches with the display at console not even with validation set, due the summary_interval!\n",
    "            print_stats('---Training in Summary---', conf_mtx, accuracy, better_acc, precision, f1score_micro, f1score_macro, auc_list, auc_mean, m_list, m_list_mean, lloss, auc_pr, auc_pr_mean, loss)                         \n",
    "            df_train.loc[len(df_train)] = [step+1, epoch+1, elapsed, loss, lloss, accuracy, better_acc, precision, f1score_micro, f1score_macro, m_list_mean, auc_mean, auc_pr_mean]                                                    \n",
    "        else:\n",
    "            accuracy, conf_mtx, better_acc, precision, loss, lr, _ = sess.run([accuracy_op, conf_mtx_op, better_acc_op, precision_op, total_loss_op] + ops_to_run, feed_dict=batch_dict)\n",
    "            \n",
    "            if (math.isnan(better_acc)):\n",
    "                better_acc = calculate_better_acc(conf_mtx)        \n",
    "            if (math.isnan(precision)):\n",
    "                precision = calculate_better_acc(conf_mtx, axis=0)            \n",
    "            f1score_micro = 2 * precision * better_acc / (precision + better_acc)\n",
    "            f1score_macro = calculate_macro_f1score(np.array(conf_mtx, dtype='float32'))\n",
    "            \n",
    "            elapsed = time.time() - start_time\n",
    "            \n",
    "        if step == 0 or (step+1) % FLAGS.display_every == 0:                    \n",
    "            feature_per_sec = FLAGS.batch_size / elapsed                        \n",
    "            logger.info(\"%6i; %5i; %7.1f; %7.3f; %7.5f; %7.5f; %7.5f; %7.5f; %7.5f; %7.5f; %7.5f\" % (\n",
    "                step+1, epoch+1, feature_per_sec*hvd.size(), elapsed, loss, lr, accuracy, better_acc, precision, f1score_micro, f1score_macro))        \n",
    "\n",
    "        if (hvd.rank() == 0 and  ((step+1) % nstep_per_epoch == 0 or step+1 == nstep)):\n",
    "            #Running validation set:\n",
    "            valid_conf_mtx, valid_time, metrics = batching_dataset(sess, epoch, valid_writer, 'valid', DATA, FLAGS)\n",
    "            #valid_conf_mtx = np.array2string(valid_conf_mtx, formatter={'int_type':lambda x: \"int(%)\" % x})\n",
    "            valid_conf_mtx = np.array(valid_conf_mtx, dtype=int)\n",
    "            logger.info(\"---Validation--- Training Step: %d; Training Epoch: %d; \\n Confusion Matrix:\\n %s\" % (step+1, epoch+1, str(valid_conf_mtx)))            \n",
    "            df_valid.loc[len(df_valid)] = [step+1, epoch+1, valid_time, metrics[6], metrics[5], metrics[0], metrics[1], metrics[2], metrics[3], metrics[4]]            \n",
    "            logger.info(\"(Training Step, Training Epoch, loss, accuracy, recall, precision, f1score_micro, f1score_macro) in Validation: %6i; %5i; %7.5f; %7.5f; %7.5f; %7.5f; %7.5f; %7.5f\" % (\n",
    "                step+1, epoch+1, metrics[5], metrics[0], metrics[1], metrics[2], metrics[3], metrics[4]))    \n",
    "            sess.run(local_init)\n",
    "        \n",
    "        del batch_dict\n",
    "                    \n",
    "    except KeyboardInterrupt:\n",
    "        if hvd.rank() == 0:\n",
    "            df_train.to_csv(train_file, index=False, mode='a', sep =';', header=False)\n",
    "            df_valid.to_csv(valid_file, index=False, mode='a', sep =';', header=False)\n",
    "        logger.info(\"Keyboard interrupt\")\n",
    "        break\n",
    "    except tf.errors.ResourceExhaustedError:\n",
    "        elapsed = -1.\n",
    "        loss    = 0.\n",
    "        lr      = -1\n",
    "        if hvd.rank() == 0:\n",
    "            df_train.to_csv(train_file, index=False, mode='a', sep =';', header=False)\n",
    "            df_valid.to_csv(valid_file, index=False, mode='a', sep =';', header=False)\n",
    "        oom = True\n",
    "    \n",
    "    if (hvd.rank() == 0 and saver is not None and\n",
    "        (time.time() - last_save_time > FLAGS.save_interval or step+1 == nstep)):\n",
    "        last_save_time += FLAGS.save_interval\n",
    "        save_path = saver.save(sess, checkpoint_file, global_step=trainer.global_step)\n",
    "        print(\"Checkpoint written to\", save_path)\n",
    "    \n",
    "    if oom:\n",
    "        break\n",
    "        \n",
    "\n",
    "if hvd.rank() == 0:                               \n",
    "    df_train.to_csv(train_file, index=False, mode='a', sep =';', header=False)\n",
    "    df_valid.to_csv(valid_file, index=False, mode='a', sep =';', header=False)\n",
    "                               \n",
    "if train_writer is not None:\n",
    "    train_writer.close()\n",
    "\n",
    "if valid_writer is not None:\n",
    "    valid_writer.close()    \n",
    "    \n",
    "global_end_time = time.time()\n",
    "#logger.info(\"start time is {}, end time is {}\".format(global_start_time, global_end_time))\n",
    "logger.info('Time used in total: %.1f seconds' % (global_end_time - global_start_time))\n",
    "\n",
    "if oom:\n",
    "    print(\"Out of memory error detected, exiting\")\n",
    "    sys.exit(-2)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter nbconvert --to=python  nn_real_hvd-ntb-v5-batch_size.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!scp nn_real_hvd-ntb-v5-batch_size.py ubuntu@ec2-18-212-174-184.compute-1.amazonaws.com:/home/ubuntu/MLMortgage/notebooks/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mpirun -np 9 -H ec2-54-173-135-25.compute-1.amazonaws.com,ec2-35-174-116-85.compute-1.amazonaws.com,ec2-54-89-251-88.compute-1.amazonaws.com,ec2-54-166-213-204.compute-1.amazonaws.com,ec2-18-204-17-119.compute-1.amazonaws.com,ec2-34-226-141-33.compute-1.amazonaws.com,ec2-34-238-240-116.compute-1.amazonaws.com,ec2-18-212-174-184.compute-1.amazonaws.com,ec2-18-234-241-74.compute-1.amazonaws.com  --mca plm_rsh_no_tree_spawn 1 --prefix /usr/local/mpi --bind-to none --map-by slot -x NCCL_DEBUG=INFO -x NCCL_MIN_NRINGS=2 -x LD_LIBRARY_PATH -x PATH  -mca pml ob1 -mca btl ^openib python nn_real_hvd-ntb-v5-batch_size.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow_p36]",
   "language": "python",
   "name": "conda-env-tensorflow_p36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
