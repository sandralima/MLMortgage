{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "import os\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "import datetime\n",
    "import glob\n",
    "from os.path import abspath\n",
    "from pathlib import Path\n",
    "from inspect import getsourcefile\n",
    "from datetime import datetime\n",
    "import math\n",
    "import argparse\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "nb_dir = os.path.join(Path(os.getcwd()).parents[0], 'src', 'data')\n",
    "if nb_dir not in sys.path:\n",
    "    sys.path.insert(0, nb_dir)\n",
    "import get_raw_data as grd\n",
    "import data_classes\n",
    "import Normalizer\n",
    "\n",
    "DT_FLOAT = np.float32 \n",
    "DT_BOOL = np.uint8\n",
    "RANDOM_SEED = 123\n",
    "logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "# logger.propagate = False # it will not log to console.\n",
    "\n",
    "RAW_DIR = os.path.join(Path(os.getcwd()).parents[0], 'data', 'raw') \n",
    "PRO_DIR = os.path.join(Path(os.getcwd()).parents[0], 'data', 'processed')\n",
    "\n",
    "print(RAW_DIR, PRO_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parser(parser):\n",
    "    \"\"\"Parse the arguments from the CLI and update the parser.\"\"\"    \n",
    "    parser.add_argument(\n",
    "        '--prepro_step',\n",
    "        type=str,\n",
    "        default='preprocessing', #'slicing', 'preprocessing'\n",
    "        help='To execute a preprocessing method')    \n",
    "    #this is for allfeatures_preprocessing:\n",
    "    parser.add_argument(\n",
    "        '--train_period',\n",
    "        type=int,\n",
    "        nargs='*',\n",
    "        default=[121,323], #[121,279], #[156, 180], [121,143],  # 279],\n",
    "        help='Training Period')\n",
    "    parser.add_argument(\n",
    "        '--valid_period',\n",
    "        type=int,\n",
    "        nargs='*',\n",
    "        default=[324,329], #[280,285], #[181,185], [144,147],\n",
    "        help='Validation Period')    \n",
    "    parser.add_argument(\n",
    "        '--test_period',\n",
    "        type=int,\n",
    "        nargs='*',\n",
    "        default=[330, 342], #[286, 304], # [186,191], [148, 155],\n",
    "        help='Testing Period')    \n",
    "    parser.add_argument(\n",
    "        '--prepro_dir',\n",
    "        type=str,\n",
    "        default='chuncks_random_c1mill',\n",
    "        help='Directory with raw data inside data/raw/ and it will be the output directory inside data/processed/')    \n",
    "    parser.add_argument(\n",
    "        '--prepro_chunksize',\n",
    "        type=int,\n",
    "        default=500000,\n",
    "        help='Chunk size to put into the h5 file...')    \n",
    "    parser.add_argument(\n",
    "        '--prepro_with_index',\n",
    "        type=bool,\n",
    "        default=True,\n",
    "        help='To keep indexes for each record')\n",
    "    parser.add_argument(\n",
    "        '--ref_norm',\n",
    "        type=bool,\n",
    "        default=True,\n",
    "        help='To execute the normalization over the raw inputs')\n",
    "        \n",
    "    return parser.parse_known_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "FLAGS, UNPARSED = update_parser(argparse.ArgumentParser())    \n",
    "#these are the more important parameters for preprocessing:\n",
    "FLAGS.prepro_dir='chuncks_random_c1mill' #this directory must be the same inside 'raw' and processed directories.\n",
    "FLAGS.prepro_chunksize=500000 \n",
    "FLAGS.train_period=[121,323] #[121,279] #[121, 143] \n",
    "FLAGS.valid_period=[324,329] #[280,285] #[144, 147] \n",
    "FLAGS.test_period=[330,342] #[286,304] #[148, 155]                                                \n",
    "FLAGS.prepro_with_index = False\n",
    "\n",
    "print(FLAGS)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glob.glob(os.path.join(RAW_DIR, FLAGS.prepro_dir,\"*.txt\"))\n",
    "# from IPython.core.debugger import Tracer; Tracer()()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def allfeatures_extract_labels(data, columns='MBA_DELINQUENCY_STATUS_next'):\n",
    "    '''Extract the labels from Dataset, order-and-transform them into one-hot matrix of labels.\n",
    "        Args: \n",
    "            data (DataFrame): Input Dataset which is modified in place.\n",
    "            columns (string): Name of the class column.\n",
    "        Returns: \n",
    "            one-hot matrix of labels of shape: [data.shape[0], 7]. \n",
    "        Raises:        \n",
    "    '''    \n",
    "    logger.name = 'allfeatures_extract_labels'\n",
    "    if (type(columns)==str):\n",
    "         indices = [i for i, elem in enumerate(data.columns) if columns in elem] # (alphabetically ordered)\n",
    "    else:\n",
    "        indices =  columns \n",
    "\n",
    "    if indices:\n",
    "        labels = data[data.columns[indices]]\n",
    "        data.drop(data.columns[indices], axis=1, inplace=True)    \n",
    "        logger.info('...Labels extracted from Dataset...')\n",
    "        return labels\n",
    "    else: return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_chunk(tag, label, chunk, chunk_periods, tag_period, log_file, with_index, tag_index, hdf=None, tfrec=None):\n",
    "    '''Extract records filtering by chunk_periods parameter, define indexes in case of with_index=True, \n",
    "        extract labels and save the results into the target file.\n",
    "        Args: \n",
    "            chunk (DataFrame): Input Dataset which is modified in place.\n",
    "            tag (string): 'train', 'valid' or 'test'\n",
    "            chunk_periods (integer array): an array containing all periods into the chunk.\n",
    "            tag_period (integer array): an array of form [init_period, end_period] for the correspond tag.\n",
    "            log_file (Logger): An object of the log file.\n",
    "            with_index (boolean): If true it will be saved the indexes.\n",
    "            tag_index (int): an index that accumulates the size of the processed chunk. \n",
    "            hdf or tfrec (HDFStore or TFRecords): an object of the target file. Only one must be distint of None.\n",
    "        Returns: \n",
    "            tag_index (int): tag_index updated.\n",
    "        Raises:        \n",
    "    '''    \n",
    "    \n",
    "    inter_periods = list(chunk_periods.intersection(set(range(tag_period[0], tag_period[1]+1))))\n",
    "    log_file.write('Periods corresponding to ' + tag +' period: %s\\r\\n' % str(inter_periods))\n",
    "    p_chunk = chunk.loc[(slice(None), slice(None), slice(None), inter_periods), :]\n",
    "    log_file.write('Records for ' + tag +  ' Set - Number of rows: %d\\r\\n' % (p_chunk.shape[0]))\n",
    "    print('Records for ' + tag + ' Set - Number of rows:', p_chunk.shape[0])\n",
    "    if (p_chunk.shape[0] > 0):\n",
    "        if (with_index==True):\n",
    "            p_chunk.index = pd.MultiIndex.from_tuples([(i, x[1], x[2],x[3]) for x,i in zip(p_chunk.index, range(tag_index, tag_index + p_chunk.shape[0]))])                                \n",
    "        else:\n",
    "            p_chunk.reset_index(drop=True, inplace=True)\n",
    "            \n",
    "        labels = allfeatures_extract_labels(p_chunk, columns=label)\n",
    "        p_chunk = p_chunk.astype(DT_FLOAT)\n",
    "        labels = labels.astype(np.int8)\n",
    "        if (p_chunk.shape[0] != labels.shape[0]) : \n",
    "            print('Error in shapes:', p_chunk.shape, labels.shape)\n",
    "        else :\n",
    "            if (hdf!=None):\n",
    "                hdf.put(tag + '/features', p_chunk, append=True) #data_columns=p_chunk.columns.values), index=False\n",
    "                hdf.put(tag + '/labels', labels, append=True) #data_columns=labels.columns.values)                         \n",
    "                hdf.flush()                      \n",
    "            elif (tfrec!=None):\n",
    "                for row, lab in zip(p_chunk.values, labels.values):\n",
    "                    feature = {tag + '/labels': _int64_feature(lab),\n",
    "                               tag + '/features': _float_feature(row)}\n",
    "                    # Create an example protocol buffer\n",
    "                    example = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "                    tfrec.write(example.SerializeToString())                            \n",
    "                tfrec.flush()\n",
    "            tag_index += p_chunk.shape[0]\n",
    "\n",
    "    return tag_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def allfeatures_drop_cols(data, columns):\n",
    "    '''Exclude from the dataset 'data' the descriptive columns as parameters.\n",
    "        Args: \n",
    "            data (DataFrame): Input Dataset which is modified in place.\n",
    "        Returns: \n",
    "            None\n",
    "        Raises:        \n",
    "    '''\n",
    "    logger.name = 'allfeatures_drop_cols'    \n",
    "    data.drop(columns, axis=1, inplace=True)\n",
    "    logger.info('...Columns Excluded from dataset...')\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oneHotDummies_column(column, categories):\n",
    "    '''Convert categorical variable into dummy/indicator variables.\n",
    "    \n",
    "    Args: \n",
    "        column (Series): Input String Categorical Column.\n",
    "    Returns: \n",
    "        DataFrame. Integer Sparse binary matrix of categorical features.\n",
    "    Raises:        \n",
    "    '''    \n",
    "    logger.name = 'oneHotDummies_column: ' +  column.name\n",
    "    cat_column = pd.Categorical(column.astype('str'), categories=categories)\n",
    "    cat_column = pd.get_dummies(cat_column)   # in the same order as categories! (alphabetically ordered) \n",
    "    cat_column = cat_column.add_prefix(column.name + '_')\n",
    "    if (cat_column.isnull().any().any()):\n",
    "        null_cols = cat_column.columns[cat_column.isnull().any()]\n",
    "        print(cat_column[null_cols].isnull().sum())\n",
    "        print(cat_column[cat_column.isnull().any(axis=1)][null_cols].head(50))\n",
    "    return cat_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imputing_nan_values(nan_dict, distribution):        \n",
    "    '''Replace nan values with a value according the nan_dict dictionary and distribution of this feature.\n",
    "        Args: \n",
    "            nan_dict (Dictionary): the key values are the name of features, the values could be a literal or \n",
    "            values belonging to the distribution.\n",
    "            distribution (DataFrame): Contains the median value for numerical features.\n",
    "        Returns: \n",
    "            new_dict (Dictionary): contains the values updated.\n",
    "        Raises:        \n",
    "    '''    \n",
    "    new_dict = {}\n",
    "    for k,v in nan_dict.items():\n",
    "        if v=='median':\n",
    "            new_dict[k] = float(distribution[k+'_MEDIAN'])    \n",
    "        else:\n",
    "            new_dict[k] = v\n",
    "            \n",
    "    return new_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_invalid_delinquency_status(data, gflag, log_file):   \n",
    "    '''Delete all subsecuent records of a loan when the feature delinquency_status_next \n",
    "       contains any of the following invalid status: S,T,X or Z.\n",
    "        Args: \n",
    "            data (DataFrame): Input Dataset which is modified in place.\n",
    "            gflag (int): Loan_id of the last loan in previous data, in case this contains some invalid status, \n",
    "            to delete all records inside the current data.\n",
    "            log_file (Logger): An object of the log file.\n",
    "        Returns: \n",
    "            gflag (int): Loan_id of the last loan in current data, in case this contains some invalid status.\n",
    "        Raises:        \n",
    "    '''        \n",
    "    logger.name = 'drop_invalid_delinquency_status'\n",
    "    delinq_ids =  data[data['MBA_DELINQUENCY_STATUS'].isin(['0', 'R', 'S', 'T', 'X', 'Z'])]['LOAN_ID']\n",
    "    groups = data[data['LOAN_ID'].isin(delinq_ids)][['LOAN_ID', 'PERIOD', 'MBA_DELINQUENCY_STATUS', 'DELINQUENCY_STATUS_NEXT']].groupby('LOAN_ID') \n",
    "    groups_list = list(groups)\n",
    "    \n",
    "    iuw= pd.Index([])\n",
    "    \n",
    "    if gflag != '': \n",
    "        try:\n",
    "            iuw= iuw.union(groups.get_group(gflag).index[0:])\n",
    "        except  Exception  as e:\n",
    "            print(str(e))\n",
    "                \n",
    "    if data.iloc[-1]['LOAN_ID'] in groups.groups.keys():\n",
    "        gflag = data.iloc[-1]['LOAN_ID']\n",
    "    else:\n",
    "        gflag = ''\n",
    "                \n",
    "    for k, group in groups_list: \n",
    "        li= group.index[(group['MBA_DELINQUENCY_STATUS'] =='S') | (group['MBA_DELINQUENCY_STATUS'] =='T') \n",
    "                         | (group['MBA_DELINQUENCY_STATUS'] =='X') | (group['MBA_DELINQUENCY_STATUS'] =='Z')].tolist()\n",
    "        if li: iuw= iuw.union(group.index[group.index.get_loc(li[0]):])\n",
    "        # In case of REO or Paid-Off, we need to exclude since the next record:\n",
    "        df_delinq_01 = group[(group['MBA_DELINQUENCY_STATUS'] =='0') | (group['MBA_DELINQUENCY_STATUS'] =='R')]\n",
    "        if df_delinq_01.shape[0]>0: \n",
    "            track_i = df_delinq_01.index[0]\n",
    "            iuw= iuw.union(group.index[group.index.get_loc(track_i)+1:])\n",
    "        \n",
    "    if iuw!=[]:\n",
    "        log_file.write('drop_invalid_delinquency_status - Total rows: %d\\r\\n' % len(iuw)) # (log_df.shape[0])\n",
    "        data.drop(iuw, inplace=True) \n",
    "        logger.info('invalid_delinquency_status dropped')             \n",
    "    \n",
    "    return gflag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zscore(x,mean,stdd):\n",
    "    return (x - mean) / stdd\n",
    "\n",
    "def zscore_apply(dist_file, data):            \n",
    "    stddv_0 = []\n",
    "    nnorm_cols = []\n",
    "    for col_name in data.columns.values:                                \n",
    "        mean = pd.Series(dist_file.iloc[0, np.where(pd.DataFrame(dist_file.columns.values)[0].str.contains(col_name+'_MEAN'))[0]], dtype='float32')    \n",
    "        stddev = dist_file.iloc[0, np.where(pd.DataFrame(dist_file.columns.values)[0].str.contains(col_name+'_STDD'))[0]]    \n",
    "        if not mean.empty and not stddev.empty:  \n",
    "            mean = np.float32(mean.values[0])\n",
    "            stddev = np.float32(stddev.values[0])            \n",
    "            if stddev == 0: \n",
    "                stddv_0.append(col_name)        \n",
    "            else:        \n",
    "                data[col_name] = data[col_name].apply(lambda x: zscore(x, mean, stddev))                        \n",
    "        else: \n",
    "            nnorm_cols.append(col_name)\n",
    "    print('STANDARD DEV zero: ', stddv_0)        \n",
    "    return data, nnorm_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepro_chunk(file_name, file_path, chunksize, label, log_file, nan_cols, categorical_cols, descriptive_cols, time_cols,\n",
    "                 dist_file, with_index, refNorm, train_period, valid_period, test_period, robust_cols, minmax_cols=None, hdf=None, tfrec=None):\n",
    "    gflag = ''    \n",
    "    i = 1                  \n",
    "    train_index = 0\n",
    "    valid_index = 0\n",
    "    test_index = 0\n",
    "    for chunk in pd.read_csv(file_path, chunksize = chunksize, sep=',', low_memory=False):    \n",
    "        print('chunk: ', i, ' chunk size: ', chunk.shape[0])\n",
    "        log_file.write('chunk: %d, chunk size: %d \\n' % (i, chunk.shape[0]))\n",
    "        chunk.columns = chunk.columns.str.upper()                            \n",
    "        \n",
    "        log_df = chunk[chunk[label].isnull()]\n",
    "        log_file.write('Dropping Rows with Null Labels - Number of rows: %d\\r\\n' % (log_df.shape[0]))\n",
    "        chunk.drop(chunk.index[chunk[label].isnull()], axis=0, inplace=True)\n",
    "        \n",
    "        log_df = chunk[chunk['INVALID_TRANSITIONS']==1]\n",
    "        log_file.write('Dropping Rows with Invalid Transitions - Number of rows: %d\\r\\n' % (log_df.shape[0]))                                \n",
    "        chunk.drop(chunk.index[chunk['INVALID_TRANSITIONS']==1], axis=0, inplace=True)        \n",
    "        \n",
    "        gflag = drop_invalid_delinquency_status(chunk, gflag, log_file)               \n",
    "                    \n",
    "        null_columns=chunk.columns[chunk.isnull().any()]\n",
    "        log_df = chunk[chunk.isnull().any(axis=1)][null_columns]\n",
    "        log_file.write('Filling NULL values - (rows, cols) : %d, %d\\r\\n' % (log_df.shape[0], log_df.shape[1]))                    \n",
    "        log_df = chunk[null_columns].isnull().sum().to_frame().reset_index()\n",
    "        log_df.to_csv(log_file, index=False, mode='a')                                    \n",
    "        nan_cols = imputing_nan_values(nan_cols, dist_file)            \n",
    "        chunk.fillna(value=nan_cols, inplace=True)   \n",
    "        \n",
    "        chunk.drop_duplicates(inplace=True) # Follow this instruction!!                        \n",
    "        logger.info('dropping invalid transitions and delinquency status, fill nan values, drop duplicates')                  \n",
    "        log_file.write('Drop duplicates - new size : %d\\r\\n' % (chunk.shape[0]))\n",
    "                               \n",
    "        chunk.reset_index(drop=True, inplace=True)  #don't remove this line! otherwise NaN values appears.\n",
    "        chunk['ORIGINATION_YEAR'][chunk['ORIGINATION_YEAR']<1995] = \"B1995\"\n",
    "        for k,v in categorical_cols.items():\n",
    "            # if (chunk[k].dtype=='O'):                \n",
    "            chunk[k] = chunk[k].astype('str')\n",
    "            chunk[k] = chunk[k].str.strip()\n",
    "            chunk[k].replace(['\\.0$'], [''], regex=True,  inplace=True)\n",
    "            new_cols = oneHotDummies_column(chunk[k], v)\n",
    "            if (chunk[k].value_counts().sum()!=new_cols.sum().sum()):\n",
    "                print('Error at categorization, different sizes', k)\n",
    "                print(chunk[k].value_counts(), new_cols.sum())                \n",
    "                log_file.write('Error at categorization, different sizes %s\\r\\n' % str(k))\n",
    "                chunk[new_cols.columns] = new_cols\n",
    "            else:\n",
    "                chunk[new_cols.columns] = new_cols\n",
    "                log_file.write('New columns added: %s\\r\\n' % str(new_cols.columns.values))\n",
    "            \n",
    "                    \n",
    "        allfeatures_drop_cols(chunk, descriptive_cols)                    \n",
    "        #np.savetxt(log_file, descriptive_cols, header='descriptive_cols dropped:', newline=\" \")\n",
    "        log_file.write('descriptive_cols dropped: %s\\r\\n' % str(descriptive_cols))\n",
    "        allfeatures_drop_cols(chunk, time_cols)\n",
    "        #np.savetxt(log_file, time_cols, header='time_cols dropped:', newline=\" \")\n",
    "        log_file.write('time_cols dropped: %s\\r\\n' % str(time_cols))\n",
    "        cat_list = list(categorical_cols.keys())\n",
    "        cat_list.remove('DELINQUENCY_STATUS_NEXT')\n",
    "        #np.savetxt(log_file, cat_list, header='categorical_cols dropped:', newline=\" \")\n",
    "        log_file.write('categorical_cols dropped: %s\\r\\n' % str(cat_list))\n",
    "        allfeatures_drop_cols(chunk, cat_list)\n",
    "\n",
    "        chunk.reset_index(drop=True, inplace=True)  \n",
    "        chunk.set_index(['LOAN_ID', 'DELINQUENCY_STATUS_NEXT', 'PERIOD'], append=True, inplace=True) #4 indexes\n",
    "        # np.savetxt(log_file, str(chunk.index.names), header='Indexes created:', newline=\" \")\n",
    "        log_file.write('Indexes created: %s\\r\\n' % str(chunk.index.names))\n",
    "         \n",
    "        \n",
    "        \n",
    "        if chunk.isnull().any().any(): \n",
    "            # from IPython.core.debugger import Tracer; Tracer()()\n",
    "            raise ValueError('There are null values...File: ' + file_name)   \n",
    "                \n",
    "        \n",
    "        if (refNorm==True):            \n",
    "            chunk[robust_cols], nnorm_cols =  zscore_apply(dist_file, chunk[robust_cols]) #robust_normalizer.transform(chunk[robust_cols])            \n",
    "            log_file.write('Columns not normalized: %s\\r\\n' % str(nnorm_cols))            \n",
    "            log_file.write('Columns normalized: %s\\r\\n' % str(set(robust_cols)-set(nnorm_cols)))\n",
    "            \n",
    "        \n",
    "        if chunk.isnull().any().any(): raise ValueError('There are null values...File: ' + file_name)       \n",
    "        \n",
    "        chunk_periods = set(list(chunk.index.get_level_values('PERIOD')))\n",
    "        print(tfrec)\n",
    "        if (tfrec!=None):\n",
    "            train_index = tag_chunk('train', label, chunk, chunk_periods, train_period, log_file, with_index, train_index, tfrec=tfrec[0])\n",
    "            valid_index = tag_chunk('valid', label, chunk, chunk_periods, valid_period, log_file, with_index, valid_index, tfrec=tfrec[1])\n",
    "            test_index = tag_chunk('test', label, chunk, chunk_periods, test_period, log_file, with_index, test_index, tfrec=tfrec[2])\n",
    "            sys.stdout.flush()\n",
    "        elif (hdf!=None):\n",
    "            train_index = tag_chunk('train', label, chunk, chunk_periods, train_period, log_file, with_index, train_index, hdf=hdf[0])\n",
    "            valid_index = tag_chunk('valid', label, chunk, chunk_periods, valid_period, log_file, with_index, valid_index, hdf=hdf[1])\n",
    "            test_index = tag_chunk('test', label, chunk, chunk_periods, test_period, log_file, with_index, test_index, hdf=hdf[2])                \n",
    "        \n",
    "        inter_periods = list(chunk_periods.intersection(set(range(test_period[1]+1,355))))    \n",
    "        log_file.write('Periods greater than test_period: %s\\r\\n' % str(inter_periods))\n",
    "        p_chunk = chunk.loc[(slice(None), slice(None), slice(None), inter_periods), :]\n",
    "        log_file.write('Records greater than test_period - Number of rows: %d\\r\\n' % (p_chunk.shape[0]))\n",
    "        \n",
    "        del chunk        \n",
    "        i +=  1   \n",
    "    \n",
    "    return train_index, valid_index, test_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_robust_normalizer(ncols, dist_file, normalizer_type='robust_scaler_sk', center_value='median'):            \n",
    "    norm_cols = []\n",
    "    scales = []\n",
    "    centers = []\n",
    "    scales_0 =[]\n",
    "    for i, x in enumerate (ncols):                        \n",
    "        x_frame = dist_file.iloc[:, np.where(pd.DataFrame(dist_file.columns.values)[0].str.contains(x+'_Q'))[0]]    \n",
    "        if not x_frame.empty and (x_frame.shape[1]>1):       \n",
    "            iqr = float(pd.to_numeric(x_frame[x+'_Q3'], errors='coerce').subtract(pd.to_numeric(x_frame[x+'_Q1'], errors='coerce')))\n",
    "            if iqr == 0: scales_0.append(x)\n",
    "            if iqr!=0: \n",
    "                norm_cols.append(x)                \n",
    "                scales.append(iqr)                    \n",
    "                if center_value == 'median':\n",
    "                    centers.append( float(x_frame[x+'_MEDIAN']) )   \n",
    "                else:\n",
    "                    centers.append( float(x_frame[x+'_Q1']) )                                       \n",
    "    if (normalizer_type == 'robust_scaler_sk'):    \n",
    "        normalizer = RobustScaler()\n",
    "        normalizer.scale_ = scales\n",
    "        normalizer.center_ = centers        \n",
    "    elif (normalizer_type == 'percentile_scaler'):    \n",
    "        normalizer = Normalizer.Normalizer(scales, centers)     \n",
    "    else: normalizer=None                  \n",
    "    \n",
    "    print(scales_0)\n",
    "    \n",
    "    return norm_cols, normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_minmax_normalizer(ncols, scales, dist_file):    \n",
    "    norm_cols = []\n",
    "    minmax_scales = []\n",
    "    centers = []\n",
    "    for i, x in enumerate (ncols):  \n",
    "        x_min = dist_file.iloc[0, np.where(pd.DataFrame(dist_file.columns.values)[0].str.contains(x+'_MIN'))[0]]\n",
    "        x_max = dist_file.iloc[0, np.where(pd.DataFrame(dist_file.columns.values)[0].str.contains(x+'_MAX'))[0]]\n",
    "        if not(x_min.empty) and not(x_max.empty):            \n",
    "            x_min = np.float32(x_min.values[0])\n",
    "            x_max = np.float32(x_max.values[0])\n",
    "            minmax_scales.append(x_max - x_min)                            \n",
    "            centers.append(x_min)\n",
    "            norm_cols.append(x)\n",
    "            # to_delete.append(i)\n",
    "        \n",
    "    normalizer = Normalizer.Normalizer(minmax_scales, centers)         \n",
    "    \n",
    "    return norm_cols, normalizer #, to_delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def allfeatures_preprocessing(RAW_DIR, PRO_DIR, raw_dir, train_period, valid_period, test_period, dividing='percentage', \n",
    "                              chunksize=500000, refNorm=True, with_index=True, output_hdf=True, label='DELINQUENCY_STATUS_NEXT'):            \n",
    "\n",
    "    descriptive_cols = [\n",
    "#        'LOAN_ID',\n",
    "    'ASOFMONTH',        \n",
    "    'PERIOD_NEXT',\n",
    "    'MOD_PER_FROM',\n",
    "    'MOD_PER_TO',\n",
    "    'PROPERTY_ZIP',\n",
    "    'INVALID_TRANSITIONS'\n",
    "    ]\n",
    "\n",
    "    numeric_cols = ['MBA_DAYS_DELINQUENT', 'MBA_DAYS_DELINQUENT_NAN',\n",
    "       'CURRENT_INTEREST_RATE', 'CURRENT_INTEREST_RATE_NAN', 'LOANAGE', 'LOANAGE_NAN',\n",
    "       'CURRENT_BALANCE', 'CURRENT_BALANCE_NAN', 'SCHEDULED_PRINCIPAL',\n",
    "       'SCHEDULED_PRINCIPAL_NAN', 'SCHEDULED_MONTHLY_PANDI',\n",
    "       'SCHEDULED_MONTHLY_PANDI_NAN', \n",
    "       'LLMA2_CURRENT_INTEREST_SPREAD', 'LLMA2_CURRENT_INTEREST_SPREAD_NAN',  \n",
    "       'LLMA2_C_IN_LAST_12_MONTHS',\n",
    "       'LLMA2_30_IN_LAST_12_MONTHS', 'LLMA2_60_IN_LAST_12_MONTHS',\n",
    "       'LLMA2_90_IN_LAST_12_MONTHS', 'LLMA2_FC_IN_LAST_12_MONTHS',\n",
    "       'LLMA2_REO_IN_LAST_12_MONTHS', 'LLMA2_0_IN_LAST_12_MONTHS',\n",
    "       'LLMA2_HIST_LAST_12_MONTHS_MIS', \n",
    "       'NUM_MODIF', 'NUM_MODIF_NAN', 'P_RATE_TO_MOD', 'P_RATE_TO_MOD_NAN', 'MOD_RATE',\n",
    "       'MOD_RATE_NAN', 'DIF_RATE', 'DIF_RATE_NAN', 'P_MONTHLY_PAY',\n",
    "       'P_MONTHLY_PAY_NAN', 'MOD_MONTHLY_PAY', 'MOD_MONTHLY_PAY_NAN',\n",
    "       'DIF_MONTHLY_PAY', 'DIF_MONTHLY_PAY_NAN', 'CAPITALIZATION_AMT',\n",
    "       'CAPITALIZATION_AMT_NAN', 'MORTGAGE_RATE', 'MORTGAGE_RATE_NAN',\n",
    "       'FICO_SCORE_ORIGINATION', 'INITIAL_INTEREST_RATE', 'ORIGINAL_LTV',\n",
    "       'ORIGINAL_BALANCE', 'BACKEND_RATIO', 'BACKEND_RATIO_NAN',\n",
    "       'ORIGINAL_TERM', 'ORIGINAL_TERM_NAN', 'SALE_PRICE', 'SALE_PRICE_NAN', \t   \n",
    "       'PREPAY_PENALTY_TERM', 'PREPAY_PENALTY_TERM_NAN', \n",
    "        'NUMBER_OF_UNITS', 'NUMBER_OF_UNITS_NAN', 'MARGIN',\n",
    "       'MARGIN_NAN', 'PERIODIC_RATE_CAP', 'PERIODIC_RATE_CAP_NAN',\n",
    "       'PERIODIC_RATE_FLOOR', 'PERIODIC_RATE_FLOOR_NAN', 'LIFETIME_RATE_CAP',\n",
    "       'LIFETIME_RATE_CAP_NAN', 'LIFETIME_RATE_FLOOR',\n",
    "       'LIFETIME_RATE_FLOOR_NAN', 'RATE_RESET_FREQUENCY',\n",
    "       'RATE_RESET_FREQUENCY_NAN', 'PAY_RESET_FREQUENCY',\n",
    "       'PAY_RESET_FREQUENCY_NAN', 'FIRST_RATE_RESET_PERIOD',\n",
    "       'FIRST_RATE_RESET_PERIOD_NAN', \t   \n",
    "        'LLMA2_PRIME',\n",
    "       'LLMA2_SUBPRIME', 'LLMA2_APPVAL_LT_SALEPRICE', 'LLMA2_ORIG_RATE_SPREAD',\n",
    "       'LLMA2_ORIG_RATE_SPREAD_NAN', 'AGI', 'AGI_NAN', 'UR', 'UR_NAN', 'LLMA2_ORIG_RATE_ORIG_MR_SPREAD', \n",
    "       'LLMA2_ORIG_RATE_ORIG_MR_SPREAD_NAN', 'COUNT_INT_RATE_LESS', 'NUM_PRIME_ZIP', 'NUM_PRIME_ZIP_NAN'\n",
    "       ]\n",
    "\n",
    "\n",
    "    nan_cols = {'MBA_DAYS_DELINQUENT': 'median', 'CURRENT_INTEREST_RATE': 'median', 'LOANAGE': 'median',\n",
    "                'CURRENT_BALANCE' : 'median', 'SCHEDULED_PRINCIPAL': 'median', 'SCHEDULED_MONTHLY_PANDI': 'median',       \n",
    "                'LLMA2_CURRENT_INTEREST_SPREAD': 'median', 'NUM_MODIF': 0, 'P_RATE_TO_MOD': 0, 'MOD_RATE': 0,\n",
    "                'DIF_RATE': 0, 'P_MONTHLY_PAY': 0, 'MOD_MONTHLY_PAY': 0, 'DIF_MONTHLY_PAY': 0, 'CAPITALIZATION_AMT': 0,\n",
    "                'MORTGAGE_RATE': 'median', 'FICO_SCORE_ORIGINATION': 'median', 'INITIAL_INTEREST_RATE': 'median', 'ORIGINAL_LTV': 'median',\n",
    "                'ORIGINAL_BALANCE': 'median', 'BACKEND_RATIO': 'median', 'ORIGINAL_TERM': 'median', 'SALE_PRICE': 'median', 'PREPAY_PENALTY_TERM': 'median',\n",
    "                'NUMBER_OF_UNITS': 'median', 'MARGIN': 'median', 'PERIODIC_RATE_CAP': 'median', 'PERIODIC_RATE_FLOOR': 'median', 'LIFETIME_RATE_CAP': 'median',\n",
    "                'LIFETIME_RATE_FLOOR': 'median', 'RATE_RESET_FREQUENCY': 'median', 'PAY_RESET_FREQUENCY': 'median',\n",
    "                'FIRST_RATE_RESET_PERIOD': 'median', 'LLMA2_ORIG_RATE_SPREAD': 'median', 'AGI': 'median', 'UR': 'median',\n",
    "                'LLMA2_C_IN_LAST_12_MONTHS': 'median', 'LLMA2_30_IN_LAST_12_MONTHS': 'median', 'LLMA2_60_IN_LAST_12_MONTHS': 'median',\n",
    "                'LLMA2_90_IN_LAST_12_MONTHS': 'median', 'LLMA2_FC_IN_LAST_12_MONTHS': 'median',\n",
    "                'LLMA2_REO_IN_LAST_12_MONTHS': 'median', 'LLMA2_0_IN_LAST_12_MONTHS': 'median', \n",
    "                'LLMA2_ORIG_RATE_ORIG_MR_SPREAD':0, 'COUNT_INT_RATE_LESS' :'median', 'NUM_PRIME_ZIP':'median'\n",
    "                }\n",
    "\n",
    "    categorical_cols = {'MBA_DELINQUENCY_STATUS':  ['0','3','6','9','C','F','R'], 'DELINQUENCY_STATUS_NEXT': ['0','3','6','9','C','F','R'],  #,'S','T','X'\n",
    "                           'BUYDOWN_FLAG': ['N','U','Y'], 'NEGATIVE_AMORTIZATION_FLAG': ['N','U','Y'], 'PREPAY_PENALTY_FLAG': ['N','U','Y'],\n",
    "                           'OCCUPANCY_TYPE': ['1','2','3','U'], 'PRODUCT_TYPE': ['10','20','30','40','50','51','52','53','54','5A','5Z',\n",
    "                                            '60','61','62','63','6Z','70','80','81','82','83','84','8Z','U'], \n",
    "                           'PROPERTY_TYPE': ['1','2','3','4','5','6','7','8','9','M','U','Z'], 'LOAN_PURPOSE_CATEGORY': ['P','R','U'], \n",
    "                           'DOCUMENTATION_TYPE': ['1','2','3','U'], 'CHANNEL': ['1','2','3','4','5','6','7','8','9','A','B','C','D','U'], \n",
    "                           'LOAN_TYPE': ['1','2','3','4','5','6','U'], 'IO_FLAG': ['N','U','Y'], \n",
    "                           'CONVERTIBLE_FLAG': ['N','U','Y'], 'POOL_INSURANCE_FLAG': ['N','U','Y'], 'STATE': ['AK', 'AL', 'AR', 'AZ', 'CA', 'CO',\n",
    "                                               'CT', 'DC', 'DE', 'FL', 'GA', 'HI', 'IA', 'ID', 'IL', 'IN', 'KS', 'KY', 'LA', 'MA', \n",
    "                                               'MD', 'ME', 'MI', 'MN', 'MO', 'MS', 'MT', 'NC', 'ND', 'NE', 'NH', 'NJ', 'NM', 'NV', \n",
    "                                               'NY', 'OH', 'OK', 'OR', 'PA', 'PR', 'RI', 'SC', 'SD', 'TN', 'TX', 'UT', 'VA', 'VT', \n",
    "                                               'WA', 'WI', 'WV', 'WY'], \n",
    "                           'CURRENT_INVESTOR_CODE': ['240', '250', '253', 'U'], 'ORIGINATION_YEAR': ['B1995','1995','1996','1997','1998','1999','2000','2001','2002','2003',\n",
    "                                                    '2004','2005','2006','2007','2008','2009','2010','2011','2012','2013','2014','2015','2016','2017','2018']}\n",
    "\n",
    "    time_cols = ['YEAR', 'MONTH'] #, 'PERIOD'] #no nan values        \n",
    "\n",
    "    total_cols = numeric_cols.copy() \n",
    "    total_cols.extend(descriptive_cols)\n",
    "    total_cols.extend(categorical_cols.keys())\n",
    "    total_cols.extend(time_cols)\n",
    "    print('total_cols size: ', len(total_cols)) #110 !=112?? set(chunk_cols) - set(total_cols): {'LOAN_ID', 'PERIOD'}\n",
    "    \n",
    "    pd.set_option('io.hdf.default_format','table')\n",
    "\n",
    "    dist_file = pd.read_csv(os.path.join(RAW_DIR, \"percentile features3-test.csv\"), sep=';', low_memory=False)\n",
    "    dist_file.columns = dist_file.columns.str.upper()\n",
    "\n",
    "    ncols = [x for x in numeric_cols if x.find('NAN')<0]\n",
    "    print(ncols)\n",
    "\n",
    "    #sum = 0\n",
    "    #for elem in categorical_cols.values():\n",
    "    #    sum += len(elem)\n",
    "    #print('total categorical values: ', sum) #181\n",
    "\n",
    "    for file_path in glob.glob(os.path.join(RAW_DIR, raw_dir,\"*.txt\")):  \n",
    "        file_name = os.path.basename(file_path)\n",
    "        if with_index==True:\n",
    "            target_path = os.path.join(PRO_DIR, raw_dir,file_name[:-4])        \n",
    "        else:\n",
    "            target_path = os.path.join(PRO_DIR, raw_dir,file_name[:-4]+'_non_index')\n",
    "        log_file=open(target_path+'-log.txt', 'w+', 1)        \n",
    "        print('Preprocessing File: ' + file_path)\n",
    "        log_file.write('Preprocessing File:  %s\\r\\n' % file_path)\n",
    "        startTime = datetime.now()      \n",
    "        \n",
    "        if (output_hdf == True):\n",
    "            #with  pd.HDFStore(target_path +'-pp.h5', complib='lzo', complevel=9) as hdf: #complib='lzo', complevel=9\n",
    "            train_writer = pd.HDFStore(target_path +'-train-pp.h5', complib='lzo', complevel=9) \n",
    "            valid_writer = pd.HDFStore(target_path +'-valid-pp.h5', complib='lzo', complevel=9)\n",
    "            test_writer = pd.HDFStore(target_path +'-test-pp.h5', complib='lzo', complevel=9) \n",
    "\n",
    "            print('generating: ', target_path +'-pp.h5')\n",
    "            train_index, valid_index, test_index = prepro_chunk(file_name, file_path, chunksize, label, log_file, \n",
    "                                                                nan_cols, categorical_cols, descriptive_cols, time_cols,\n",
    "                                                                dist_file, with_index, \n",
    "                                                                refNorm, train_period, valid_period, test_period, ncols,                                                                \n",
    "                                                                hdf=[train_writer, valid_writer, test_writer], tfrec=None)            \n",
    "\n",
    "\n",
    "            if train_writer.get_storer('train/features').nrows != train_writer.get_storer('train/labels').nrows:\n",
    "                    raise ValueError('Train-DataSet: Sizes should match!')  \n",
    "            if valid_writer.get_storer('valid/features').nrows != valid_writer.get_storer('valid/labels').nrows:\n",
    "                    raise ValueError('Valid-DataSet: Sizes should match!')  \n",
    "            if test_writer.get_storer('test/features').nrows != test_writer.get_storer('test/labels').nrows:\n",
    "                    raise ValueError('Test-DataSet: Sizes should match!')  \n",
    "\n",
    "            print('train/features size: ', train_writer.get_storer('train/features').nrows)\n",
    "            print('valid/features size: ', valid_writer.get_storer('valid/features').nrows)\n",
    "            print('test/features size: ', test_writer.get_storer('test/features').nrows)\n",
    "\n",
    "            log_file.write('***SUMMARY***\\n')\n",
    "            log_file.write('train/features size: %d\\r\\n' %(train_writer.get_storer('train/features').nrows))\n",
    "            log_file.write('valid/features size: %d\\r\\n' %(valid_writer.get_storer('valid/features').nrows))\n",
    "            log_file.write('test/features size: %d\\r\\n' %(test_writer.get_storer('test/features').nrows))\n",
    "\n",
    "            logger.info('training, validation and testing set into .h5 file')        \n",
    "        else:        \n",
    "            train_writer = tf.python_io.TFRecordWriter(target_path +'-train-pp.tfrecords')\n",
    "            valid_writer = tf.python_io.TFRecordWriter(target_path +'-valid-pp.tfrecords')\n",
    "            test_writer = tf.python_io.TFRecordWriter(target_path +'-test-pp.tfrecords')\n",
    "            train_index, valid_index, test_index = prepro_chunk(file_name, file_path, chunksize, label, log_file, \n",
    "                                                                nan_cols, categorical_cols, descriptive_cols, time_cols,\n",
    "                                                                dist_file, with_index, \n",
    "                                                                refNorm, train_period, valid_period, test_period, ncols,\n",
    "                                                                hdf=None, tfrec=[train_writer, valid_writer, test_writer]) \n",
    "        print(train_index, valid_index, test_index)\n",
    "        train_writer.close()\n",
    "        valid_writer.close()\n",
    "        test_writer.close()        \n",
    "        \n",
    "        #def allfeatures_prepro_file(RAW_DIR, file_path, raw_dir, file_name, target_path, train_period, valid_period, test_period, log_file, dividing='percentage', chunksize=500000, \n",
    "        #                    refNorm=True, , with_index=True, output_hdf=True):\n",
    "\n",
    "        #allfeatures_prepro_file(RAW_DIR, file_path, raw_dir, file_name, target_path, train_num, valid_num, test_num, log_file, dividing=dividing, chunksize=chunksize, \n",
    "        #                        refNorm=refNorm, with_index=with_index, output_hdf=output_hdf)          \n",
    "        \n",
    "        startTime = datetime.now() - startTime\n",
    "        print('Preprocessing Time: ', startTime)     \n",
    "        log_file.write('Preprocessing Time:  %s\\r\\n' % str(startTime))\n",
    "        log_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "startTime = datetime.now()\n",
    "\n",
    "if not os.path.exists(os.path.join(PRO_DIR, FLAGS.prepro_dir)): #os.path.exists\n",
    "    os.makedirs(os.path.join(PRO_DIR, FLAGS.prepro_dir))\n",
    "\n",
    "allfeatures_preprocessing(RAW_DIR, PRO_DIR, FLAGS.prepro_dir, FLAGS.train_period, FLAGS.valid_period, FLAGS.test_period, dividing='percentage', \n",
    "                          chunksize=FLAGS.prepro_chunksize, refNorm=FLAGS.ref_norm, with_index=FLAGS.prepro_with_index, output_hdf=True)        \n",
    "print('Preprocessing - Time: ', datetime.now() - startTime)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
